{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4cd295-d147-4055-b0df-8c9beca7fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig  # ExperimentConfig,\n",
    "from pytorch_tabular.config import OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models import FTTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9dadf4-ee4a-4c26-9756-a1e95f447cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig(\n",
    "             early_stopping=\"precision\",\n",
    "            early_stopping_mode=\"min\",  # Set the mode as min because for val_loss, lower is better\n",
    "            early_stopping_patience= 5 #No. of epochs of degradation training will wait before terminating\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd908bf-2a5b-4a75-91fb-5ba881a492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainerConfig in module pytorch_tabular.config.config:\n",
      "\n",
      "class TrainerConfig(builtins.object)\n",
      " |  TrainerConfig(batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |  \n",
      " |  Trainer configuration\n",
      " |  Args:\n",
      " |      batch_size (int): Number of samples in each batch of training\n",
      " |  \n",
      " |      data_aware_init_batch_size (int): Number of samples in each batch of training for the data-aware initialization,\n",
      " |          when applicable. Defaults to 2000\n",
      " |  \n",
      " |      fast_dev_run (bool): runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val\n",
      " |              and test to find any bugs (ie: a sort of unit test).\n",
      " |  \n",
      " |      max_epochs (int): Maximum number of epochs to be run\n",
      " |  \n",
      " |      min_epochs (Optional[int]): Force training for at least these many epochs. 1 by default\n",
      " |  \n",
      " |      max_time (Optional[int]): Stop training after this amount of time has passed. Disabled by default\n",
      " |              (None)\n",
      " |  \n",
      " |      gpus (Optional[int]): DEPRECATED: Number of gpus to train on (int). -1 uses all available GPUs. By\n",
      " |              default uses CPU (None)\n",
      " |  \n",
      " |      accelerator (Optional[str]): The accelerator to use for training. Can be one of\n",
      " |              'cpu','gpu','tpu','ipu', 'mps', 'auto'. Defaults to 'auto'.\n",
      " |              Choices are: [`cpu`,`gpu`,`tpu`,`ipu`,'mps',`auto`].\n",
      " |  \n",
      " |      devices (Optional[int]): Number of devices to train on (int). -1 uses all available devices. By\n",
      " |              default uses all available devices (-1)\n",
      " |  \n",
      " |      devices_list (Optional[List[int]]): List of devices to train on (list). If specified, takes\n",
      " |              precedence over `devices` argument. Defaults to None\n",
      " |  \n",
      " |      accumulate_grad_batches (int): Accumulates grads every k batches or as set up in the dict. Trainer\n",
      " |              also calls optimizer.step() for the last indivisible step number.\n",
      " |  \n",
      " |      auto_lr_find (bool): Runs a learning rate finder algorithm (see this paper) when calling\n",
      " |              trainer.tune(), to find optimal initial learning rate.\n",
      " |  \n",
      " |      auto_select_gpus (bool): If enabled and `devices` is an integer, pick available gpus automatically.\n",
      " |              This is especially useful when GPUs are configured to be in 'exclusive mode', such that only one\n",
      " |              process at a time can access them.\n",
      " |  \n",
      " |      check_val_every_n_epoch (int): Check val every n train epochs.\n",
      " |  \n",
      " |      gradient_clip_val (float): Gradient clipping value\n",
      " |  \n",
      " |      overfit_batches (float): Uses this much data of the training set. If nonzero, will use the same\n",
      " |              training set for validation and testing. If the training dataloaders have shuffle=True, Lightning\n",
      " |              will automatically disable it. Useful for quickly debugging or trying to overfit on purpose.\n",
      " |  \n",
      " |      deterministic (bool): If true enables cudnn.deterministic. Might make your system slower, but\n",
      " |              ensures reproducibility.\n",
      " |  \n",
      " |      profiler (Optional[str]): To profile individual steps during training and assist in identifying\n",
      " |              bottlenecks. None, simple or advanced, pytorch. Choices are:\n",
      " |              [`None`,`simple`,`advanced`,`pytorch`].\n",
      " |  \n",
      " |      early_stopping (Optional[str]): The loss/metric that needed to be monitored for early stopping. If\n",
      " |              None, there will be no early stopping\n",
      " |  \n",
      " |      early_stopping_min_delta (float): The minimum delta in the loss/metric which qualifies as an\n",
      " |              improvement in early stopping\n",
      " |  \n",
      " |      early_stopping_mode (str): The direction in which the loss/metric should be optimized. Choices are:\n",
      " |              [`max`,`min`].\n",
      " |  \n",
      " |      early_stopping_patience (int): The number of epochs to wait until there is no further improvements\n",
      " |              in loss/metric\n",
      " |  \n",
      " |      early_stopping_kwargs (Optional[Dict]): Additional keyword arguments for the early stopping callback.\n",
      " |              See the documentation for the PyTorch Lightning EarlyStopping callback for more details.\n",
      " |  \n",
      " |      checkpoints (Optional[str]): The loss/metric that needed to be monitored for checkpoints. If None,\n",
      " |              there will be no checkpoints\n",
      " |  \n",
      " |      checkpoints_path (str): The path where the saved models will be\n",
      " |  \n",
      " |      checkpoints_every_n_epochs (int): Number of training steps between checkpoints\n",
      " |  \n",
      " |      checkpoints_name (Optional[str]): The name under which the models will be saved. If left blank,\n",
      " |              first it will look for `run_name` in experiment_config and if that is also None then it will use a\n",
      " |              generic name like task_version.\n",
      " |  \n",
      " |      checkpoints_mode (str): The direction in which the loss/metric should be optimized\n",
      " |  \n",
      " |      checkpoints_save_top_k (int): The number of best models to save\n",
      " |  \n",
      " |      checkpoints_kwargs (Optional[Dict]): Additional keyword arguments for the checkpoints callback.\n",
      " |              See the documentation for the PyTorch Lightning ModelCheckpoint callback for more details.\n",
      " |  \n",
      " |      load_best (bool): Flag to load the best model saved during training\n",
      " |  \n",
      " |      track_grad_norm (int): Track and Log Gradient Norms in the logger. -1 by default means no tracking.\n",
      " |              1 for the L1 norm, 2 for L2 norm, etc.\n",
      " |  \n",
      " |      progress_bar (str): Progress bar type. Can be one of: `none`, `simple`, `rich`. Defaults to `rich`.\n",
      " |  \n",
      " |      precision (int): Precision of the model. Can be one of: `32`, `16`, `64`. Defaults to `32`..\n",
      " |              Choices are: [`32`,`16`,`64`].\n",
      " |  \n",
      " |      seed (int): Seed for random number generators. Defaults to 42\n",
      " |  \n",
      " |      trainer_kwargs (Dict[str, Any]): Additional kwargs to be passed to PyTorch Lightning Trainer. See\n",
      " |              https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.html#pytorch_lightning.trainer.Trainer\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'accelerator': typing.Optional[str], 'accumulate_gr...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'accelerator': Field(name='accelerator',type=t...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('batch_size', 'data_aware_init_batch_size', 'fast_de...\n",
      " |  \n",
      " |  accelerator = 'auto'\n",
      " |  \n",
      " |  accumulate_grad_batches = 1\n",
      " |  \n",
      " |  auto_lr_find = False\n",
      " |  \n",
      " |  auto_select_gpus = True\n",
      " |  \n",
      " |  batch_size = 64\n",
      " |  \n",
      " |  check_val_every_n_epoch = 1\n",
      " |  \n",
      " |  checkpoints = 'valid_loss'\n",
      " |  \n",
      " |  checkpoints_every_n_epochs = 1\n",
      " |  \n",
      " |  checkpoints_mode = 'min'\n",
      " |  \n",
      " |  checkpoints_name = None\n",
      " |  \n",
      " |  checkpoints_path = 'saved_models'\n",
      " |  \n",
      " |  checkpoints_save_top_k = 1\n",
      " |  \n",
      " |  data_aware_init_batch_size = 2000\n",
      " |  \n",
      " |  deterministic = False\n",
      " |  \n",
      " |  devices = None\n",
      " |  \n",
      " |  devices_list = None\n",
      " |  \n",
      " |  early_stopping = 'valid_loss'\n",
      " |  \n",
      " |  early_stopping_min_delta = 0.001\n",
      " |  \n",
      " |  early_stopping_mode = 'min'\n",
      " |  \n",
      " |  early_stopping_patience = 3\n",
      " |  \n",
      " |  fast_dev_run = False\n",
      " |  \n",
      " |  gpus = None\n",
      " |  \n",
      " |  gradient_clip_val = 0.0\n",
      " |  \n",
      " |  load_best = True\n",
      " |  \n",
      " |  max_epochs = 10\n",
      " |  \n",
      " |  max_time = None\n",
      " |  \n",
      " |  min_epochs = 1\n",
      " |  \n",
      " |  overfit_batches = 0.0\n",
      " |  \n",
      " |  precision = 32\n",
      " |  \n",
      " |  profiler = None\n",
      " |  \n",
      " |  progress_bar = 'rich'\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  track_grad_norm = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TrainerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801bd075-1f83-4e8f-af1f-880a46e5426f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Callback' from 'pytorch_tabular' (/home/boom/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_tabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Callback' from 'pytorch_tabular' (/home/boom/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca547bb4-a4f1-4f60-bcfe-2984984d302a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
