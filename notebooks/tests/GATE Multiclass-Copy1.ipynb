{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385dfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import GatedAdditiveTreeEnsembleConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    "    #ExperimentConfig,\n",
    ")\n",
    "from pytorch_tabular.utils import get_class_weighted_cross_entropy\n",
    "#pip install pytorch_tabular[extra]\n",
    "from evaluation.generalevaluator import *\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris, fetch_california_housing, load_breast_cancer\n",
    "from factory import create_data_loader\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d2cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainerConfig in module pytorch_tabular.config.config:\n",
      "\n",
      "class TrainerConfig(builtins.object)\n",
      " |  TrainerConfig(batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |  \n",
      " |  Trainer configuration\n",
      " |  Args:\n",
      " |      batch_size (int): Number of samples in each batch of training\n",
      " |  \n",
      " |      data_aware_init_batch_size (int): Number of samples in each batch of training for the data-aware initialization,\n",
      " |          when applicable. Defaults to 2000\n",
      " |  \n",
      " |      fast_dev_run (bool): runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val\n",
      " |              and test to find any bugs (ie: a sort of unit test).\n",
      " |  \n",
      " |      max_epochs (int): Maximum number of epochs to be run\n",
      " |  \n",
      " |      min_epochs (Optional[int]): Force training for at least these many epochs. 1 by default\n",
      " |  \n",
      " |      max_time (Optional[int]): Stop training after this amount of time has passed. Disabled by default\n",
      " |              (None)\n",
      " |  \n",
      " |      gpus (Optional[int]): DEPRECATED: Number of gpus to train on (int). -1 uses all available GPUs. By\n",
      " |              default uses CPU (None)\n",
      " |  \n",
      " |      accelerator (Optional[str]): The accelerator to use for training. Can be one of\n",
      " |              'cpu','gpu','tpu','ipu', 'mps', 'auto'. Defaults to 'auto'.\n",
      " |              Choices are: [`cpu`,`gpu`,`tpu`,`ipu`,'mps',`auto`].\n",
      " |  \n",
      " |      devices (Optional[int]): Number of devices to train on (int). -1 uses all available devices. By\n",
      " |              default uses all available devices (-1)\n",
      " |  \n",
      " |      devices_list (Optional[List[int]]): List of devices to train on (list). If specified, takes\n",
      " |              precedence over `devices` argument. Defaults to None\n",
      " |  \n",
      " |      accumulate_grad_batches (int): Accumulates grads every k batches or as set up in the dict. Trainer\n",
      " |              also calls optimizer.step() for the last indivisible step number.\n",
      " |  \n",
      " |      auto_lr_find (bool): Runs a learning rate finder algorithm (see this paper) when calling\n",
      " |              trainer.tune(), to find optimal initial learning rate.\n",
      " |  \n",
      " |      auto_select_gpus (bool): If enabled and `devices` is an integer, pick available gpus automatically.\n",
      " |              This is especially useful when GPUs are configured to be in 'exclusive mode', such that only one\n",
      " |              process at a time can access them.\n",
      " |  \n",
      " |      check_val_every_n_epoch (int): Check val every n train epochs.\n",
      " |  \n",
      " |      gradient_clip_val (float): Gradient clipping value\n",
      " |  \n",
      " |      overfit_batches (float): Uses this much data of the training set. If nonzero, will use the same\n",
      " |              training set for validation and testing. If the training dataloaders have shuffle=True, Lightning\n",
      " |              will automatically disable it. Useful for quickly debugging or trying to overfit on purpose.\n",
      " |  \n",
      " |      deterministic (bool): If true enables cudnn.deterministic. Might make your system slower, but\n",
      " |              ensures reproducibility.\n",
      " |  \n",
      " |      profiler (Optional[str]): To profile individual steps during training and assist in identifying\n",
      " |              bottlenecks. None, simple or advanced, pytorch. Choices are:\n",
      " |              [`None`,`simple`,`advanced`,`pytorch`].\n",
      " |  \n",
      " |      early_stopping (Optional[str]): The loss/metric that needed to be monitored for early stopping. If\n",
      " |              None, there will be no early stopping\n",
      " |  \n",
      " |      early_stopping_min_delta (float): The minimum delta in the loss/metric which qualifies as an\n",
      " |              improvement in early stopping\n",
      " |  \n",
      " |      early_stopping_mode (str): The direction in which the loss/metric should be optimized. Choices are:\n",
      " |              [`max`,`min`].\n",
      " |  \n",
      " |      early_stopping_patience (int): The number of epochs to wait until there is no further improvements\n",
      " |              in loss/metric\n",
      " |  \n",
      " |      early_stopping_kwargs (Optional[Dict]): Additional keyword arguments for the early stopping callback.\n",
      " |              See the documentation for the PyTorch Lightning EarlyStopping callback for more details.\n",
      " |  \n",
      " |      checkpoints (Optional[str]): The loss/metric that needed to be monitored for checkpoints. If None,\n",
      " |              there will be no checkpoints\n",
      " |  \n",
      " |      checkpoints_path (str): The path where the saved models will be\n",
      " |  \n",
      " |      checkpoints_every_n_epochs (int): Number of training steps between checkpoints\n",
      " |  \n",
      " |      checkpoints_name (Optional[str]): The name under which the models will be saved. If left blank,\n",
      " |              first it will look for `run_name` in experiment_config and if that is also None then it will use a\n",
      " |              generic name like task_version.\n",
      " |  \n",
      " |      checkpoints_mode (str): The direction in which the loss/metric should be optimized\n",
      " |  \n",
      " |      checkpoints_save_top_k (int): The number of best models to save\n",
      " |  \n",
      " |      checkpoints_kwargs (Optional[Dict]): Additional keyword arguments for the checkpoints callback.\n",
      " |              See the documentation for the PyTorch Lightning ModelCheckpoint callback for more details.\n",
      " |  \n",
      " |      load_best (bool): Flag to load the best model saved during training\n",
      " |  \n",
      " |      track_grad_norm (int): Track and Log Gradient Norms in the logger. -1 by default means no tracking.\n",
      " |              1 for the L1 norm, 2 for L2 norm, etc.\n",
      " |  \n",
      " |      progress_bar (str): Progress bar type. Can be one of: `none`, `simple`, `rich`. Defaults to `rich`.\n",
      " |  \n",
      " |      precision (int): Precision of the model. Can be one of: `32`, `16`, `64`. Defaults to `32`..\n",
      " |              Choices are: [`32`,`16`,`64`].\n",
      " |  \n",
      " |      seed (int): Seed for random number generators. Defaults to 42\n",
      " |  \n",
      " |      trainer_kwargs (Dict[str, Any]): Additional kwargs to be passed to PyTorch Lightning Trainer. See\n",
      " |              https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.html#pytorch_lightning.trainer.Trainer\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'accelerator': typing.Optional[str], 'accumulate_gr...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'accelerator': Field(name='accelerator',type=t...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('batch_size', 'data_aware_init_batch_size', 'fast_de...\n",
      " |  \n",
      " |  accelerator = 'auto'\n",
      " |  \n",
      " |  accumulate_grad_batches = 1\n",
      " |  \n",
      " |  auto_lr_find = False\n",
      " |  \n",
      " |  auto_select_gpus = True\n",
      " |  \n",
      " |  batch_size = 64\n",
      " |  \n",
      " |  check_val_every_n_epoch = 1\n",
      " |  \n",
      " |  checkpoints = 'valid_loss'\n",
      " |  \n",
      " |  checkpoints_every_n_epochs = 1\n",
      " |  \n",
      " |  checkpoints_mode = 'min'\n",
      " |  \n",
      " |  checkpoints_name = None\n",
      " |  \n",
      " |  checkpoints_path = 'saved_models'\n",
      " |  \n",
      " |  checkpoints_save_top_k = 1\n",
      " |  \n",
      " |  data_aware_init_batch_size = 2000\n",
      " |  \n",
      " |  deterministic = False\n",
      " |  \n",
      " |  devices = None\n",
      " |  \n",
      " |  devices_list = None\n",
      " |  \n",
      " |  early_stopping = 'valid_loss'\n",
      " |  \n",
      " |  early_stopping_min_delta = 0.001\n",
      " |  \n",
      " |  early_stopping_mode = 'min'\n",
      " |  \n",
      " |  early_stopping_patience = 3\n",
      " |  \n",
      " |  fast_dev_run = False\n",
      " |  \n",
      " |  gpus = None\n",
      " |  \n",
      " |  gradient_clip_val = 0.0\n",
      " |  \n",
      " |  load_best = True\n",
      " |  \n",
      " |  max_epochs = 10\n",
      " |  \n",
      " |  max_time = None\n",
      " |  \n",
      " |  min_epochs = 1\n",
      " |  \n",
      " |  overfit_batches = 0.0\n",
      " |  \n",
      " |  precision = 32\n",
      " |  \n",
      " |  profiler = None\n",
      " |  \n",
      " |  progress_bar = 'rich'\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  track_grad_norm = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TrainerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22eff7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_data_loader('iris', test_size=0.2, normalize_features = \"mean_std\", return_extra_info = True)\n",
    "X_train, X_val, y_train, y_val, extra_info = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fef8ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.473937</td>\n",
       "      <td>1.203658</td>\n",
       "      <td>-1.562535</td>\n",
       "      <td>-1.187793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.133071</td>\n",
       "      <td>2.992376</td>\n",
       "      <td>-1.276006</td>\n",
       "      <td>-1.187793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.085898</td>\n",
       "      <td>0.085709</td>\n",
       "      <td>0.385858</td>\n",
       "      <td>0.627942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "22          -1.473937          1.203658          -1.562535         -1.187793\n",
       "15          -0.133071          2.992376          -1.276006         -1.187793\n",
       "65           1.085898          0.085709           0.385858          0.627942"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72a140",
   "metadata": {},
   "source": [
    "# GATE Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GatedAdditiveTreeEnsembleConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(OptimizerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbfec264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 18:05:16,439 - {pytorch_tabular.tabular_model:102} - INFO - Experiment Tracking is turned off\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "data_config = DataConfig(\n",
    "    target=['target'],\n",
    "    continuous_cols= [i for i in extra_info[\"num_col_names\"] if i != \"target\"],\n",
    "    categorical_cols=extra_info[\"cat_col_names\"],\n",
    "    #num_workers = 4\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=16,\n",
    "    max_epochs=300,\n",
    "    early_stopping=\"valid_loss\", # Monitor valid_loss for early stopping\n",
    "    early_stopping_mode = \"min\", # Set the mode as min because for val_loss, lower is better\n",
    "    early_stopping_patience=20, # No. of epochs of degradation training will wait before terminating\n",
    "    checkpoints=\"valid_loss\", # Save best checkpoint monitoring val_loss\n",
    "    load_best=True, # After training, load the best checkpoint\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"Adam\",\n",
    "    optimizer_params={\n",
    "        \"weight_decay\": 0.001\n",
    "    },\n",
    "    lr_scheduler=\"ReduceLROnPlateau\",\n",
    "    lr_scheduler_params={\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.1,\n",
    "        \"patience\": 5,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    lr_scheduler_monitor_metric=\"valid_loss\"\n",
    ")\n",
    "\n",
    "model_config = GatedAdditiveTreeEnsembleConfig(\n",
    "    task=\"classification\",\n",
    "    tree_depth  =  5,\n",
    "    num_trees   =  12,\n",
    "    chain_trees = False, # akin to bagging, True is akin to boosting\n",
    "    gflu_stages =  2\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21d32c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af947a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7410984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_class_weight in module sklearn.utils.class_weight:\n",
      "\n",
      "compute_class_weight(class_weight, *, classes, y)\n",
      "    Estimate class weights for unbalanced datasets.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    class_weight : dict, 'balanced' or None\n",
      "        If 'balanced', class weights will be given by\n",
      "        ``n_samples / (n_classes * np.bincount(y))``.\n",
      "        If a dictionary is given, keys are classes and values\n",
      "        are corresponding class weights.\n",
      "        If None is given, the class weights will be uniform.\n",
      "    \n",
      "    classes : ndarray\n",
      "        Array of the classes occurring in the data, as given by\n",
      "        ``np.unique(y_org)`` with ``y_org`` the original class labels.\n",
      "    \n",
      "    y : array-like of shape (n_samples,)\n",
      "        Array of original class labels per sample.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    class_weight_vect : ndarray of shape (n_classes,)\n",
      "        Array with class_weight_vect[i] the weight for i-th class.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    The \"balanced\" heuristic is inspired by\n",
      "    Logistic Regression in Rare Events Data, King, Zen, 2001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(compute_class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "047190ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_class_weight() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_weights\u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m class_weights\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(class_weights,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_weights) \u001b[38;5;66;03m#([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_class_weight() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "class_weights= compute_class_weight('balanced',np.unique(y_train.values),y_train.values)\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    " \n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])\n",
    "#Then pass it to nn.CrossEntropyLoss's weight variable\n",
    "\n",
    "weighted_loss = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff8979c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted loss\n",
    "weighted_loss = get_class_weighted_cross_entropy(y_train.values.ravel(), mu =1.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7488c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 18:05:17,886 - {pytorch_tabular.tabular_model:102} - INFO - Experiment Tracking is turned off\n",
      "Global seed set to 42\n",
      "2023-07-13 18:05:17,913 - {pytorch_tabular.tabular_model:465} - INFO - Preparing the DataLoaders\n",
      "2023-07-13 18:05:17,916 - {pytorch_tabular.tabular_datamodule:286} - INFO - Setting up the datamodule for classification task\n",
      "2023-07-13 18:05:17,941 - {pytorch_tabular.tabular_model:508} - INFO - Preparing the Model: GatedAdditiveTreeEnsembleModel\n",
      "2023-07-13 18:05:18,073 - {pytorch_tabular.tabular_model:264} - INFO - Preparing the Trainer\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-07-13 18:05:18,142 - {pytorch_tabular.tabular_model:566} - INFO - Training Started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ GatedAdditiveTreesBackbone │ 28.1 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _embedding_layer │ Embedding1dLayer           │      8 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ CustomHead                 │    111 │\n",
       "└───┴──────────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GatedAdditiveTreesBackbone │ 28.1 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer           │      8 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ CustomHead                 │    111 │\n",
       "└───┴──────────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 28.2 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 28.2 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 28.2 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 28.2 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca359737e7e435f9a224a4956c7de1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 00138: reducing learning rate of group 0 to 1.0000e-04.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 00138: reducing learning rate of group 0 to 1.0000e-04.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 00149: reducing learning rate of group 0 to 1.0000e-05.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 00149: reducing learning rate of group 0 to 1.0000e-05.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 00156: reducing learning rate of group 0 to 1.0000e-06.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 00156: reducing learning rate of group 0 to 1.0000e-06.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 00168: reducing learning rate of group 0 to 1.0000e-07.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 00168: reducing learning rate of group 0 to 1.0000e-07.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 18:13:07,327 - {pytorch_tabular.tabular_model:568} - INFO - Training the model completed\n",
      "2023-07-13 18:13:07,328 - {pytorch_tabular.tabular_model:1207} - INFO - Loading the best model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x15d18c850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert X_train.isnull().sum().sum() == 0\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Merge X_train and y_train  \n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Merge X_val and y_val\n",
    "validation = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "tabular_model.fit(\n",
    "    train=train, \n",
    "    validation=validation,\n",
    "    loss=weighted_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ac69fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4280ab622d76482b9bcfa11255558091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = validation[\"target\"]\n",
    "y_pred = tabular_model.predict(validation)[\"prediction\"]\n",
    "evaluator = Evaluator(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                run_metrics=[\"mse\", \"f1\", \"accuracy\"],\n",
    "                metric=\"mse\",\n",
    "                problem_type=\"multiclass_classification\",\n",
    "            )\n",
    "output_metrics = evaluator.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a628fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0, 'accuracy': 1.0, 'f1': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb8fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
