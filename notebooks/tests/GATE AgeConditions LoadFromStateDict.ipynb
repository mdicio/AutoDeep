{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385dfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from evaluation.generalevaluator import *\n",
    "from modelsdefinition.GATE import GATE\n",
    "from factory import create_data_loader\n",
    "import pandas as pd\n",
    "\n",
    "# pip install pytorch_tabular[extra]\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,  # ExperimentConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")\n",
    "from pytorch_tabular.models import GatedAdditiveTreeEnsembleConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9cdee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_loader = create_data_loader('ageconditions', test_size=0.2, normalize_features = \"mean_std\", return_extra_info = True)\n",
    "X_train, X_val, y_train, y_val, extra_info = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b643ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 22:29:46,508 - {pytorch_tabular.tabular_model:129} - INFO - Experiment Tracking is turned off\n",
      "2023-07-16 22:29:46,512 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "/Users/mdicio/.pyenv/versions/3.10.0/envs/WTab/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:589: LightningDeprecationWarning: The Trainer argument `auto_select_gpus` has been deprecated in v1.9.0 and will be removed in v2.0.0. Please use the function `pytorch_lightning.accelerators.find_usable_cuda_devices` instead.\n",
      "  rank_zero_deprecation(\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = TabularModel.load_model(dir = \"../output/modelsaves/ageconditions/GATE/202307-1622-2744-6b41faa3-9de4-4bf3-9e73-62cac06161ec\", strict = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72a140",
   "metadata": {},
   "source": [
    "# GATE Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2c85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GatedAdditiveTreeEnsembleConfig in module pytorch_tabular.models.gate.config:\n",
      "\n",
      "class GatedAdditiveTreeEnsembleConfig(pytorch_tabular.config.config.ModelConfig)\n",
      " |  GatedAdditiveTreeEnsembleConfig(task: str, head: Optional[str] = 'LinearHead', head_config: Optional[Dict] = <factory>, embedding_dims: Optional[List] = None, embedding_dropout: float = 0.0, batch_norm_continuous_input: bool = True, learning_rate: float = 0.001, loss: Optional[str] = None, metrics: Optional[List[str]] = None, metrics_prob_input: Optional[List[bool]] = None, metrics_params: Optional[List] = None, target_range: Optional[List] = None, seed: int = 42, _module_src: str = 'models.gate', _model_name: str = 'GatedAdditiveTreeEnsembleModel', _backbone_name: str = 'GatedAdditiveTreesBackbone', _config_name: str = 'GatedAdditiveTreeEnsembleConfig', gflu_stages: int = 6, gflu_dropout: float = 0.0, tree_depth: int = 4, num_trees: int = 10, binning_activation: str = 'sparsemoid', feature_mask_function: str = 't-softmax', gflu_feature_init_sparsity: float = 0.3, tree_feature_init_sparsity: float = 0.8, learnable_sparsity: bool = True, tree_dropout: float = 0.0, chain_trees: bool = True, tree_wise_attention: bool = True, tree_wise_attention_dropout: float = 0.0, share_head_weights: bool = True) -> None\n",
      " |  \n",
      " |  Gated Additive Tree Ensemble Config.\n",
      " |  \n",
      " |  Args:\n",
      " |      gflu_stages (int): Number of layers in the feature abstraction layer. Defaults to 6\n",
      " |  \n",
      " |      gflu_dropout (float): Dropout rate for the feature abstraction layer. Defaults to 0.0\n",
      " |  \n",
      " |      tree_depth (int): Depth of the tree. Defaults to 5\n",
      " |  \n",
      " |      num_trees (int): Number of trees to use in the ensemble. Defaults to 20\n",
      " |  \n",
      " |      binning_activation (str): The binning function to use. Defaults to entmoid. Defaults to sparsemoid.\n",
      " |              Choices are: [`entmoid`,`sparsemoid`,`sigmoid`].\n",
      " |  \n",
      " |      feature_mask_function (str): The feature mask function to use. Defaults to sparsemax. Choices are:\n",
      " |              [`entmax`,`sparsemax`,`softmax`].\n",
      " |  \n",
      " |      tree_dropout (float): probability of dropout in tree binning transformation. Defaults to 0.0\n",
      " |  \n",
      " |      chain_trees (bool): If True, we will chain the trees together. Synonymous to boosting\n",
      " |          (chaining trees) or bagging (parallel trees). Defaults to True\n",
      " |  \n",
      " |      tree_wise_attention (bool): If True, we will use tree wise attention to combine trees. Defaults to\n",
      " |              True\n",
      " |  \n",
      " |      tree_wise_attention_dropout (float): probability of dropout in the tree wise attention layer.\n",
      " |              Defaults to 0.0\n",
      " |  \n",
      " |      share_head_weights (bool): If True, we will share the weights between the heads. Defaults to True\n",
      " |  \n",
      " |  \n",
      " |      task (str): Specify whether the problem is regression or classification. `backbone` is a task which\n",
      " |              considers the model as a backbone to generate features. Mostly used internally for SSL and related\n",
      " |              tasks.. Choices are: [`regression`,`classification`,`backbone`].\n",
      " |  \n",
      " |      head (Optional[str]): The head to be used for the model. Should be one of the heads defined in\n",
      " |              `pytorch_tabular.models.common.heads`. Defaults to  LinearHead. Choices are:\n",
      " |              [`None`,`LinearHead`,`MixtureDensityHead`].\n",
      " |  \n",
      " |      head_config (Optional[Dict]): The config as a dict which defines the head. If left empty, will be\n",
      " |              initialized as default linear head.\n",
      " |  \n",
      " |      embedding_dims (Optional[List]): The dimensions of the embedding for each categorical column as a\n",
      " |              list of tuples (cardinality, embedding_dim). If left empty, will infer using the cardinality of\n",
      " |              the categorical column using the rule min(50, (x + 1) // 2)\n",
      " |  \n",
      " |      embedding_dropout (float): Dropout to be applied to the Categorical Embedding. Defaults to 0.1\n",
      " |  \n",
      " |      batch_norm_continuous_input (bool): If True, we will normalize the continuous layer by passing it\n",
      " |              through a BatchNorm layer.\n",
      " |  \n",
      " |      learning_rate (float): The learning rate of the model. Defaults to 1e-3.\n",
      " |  \n",
      " |      loss (Optional[str]): The loss function to be applied. By Default it is MSELoss for regression and\n",
      " |              CrossEntropyLoss for classification. Unless you are sure what you are doing, leave it at MSELoss\n",
      " |              or L1Loss for regression and CrossEntropyLoss for classification\n",
      " |  \n",
      " |      metrics (Optional[List[str]]): the list of metrics you need to track during training. The metrics\n",
      " |              should be one of the functional metrics implemented in ``torchmetrics``. By default, it is\n",
      " |              accuracy if classification and mean_squared_error for regression\n",
      " |  \n",
      " |      metrics_params (Optional[List]): The parameters to be passed to the metrics function. `task` is forced to\n",
      " |              be `multiclass` because the multiclass version can handle binary as well and for simplicity we are\n",
      " |              only using `multiclass`.\n",
      " |  \n",
      " |      metrics_prob_input (Optional[List]): Is a mandatory parameter for classification metrics defined in the config.\n",
      " |          This defines whether the input to the metric function is the probability or the class. Length should be\n",
      " |          same as the number of metrics. Defaults to None.\n",
      " |  \n",
      " |      target_range (Optional[List]): The range in which we should limit the output variable. Currently\n",
      " |              ignored for multi-target regression. Typically used for Regression problems. If left empty, will\n",
      " |              not apply any restrictions\n",
      " |  \n",
      " |      seed (int): The seed for reproducibility. Defaults to 42\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GatedAdditiveTreeEnsembleConfig\n",
      " |      pytorch_tabular.config.config.ModelConfig\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, task: str, head: Optional[str] = 'LinearHead', head_config: Optional[Dict] = <factory>, embedding_dims: Optional[List] = None, embedding_dropout: float = 0.0, batch_norm_continuous_input: bool = True, learning_rate: float = 0.001, loss: Optional[str] = None, metrics: Optional[List[str]] = None, metrics_prob_input: Optional[List[bool]] = None, metrics_params: Optional[List] = None, target_range: Optional[List] = None, seed: int = 42, _module_src: str = 'models.gate', _model_name: str = 'GatedAdditiveTreeEnsembleModel', _backbone_name: str = 'GatedAdditiveTreesBackbone', _config_name: str = 'GatedAdditiveTreeEnsembleConfig', gflu_stages: int = 6, gflu_dropout: float = 0.0, tree_depth: int = 4, num_trees: int = 10, binning_activation: str = 'sparsemoid', feature_mask_function: str = 't-softmax', gflu_feature_init_sparsity: float = 0.3, tree_feature_init_sparsity: float = 0.8, learnable_sparsity: bool = True, tree_dropout: float = 0.0, chain_trees: bool = True, tree_wise_attention: bool = True, tree_wise_attention_dropout: float = 0.0, share_head_weights: bool = True) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_backbone_name': <class 'str'>, '_config_name': <c...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'_backbone_name': Field(name='_backbone_name',...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('task', 'head', 'head_config', 'embedding_dims', 'em...\n",
      " |  \n",
      " |  binning_activation = 'sparsemoid'\n",
      " |  \n",
      " |  chain_trees = True\n",
      " |  \n",
      " |  feature_mask_function = 't-softmax'\n",
      " |  \n",
      " |  gflu_dropout = 0.0\n",
      " |  \n",
      " |  gflu_feature_init_sparsity = 0.3\n",
      " |  \n",
      " |  gflu_stages = 6\n",
      " |  \n",
      " |  learnable_sparsity = True\n",
      " |  \n",
      " |  num_trees = 10\n",
      " |  \n",
      " |  share_head_weights = True\n",
      " |  \n",
      " |  tree_depth = 4\n",
      " |  \n",
      " |  tree_dropout = 0.0\n",
      " |  \n",
      " |  tree_feature_init_sparsity = 0.8\n",
      " |  \n",
      " |  tree_wise_attention = True\n",
      " |  \n",
      " |  tree_wise_attention_dropout = 0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_tabular.config.config.ModelConfig:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_tabular.config.config.ModelConfig:\n",
      " |  \n",
      " |  batch_norm_continuous_input = True\n",
      " |  \n",
      " |  embedding_dims = None\n",
      " |  \n",
      " |  embedding_dropout = 0.0\n",
      " |  \n",
      " |  head = 'LinearHead'\n",
      " |  \n",
      " |  learning_rate = 0.001\n",
      " |  \n",
      " |  loss = None\n",
      " |  \n",
      " |  metrics = None\n",
      " |  \n",
      " |  metrics_params = None\n",
      " |  \n",
      " |  metrics_prob_input = None\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  target_range = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GatedAdditiveTreeEnsembleConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14a70a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OptimizerConfig in module pytorch_tabular.config.config:\n",
      "\n",
      "class OptimizerConfig(builtins.object)\n",
      " |  OptimizerConfig(optimizer: str = 'Adam', optimizer_params: Dict = <factory>, lr_scheduler: Optional[str] = None, lr_scheduler_params: Optional[Dict] = <factory>, lr_scheduler_monitor_metric: Optional[str] = 'valid_loss') -> None\n",
      " |  \n",
      " |  Optimizer and Learning Rate Scheduler configuration.\n",
      " |  Args:\n",
      " |      optimizer (str): Any of the standard optimizers from\n",
      " |              [torch.optim](https://pytorch.org/docs/stable/optim.html#algorithms).\n",
      " |  \n",
      " |      optimizer_params (Dict): The parameters for the optimizer. If left blank, will use default\n",
      " |              parameters.\n",
      " |  \n",
      " |      lr_scheduler (Optional[str]): The name of the LearningRateScheduler to use, if any, from\n",
      " |              [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-\n",
      " |              rate). If None, will not use any scheduler. Defaults to `None`\n",
      " |  \n",
      " |      lr_scheduler_params (Optional[Dict]): The parameters for the LearningRateScheduler. If left blank,\n",
      " |              will use default parameters.\n",
      " |  \n",
      " |      lr_scheduler_monitor_metric (Optional[str]): Used with ReduceLROnPlateau, where the plateau is\n",
      " |              decided based on this metric\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, optimizer: str = 'Adam', optimizer_params: Dict = <factory>, lr_scheduler: Optional[str] = None, lr_scheduler_params: Optional[Dict] = <factory>, lr_scheduler_monitor_metric: Optional[str] = 'valid_loss') -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  read_from_yaml(filename: str = 'config/optimizer_config.yml')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'lr_scheduler': typing.Optional[str], 'lr_scheduler...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'lr_scheduler': Field(name='lr_scheduler',type...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('optimizer', 'optimizer_params', 'lr_scheduler', 'lr...\n",
      " |  \n",
      " |  lr_scheduler = None\n",
      " |  \n",
      " |  lr_scheduler_monitor_metric = 'valid_loss'\n",
      " |  \n",
      " |  optimizer = 'Adam'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OptimizerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635cd7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainerConfig in module pytorch_tabular.config.config:\n",
      "\n",
      "class TrainerConfig(builtins.object)\n",
      " |  TrainerConfig(batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |  \n",
      " |  Trainer configuration\n",
      " |  Args:\n",
      " |      batch_size (int): Number of samples in each batch of training\n",
      " |  \n",
      " |      data_aware_init_batch_size (int): Number of samples in each batch of training for the data-aware initialization,\n",
      " |          when applicable. Defaults to 2000\n",
      " |  \n",
      " |      fast_dev_run (bool): runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val\n",
      " |              and test to find any bugs (ie: a sort of unit test).\n",
      " |  \n",
      " |      max_epochs (int): Maximum number of epochs to be run\n",
      " |  \n",
      " |      min_epochs (Optional[int]): Force training for at least these many epochs. 1 by default\n",
      " |  \n",
      " |      max_time (Optional[int]): Stop training after this amount of time has passed. Disabled by default\n",
      " |              (None)\n",
      " |  \n",
      " |      gpus (Optional[int]): DEPRECATED: Number of gpus to train on (int). -1 uses all available GPUs. By\n",
      " |              default uses CPU (None)\n",
      " |  \n",
      " |      accelerator (Optional[str]): The accelerator to use for training. Can be one of\n",
      " |              'cpu','gpu','tpu','ipu', 'mps', 'auto'. Defaults to 'auto'.\n",
      " |              Choices are: [`cpu`,`gpu`,`tpu`,`ipu`,'mps',`auto`].\n",
      " |  \n",
      " |      devices (Optional[int]): Number of devices to train on (int). -1 uses all available devices. By\n",
      " |              default uses all available devices (-1)\n",
      " |  \n",
      " |      devices_list (Optional[List[int]]): List of devices to train on (list). If specified, takes\n",
      " |              precedence over `devices` argument. Defaults to None\n",
      " |  \n",
      " |      accumulate_grad_batches (int): Accumulates grads every k batches or as set up in the dict. Trainer\n",
      " |              also calls optimizer.step() for the last indivisible step number.\n",
      " |  \n",
      " |      auto_lr_find (bool): Runs a learning rate finder algorithm (see this paper) when calling\n",
      " |              trainer.tune(), to find optimal initial learning rate.\n",
      " |  \n",
      " |      auto_select_gpus (bool): If enabled and `devices` is an integer, pick available gpus automatically.\n",
      " |              This is especially useful when GPUs are configured to be in 'exclusive mode', such that only one\n",
      " |              process at a time can access them.\n",
      " |  \n",
      " |      check_val_every_n_epoch (int): Check val every n train epochs.\n",
      " |  \n",
      " |      gradient_clip_val (float): Gradient clipping value\n",
      " |  \n",
      " |      overfit_batches (float): Uses this much data of the training set. If nonzero, will use the same\n",
      " |              training set for validation and testing. If the training dataloaders have shuffle=True, Lightning\n",
      " |              will automatically disable it. Useful for quickly debugging or trying to overfit on purpose.\n",
      " |  \n",
      " |      deterministic (bool): If true enables cudnn.deterministic. Might make your system slower, but\n",
      " |              ensures reproducibility.\n",
      " |  \n",
      " |      profiler (Optional[str]): To profile individual steps during training and assist in identifying\n",
      " |              bottlenecks. None, simple or advanced, pytorch. Choices are:\n",
      " |              [`None`,`simple`,`advanced`,`pytorch`].\n",
      " |  \n",
      " |      early_stopping (Optional[str]): The loss/metric that needed to be monitored for early stopping. If\n",
      " |              None, there will be no early stopping\n",
      " |  \n",
      " |      early_stopping_min_delta (float): The minimum delta in the loss/metric which qualifies as an\n",
      " |              improvement in early stopping\n",
      " |  \n",
      " |      early_stopping_mode (str): The direction in which the loss/metric should be optimized. Choices are:\n",
      " |              [`max`,`min`].\n",
      " |  \n",
      " |      early_stopping_patience (int): The number of epochs to wait until there is no further improvements\n",
      " |              in loss/metric\n",
      " |  \n",
      " |      early_stopping_kwargs (Optional[Dict]): Additional keyword arguments for the early stopping callback.\n",
      " |              See the documentation for the PyTorch Lightning EarlyStopping callback for more details.\n",
      " |  \n",
      " |      checkpoints (Optional[str]): The loss/metric that needed to be monitored for checkpoints. If None,\n",
      " |              there will be no checkpoints\n",
      " |  \n",
      " |      checkpoints_path (str): The path where the saved models will be\n",
      " |  \n",
      " |      checkpoints_every_n_epochs (int): Number of training steps between checkpoints\n",
      " |  \n",
      " |      checkpoints_name (Optional[str]): The name under which the models will be saved. If left blank,\n",
      " |              first it will look for `run_name` in experiment_config and if that is also None then it will use a\n",
      " |              generic name like task_version.\n",
      " |  \n",
      " |      checkpoints_mode (str): The direction in which the loss/metric should be optimized\n",
      " |  \n",
      " |      checkpoints_save_top_k (int): The number of best models to save\n",
      " |  \n",
      " |      checkpoints_kwargs (Optional[Dict]): Additional keyword arguments for the checkpoints callback.\n",
      " |              See the documentation for the PyTorch Lightning ModelCheckpoint callback for more details.\n",
      " |  \n",
      " |      load_best (bool): Flag to load the best model saved during training\n",
      " |  \n",
      " |      track_grad_norm (int): Track and Log Gradient Norms in the logger. -1 by default means no tracking.\n",
      " |              1 for the L1 norm, 2 for L2 norm, etc.\n",
      " |  \n",
      " |      progress_bar (str): Progress bar type. Can be one of: `none`, `simple`, `rich`. Defaults to `rich`.\n",
      " |  \n",
      " |      precision (int): Precision of the model. Can be one of: `32`, `16`, `64`. Defaults to `32`..\n",
      " |              Choices are: [`32`,`16`,`64`].\n",
      " |  \n",
      " |      seed (int): Seed for random number generators. Defaults to 42\n",
      " |  \n",
      " |      trainer_kwargs (Dict[str, Any]): Additional kwargs to be passed to PyTorch Lightning Trainer. See\n",
      " |              https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.trainer.html#pytorch_lightning.trainer.Trainer\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __init__(self, batch_size: int = 64, data_aware_init_batch_size: int = 2000, fast_dev_run: bool = False, max_epochs: int = 10, min_epochs: Optional[int] = 1, max_time: Optional[int] = None, gpus: Optional[int] = None, accelerator: Optional[str] = 'auto', devices: Optional[int] = None, devices_list: Optional[List[int]] = None, accumulate_grad_batches: int = 1, auto_lr_find: bool = False, auto_select_gpus: bool = True, check_val_every_n_epoch: int = 1, gradient_clip_val: float = 0.0, overfit_batches: float = 0.0, deterministic: bool = False, profiler: Optional[str] = None, early_stopping: Optional[str] = 'valid_loss', early_stopping_min_delta: float = 0.001, early_stopping_mode: str = 'min', early_stopping_patience: int = 3, early_stopping_kwargs: Optional[Dict[str, Any]] = <factory>, checkpoints: Optional[str] = 'valid_loss', checkpoints_path: str = 'saved_models', checkpoints_every_n_epochs: int = 1, checkpoints_name: Optional[str] = None, checkpoints_mode: str = 'min', checkpoints_save_top_k: int = 1, checkpoints_kwargs: Optional[Dict[str, Any]] = <factory>, load_best: bool = True, track_grad_norm: int = -1, progress_bar: str = 'rich', precision: int = 32, seed: int = 42, trainer_kwargs: Dict[str, Any] = <factory>) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'accelerator': typing.Optional[str], 'accumulate_gr...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'accelerator': Field(name='accelerator',type=t...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __match_args__ = ('batch_size', 'data_aware_init_batch_size', 'fast_de...\n",
      " |  \n",
      " |  accelerator = 'auto'\n",
      " |  \n",
      " |  accumulate_grad_batches = 1\n",
      " |  \n",
      " |  auto_lr_find = False\n",
      " |  \n",
      " |  auto_select_gpus = True\n",
      " |  \n",
      " |  batch_size = 64\n",
      " |  \n",
      " |  check_val_every_n_epoch = 1\n",
      " |  \n",
      " |  checkpoints = 'valid_loss'\n",
      " |  \n",
      " |  checkpoints_every_n_epochs = 1\n",
      " |  \n",
      " |  checkpoints_mode = 'min'\n",
      " |  \n",
      " |  checkpoints_name = None\n",
      " |  \n",
      " |  checkpoints_path = 'saved_models'\n",
      " |  \n",
      " |  checkpoints_save_top_k = 1\n",
      " |  \n",
      " |  data_aware_init_batch_size = 2000\n",
      " |  \n",
      " |  deterministic = False\n",
      " |  \n",
      " |  devices = None\n",
      " |  \n",
      " |  devices_list = None\n",
      " |  \n",
      " |  early_stopping = 'valid_loss'\n",
      " |  \n",
      " |  early_stopping_min_delta = 0.001\n",
      " |  \n",
      " |  early_stopping_mode = 'min'\n",
      " |  \n",
      " |  early_stopping_patience = 3\n",
      " |  \n",
      " |  fast_dev_run = False\n",
      " |  \n",
      " |  gpus = None\n",
      " |  \n",
      " |  gradient_clip_val = 0.0\n",
      " |  \n",
      " |  load_best = True\n",
      " |  \n",
      " |  max_epochs = 10\n",
      " |  \n",
      " |  max_time = None\n",
      " |  \n",
      " |  min_epochs = 1\n",
      " |  \n",
      " |  overfit_batches = 0.0\n",
      " |  \n",
      " |  precision = 32\n",
      " |  \n",
      " |  profiler = None\n",
      " |  \n",
      " |  progress_bar = 'rich'\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  track_grad_norm = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TrainerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbfec264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">011</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:07\u001b[0m,\u001b[1;36m011\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m105\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "data_config = DataConfig(\n",
    "    target=['target'],\n",
    "    continuous_cols= [i for i in extra_info[\"num_col_names\"] if i != \"target\"],\n",
    "    categorical_cols=extra_info[\"cat_col_names\"],\n",
    "    #num_workers = 4\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=100,\n",
    "    max_epochs=10,\n",
    "    early_stopping=\"valid_accuracy\", # Monitor valid_loss for early stopping\n",
    "    early_stopping_mode = \"min\", # Set the mode as min because for val_loss, lower is better\n",
    "    early_stopping_patience=2, # No. of epochs of degradation training will wait before terminating\n",
    "    checkpoints=\"valid_loss\", # Save best checkpoint monitoring val_loss\n",
    "    load_best=True, # After training, load the best checkpoint\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"Adam\",\n",
    "    optimizer_params={\n",
    "        \"weight_decay\": 0.001,\n",
    "        #\"lr\":0.001\n",
    "    },\n",
    "    lr_scheduler=\"ReduceLROnPlateau\",\n",
    "    lr_scheduler_params={\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.1,\n",
    "        \"patience\": 5,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    lr_scheduler_monitor_metric=\"valid_loss\"\n",
    ")\n",
    "\n",
    "model_config = GatedAdditiveTreeEnsembleConfig(\n",
    "    task=\"classification\",\n",
    "    tree_depth  =  5,\n",
    "    num_trees   =  5,\n",
    "    chain_trees = False, # akin to bagging, True is akin to boosting\n",
    "    gflu_stages =  2,\n",
    "    learning_rate = 0.001,\n",
    "    metrics=['accuracy', \"auroc\"],\n",
    "    metrics_params=[dict(task=\"multiclass\", num_classes=2), dict(task=\"multiclass\", num_classes=2)]\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff8979c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted loss\n",
    "from pytorch_tabular.utils import get_class_weighted_cross_entropy\n",
    "weighted_loss = get_class_weighted_cross_entropy(y_train.values.ravel(), mu =1.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61110fe7-de63-4681-85e0-2c70e13db45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, 56)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea9b27a4-b4f2-4740-b0d6-724dfde3d80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99     0\n",
       "256    1\n",
       "577    0\n",
       "76     1\n",
       "599    0\n",
       "      ..\n",
       "200    0\n",
       "450    0\n",
       "606    0\n",
       "149    0\n",
       "471    1\n",
       "Name: target, Length: 493, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7488c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">105</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m028\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m105\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">059</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">473</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m059\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m473\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">062</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:290</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m062\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:290\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">080</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">521</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m080\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m521\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">268</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m184\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m268\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto select gpus: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:45:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">215</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">582</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2023\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m13:45:09\u001b[0m,\u001b[1;36m215\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m582\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ GatedAdditiveTreesBackbone │ 94.4 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _embedding_layer │ Embedding1dLayer           │    112 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ CustomHead                 │     71 │\n",
       "└───┴──────────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ GatedAdditiveTreesBackbone │ 94.4 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer           │    112 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ CustomHead                 │     71 │\n",
       "└───┴──────────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 94.5 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 94.5 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 94.5 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 94.5 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884635132d6e4de4a0cf6237c110b1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected `preds` to have one more dimension than `target` but got 1 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Merge X_val and y_val\u001b[39;00m\n\u001b[1;32m     12\u001b[0m validation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_val, y_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtabular_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:694\u001b[0m, in \u001b[0;36mTabularModel.fit\u001b[0;34m(self, train, validation, test, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule)\u001b[0m\n\u001b[1;32m    681\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProviding test data in `fit` is deprecated and will be removed in next major release.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Plese use `evaluate` for evaluating on test data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         )\n\u001b[1;32m    685\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_model(\n\u001b[1;32m    686\u001b[0m     datamodule,\n\u001b[1;32m    687\u001b[0m     loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m     optimizer_params,\n\u001b[1;32m    692\u001b[0m )\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/tabular_model.py:583\u001b[0m, in \u001b[0;36mTabularModel.train\u001b[0;34m(self, model, datamodule, callbacks, max_epochs, min_epochs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    582\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mload_best:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1276\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:446\u001b[0m, in \u001b[0;36mBaseModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    444\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(output, y, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat, y\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/pytorch_tabular/models/base_model.py:311\u001b[0m, in \u001b[0;36mBaseModel.calculate_metrics\u001b[0;34m(self, y, y_hat, tag)\u001b[0m\n\u001b[1;32m    309\u001b[0m         avg_metric \u001b[38;5;241m=\u001b[39m metric(y_hat, y\u001b[38;5;241m.\u001b[39msqueeze(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_params)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m         avg_metric \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(avg_metric)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    315\u001b[0m     avg_metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    320\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/torchmetrics/functional/classification/auroc.py:457\u001b[0m, in \u001b[0;36mauroc\u001b[0;34m(preds, target, task, thresholds, num_classes, num_labels, average, max_fpr, ignore_index, validate_args)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmulticlass_auroc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_labels, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/torchmetrics/functional/classification/auroc.py:280\u001b[0m, in \u001b[0;36mmulticlass_auroc\u001b[0;34m(preds, target, num_classes, average, thresholds, ignore_index, validate_args)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[1;32m    279\u001b[0m     _multiclass_auroc_arg_validation(num_classes, average, thresholds, ignore_index)\n\u001b[0;32m--> 280\u001b[0m     \u001b[43m_multiclass_precision_recall_curve_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m preds, target, thresholds \u001b[38;5;241m=\u001b[39m _multiclass_precision_recall_curve_format(\n\u001b[1;32m    282\u001b[0m     preds, target, num_classes, thresholds, ignore_index\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m state \u001b[38;5;241m=\u001b[39m _multiclass_precision_recall_curve_update(preds, target, num_classes, thresholds)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/thesis/lib/python3.10/site-packages/torchmetrics/functional/classification/precision_recall_curve.py:340\u001b[0m, in \u001b[0;36m_multiclass_precision_recall_curve_tensor_validation\u001b[0;34m(preds, target, num_classes, ignore_index)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate tensor input.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m- target should have one more dimension than preds and all dimensions except for preds.shape[1] should match\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mexactly. preds.shape[1] should have size equal to number of classes\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m- all values in target tensor that are not ignored have to be in {0, 1}\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `preds` to have one more dimension than `target` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected argument `target` to be an int or long tensor, but got tensor with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected `preds` to have one more dimension than `target` but got 1 and 1"
     ]
    }
   ],
   "source": [
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Merge X_train and y_train  \n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Merge X_val and y_val\n",
    "validation = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "tabular_model.fit(\n",
    "    train=train, \n",
    "    validation=validation,\n",
    "    loss=weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac69342b-f383-48b2-afb4-be1e943ef1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.632923</td>\n",
       "      <td>-0.949501</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.354858</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>0.083184</td>\n",
       "      <td>-0.030176</td>\n",
       "      <td>-0.258262</td>\n",
       "      <td>-0.039161</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438346</td>\n",
       "      <td>-0.036026</td>\n",
       "      <td>0.194507</td>\n",
       "      <td>-0.106732</td>\n",
       "      <td>1.054971</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>-0.481870</td>\n",
       "      <td>1.424759</td>\n",
       "      <td>1.316702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.110749</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>-0.062110</td>\n",
       "      <td>-0.454650</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>0.148114</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>-0.416150</td>\n",
       "      <td>0.076005</td>\n",
       "      <td>-0.831181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438346</td>\n",
       "      <td>-0.063017</td>\n",
       "      <td>-0.185328</td>\n",
       "      <td>-0.521587</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>-0.714218</td>\n",
       "      <td>-0.799016</td>\n",
       "      <td>-1.143319</td>\n",
       "      <td>1.316702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>-0.537802</td>\n",
       "      <td>-0.476063</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.293009</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>-0.767734</td>\n",
       "      <td>0.098212</td>\n",
       "      <td>-0.691327</td>\n",
       "      <td>-0.071581</td>\n",
       "      <td>-0.446374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438346</td>\n",
       "      <td>-0.063017</td>\n",
       "      <td>-0.255668</td>\n",
       "      <td>-0.920874</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>-0.163882</td>\n",
       "      <td>0.413069</td>\n",
       "      <td>-0.676792</td>\n",
       "      <td>1.316702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.326933</td>\n",
       "      <td>1.097547</td>\n",
       "      <td>-0.130946</td>\n",
       "      <td>3.038008</td>\n",
       "      <td>2.391755</td>\n",
       "      <td>-0.487512</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>0.152247</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>-0.135086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.073151</td>\n",
       "      <td>-0.057340</td>\n",
       "      <td>-0.213464</td>\n",
       "      <td>-0.444661</td>\n",
       "      <td>-0.143927</td>\n",
       "      <td>-0.741026</td>\n",
       "      <td>-0.242316</td>\n",
       "      <td>2.795991</td>\n",
       "      <td>-0.821683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.260848</td>\n",
       "      <td>-0.085977</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.100712</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>1.026370</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>-0.156011</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>0.383064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416992</td>\n",
       "      <td>-0.051529</td>\n",
       "      <td>0.883837</td>\n",
       "      <td>-0.011489</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>-0.026810</td>\n",
       "      <td>1.177307</td>\n",
       "      <td>-0.804475</td>\n",
       "      <td>-0.669534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.312971</td>\n",
       "      <td>0.905750</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>-0.341811</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>-0.183368</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>1.110102</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>0.710945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438346</td>\n",
       "      <td>-0.052274</td>\n",
       "      <td>-0.255668</td>\n",
       "      <td>0.342008</td>\n",
       "      <td>0.060801</td>\n",
       "      <td>-0.465036</td>\n",
       "      <td>-0.315387</td>\n",
       "      <td>-0.137747</td>\n",
       "      <td>1.316702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>-0.468624</td>\n",
       "      <td>1.558569</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.457458</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>-0.883923</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>-1.728878</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>-0.980172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438346</td>\n",
       "      <td>-0.052150</td>\n",
       "      <td>-0.255668</td>\n",
       "      <td>-1.236824</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.517403</td>\n",
       "      <td>-1.380618</td>\n",
       "      <td>1.316702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>-0.010314</td>\n",
       "      <td>-0.197598</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.133638</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>-0.870254</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>0.504112</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>0.108919</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061630</td>\n",
       "      <td>-0.048342</td>\n",
       "      <td>-0.068095</td>\n",
       "      <td>0.690926</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>0.263109</td>\n",
       "      <td>-1.005424</td>\n",
       "      <td>-1.129180</td>\n",
       "      <td>-0.796277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>-0.615629</td>\n",
       "      <td>1.832286</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.274264</td>\n",
       "      <td>-0.201277</td>\n",
       "      <td>0.011420</td>\n",
       "      <td>-0.082579</td>\n",
       "      <td>0.347728</td>\n",
       "      <td>-0.103421</td>\n",
       "      <td>-0.098937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217110</td>\n",
       "      <td>-0.046168</td>\n",
       "      <td>0.142924</td>\n",
       "      <td>-0.114974</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>-0.761871</td>\n",
       "      <td>-0.587711</td>\n",
       "      <td>1.771031</td>\n",
       "      <td>-0.764117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>-0.442681</td>\n",
       "      <td>-1.029051</td>\n",
       "      <td>-0.263144</td>\n",
       "      <td>-0.393338</td>\n",
       "      <td>0.038605</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>-1.728878</td>\n",
       "      <td>-0.077485</td>\n",
       "      <td>-0.162788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104392</td>\n",
       "      <td>-0.051617</td>\n",
       "      <td>-0.255668</td>\n",
       "      <td>0.196396</td>\n",
       "      <td>-0.394262</td>\n",
       "      <td>-0.524485</td>\n",
       "      <td>-0.831408</td>\n",
       "      <td>-0.113431</td>\n",
       "      <td>-0.815841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>493 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AB        AF        AH        AM        AR        AX        AY  \\\n",
       "99  -0.632923 -0.949501 -0.263144 -0.354858 -0.201277  0.083184 -0.030176   \n",
       "256  0.110749  0.717663 -0.062110 -0.454650 -0.201277  0.148114  0.019607   \n",
       "577 -0.537802 -0.476063 -0.263144 -0.293009 -0.201277 -0.767734  0.098212   \n",
       "76   0.326933  1.097547 -0.130946  3.038008  2.391755 -0.487512 -0.082579   \n",
       "599  1.260848 -0.085977  0.025847  0.100712 -0.201277  1.026370 -0.082579   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "200 -0.312971  0.905750  0.238654 -0.341811 -0.201277 -0.183368 -0.082579   \n",
       "450 -0.468624  1.558569 -0.263144 -0.457458 -0.201277 -0.883923 -0.082579   \n",
       "606 -0.010314 -0.197598 -0.263144 -0.133638 -0.201277 -0.870254 -0.082579   \n",
       "149 -0.615629  1.832286 -0.263144 -0.274264 -0.201277  0.011420 -0.082579   \n",
       "471 -0.442681 -1.029051 -0.263144 -0.393338  0.038605  0.042176  0.023538   \n",
       "\n",
       "           AZ        BC       BD   ...        FL        FR        FS  \\\n",
       "99  -0.258262 -0.039161  0.013384  ... -0.438346 -0.036026  0.194507   \n",
       "256 -0.416150  0.076005 -0.831181  ... -0.438346 -0.063017 -0.185328   \n",
       "577 -0.691327 -0.071581 -0.446374  ... -0.438346 -0.063017 -0.255668   \n",
       "76   0.152247 -0.103421 -0.135086  ...  1.073151 -0.057340 -0.213464   \n",
       "599 -0.156011 -0.103421  0.383064  ... -0.416992 -0.051529  0.883837   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "200  1.110102 -0.103421  0.710945  ... -0.438346 -0.052274 -0.255668   \n",
       "450 -1.728878 -0.103421 -0.980172  ... -0.438346 -0.052150 -0.255668   \n",
       "606  0.504112 -0.103421  0.108919  ... -0.061630 -0.048342 -0.068095   \n",
       "149  0.347728 -0.103421 -0.098937  ... -0.217110 -0.046168  0.142924   \n",
       "471 -1.728878 -0.077485 -0.162788  ...  2.104392 -0.051617 -0.255668   \n",
       "\n",
       "           GB        GE        GF        GH        GI        GL  target  \n",
       "99  -0.106732  1.054971  0.004009 -0.481870  1.424759  1.316702       0  \n",
       "256 -0.521587 -0.394262 -0.714218 -0.799016 -1.143319  1.316702       1  \n",
       "577 -0.920874 -0.394262 -0.163882  0.413069 -0.676792  1.316702       0  \n",
       "76  -0.444661 -0.143927 -0.741026 -0.242316  2.795991 -0.821683       1  \n",
       "599 -0.011489 -0.394262 -0.026810  1.177307 -0.804475 -0.669534       0  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "200  0.342008  0.060801 -0.465036 -0.315387 -0.137747  1.316702       0  \n",
       "450 -1.236824 -0.394262  0.023072  0.517403 -1.380618  1.316702       0  \n",
       "606  0.690926 -0.394262  0.263109 -1.005424 -1.129180 -0.796277       0  \n",
       "149 -0.114974  0.078609 -0.761871 -0.587711  1.771031 -0.764117       0  \n",
       "471  0.196396 -0.394262 -0.524485 -0.831408 -0.113431 -0.815841       1  \n",
       "\n",
       "[493 rows x 57 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ac69fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4280ab622d76482b9bcfa11255558091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = validation[\"target\"]\n",
    "y_pred = tabular_model.predict(validation)[\"prediction\"]\n",
    "evaluator = Evaluator(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                run_metrics=[\"mse\", \"f1\", \"accuracy\"],\n",
    "                metric=\"mse\",\n",
    "                problem_type=\"multiclass_classification\",\n",
    "            )\n",
    "output_metrics = evaluator.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a628fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 0.0, 'accuracy': 1.0, 'f1': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb8fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
