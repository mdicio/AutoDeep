{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48825b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, space_eval, tpe\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from evaluation.generalevaluator import *\n",
    "from modelutils.trainingutilities import (\n",
    "    infer_hyperopt_space_s1dcnn,\n",
    "    stop_on_perfect_lossCondition,\n",
    ")\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size=4196):\n",
    "        super(Model, self).__init__()\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "        self.num_targets = num_targets\n",
    "        cha_1_reshape = int(hidden_size / cha_1)\n",
    "        cha_po_1 = int(hidden_size / cha_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / cha_1 / 2 / 2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.dropout_c1 = nn.Dropout(0.1)\n",
    "        self.conv1 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(cha_1, cha_2, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            dim=None,\n",
    "        )\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=cha_po_1)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2 = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(cha_2, cha_2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            dim=None,\n",
    "        )\n",
    "\n",
    "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "        self.conv2_1 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(cha_2, cha_2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            dim=None,\n",
    "        )\n",
    "\n",
    "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "        self.conv2_2 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(cha_2, cha_3, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            dim=None,\n",
    "        )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c2_1(x)\n",
    "        x = self.dropout_c2_1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "\n",
    "        x = self.batch_norm_c2_2(x)\n",
    "        x = self.dropout_c2_2(x)\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = x * x_s\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftOrdering1DCNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem_type=\"binary_classification\",\n",
    "        num_targets=1,\n",
    "        **params,\n",
    "    ):\n",
    "        self.num_features = 42\n",
    "        self.hidden_size = 4096\n",
    "        self.problem_type = problem_type\n",
    "        self.num_targets = num_targets\n",
    "\n",
    "        self.batch_size = 512\n",
    "        self.save_path = None\n",
    "        self.device = params.get(\n",
    "            \"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.transformation = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        if not any(\n",
    "            isinstance(handler, logging.StreamHandler)\n",
    "            for handler in self.logger.handlers\n",
    "        ):\n",
    "            self.logger.addHandler(console_handler)\n",
    "\n",
    "        self.random_state = 4200\n",
    "\n",
    "    def load_best_model(self):\n",
    "        \"\"\"Load a trained model from a given path\"\"\"\n",
    "        self.logger.info(f\"Loading model\")\n",
    "        self.logger.debug(\"Model loaded successfully\")\n",
    "        self.model = self.best_model\n",
    "\n",
    "    def build_model(self, num_features, num_targets, hidden_size):\n",
    "        model = Model(num_features, num_targets, hidden_size)\n",
    "        return model\n",
    "\n",
    "    def process_inputs_labels(self, inputs, labels):\n",
    "        inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "        if self.problem_type == \"binary_classification\":\n",
    "            outputs = torch.sigmoid(self.model(inputs)).reshape(-1)\n",
    "            labels = labels.float()\n",
    "        elif self.problem_type == \"regression\":\n",
    "            outputs = self.model(inputs).reshape(-1)\n",
    "            labels = labels.float()\n",
    "        elif self.problem_type == \"multiclass_classification\":\n",
    "            labels = labels.long()\n",
    "            outputs = self.model(inputs)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid problem_type. Supported options: binary_classification, multiclass_classification\"\n",
    "            )\n",
    "\n",
    "        return outputs, labels\n",
    "\n",
    "    def train_step(self, train_loader):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            if len(labels) < 2:\n",
    "                continue\n",
    "            # print(\"inputs, labels\", inputs.shape, labels.shape)\n",
    "            outputs, labels = self.process_inputs_labels(inputs, labels)\n",
    "\n",
    "            # print(\"inputs, labels, outputs\", inputs.shape, labels.shape, outputs.shape)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(train_loader)\n",
    "\n",
    "    def validate_step(self, validation_loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                if len(labels) < 2:\n",
    "                    continue\n",
    "                outputs, labels = self.process_inputs_labels(inputs, labels)\n",
    "\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        return val_loss / total_samples\n",
    "\n",
    "    def _set_loss_function(self, y_train):\n",
    "        if self.problem_type == \"binary_classification\":\n",
    "            num_positives = y_train.sum()\n",
    "            num_negatives = len(y_train) - num_positives\n",
    "            pos_weight = torch.tensor(num_negatives / num_positives)\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        elif self.problem_type == \"multiclass_classification\":\n",
    "            y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).flatten()\n",
    "            classes = torch.unique(y_train_tensor)\n",
    "            class_weights = compute_class_weight(\n",
    "                \"balanced\", classes=np.array(classes), y=y_train.values\n",
    "            )\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            )\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, reduction=\"mean\")\n",
    "        elif self.problem_type == \"regression\":\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid problem_type. Supported values are 'binary', 'multiclass', and 'regression'.\"\n",
    "            )\n",
    "\n",
    "    def _pandas_to_torch_dataloaders(\n",
    "        self, X_train, y_train, batch_size, validation_split\n",
    "    ):\n",
    "        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
    "        y_train_tensor = torch.tensor(\n",
    "            y_train.values,\n",
    "            dtype=torch.float if self.problem_type == \"regression\" else torch.long,\n",
    "        )\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "        num_samples = len(dataset)\n",
    "\n",
    "        num_train_samples = int((1 - validation_split) * num_samples)\n",
    "        num_val_samples = num_samples - num_train_samples\n",
    "\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, [num_train_samples, num_val_samples]\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, drop_last=False\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load a trained model from a given path\"\"\"\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "    def save_model(self, model_dir, model_name):\n",
    "        \"\"\"Save the trained model to a given directory with the specified name\"\"\"\n",
    "        save_path = os.path.join(model_dir, model_name)\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        print(f\"Model saved successfully at {save_path}\")\n",
    "\n",
    "    def train(self, X_train, y_train, params={}, extra_info=None):\n",
    "        outer_params = params[\"outer_params\"]\n",
    "        validation_fraction = params.get(\"validation_fraction\", 0.2)\n",
    "        num_epochs = outer_params.get(\"num_epochs\", 3)\n",
    "        batch_size = params.get(\"batch_size\", 32)\n",
    "        validation_split = params.get(\"validation_fraction\", 0.2)\n",
    "        early_stopping = outer_params.get(\"early_stopping\", True)\n",
    "        patience = params.get(\"early_stopping_patience\", 5)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_features = extra_info[\"num_features\"]\n",
    "        self.hidden_size = params.get(\"hidden_size\", 4096)\n",
    "\n",
    "        self.model = self.build_model(\n",
    "            self.num_features, self.num_targets, self.hidden_size\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=params[\"learning_rate\"])\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=params[\"scheduler_factor\"],\n",
    "            patience=params[\"scheduler_patience\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "        self._set_loss_function(y_train)\n",
    "\n",
    "        train_loader, val_loader = self._pandas_to_torch_dataloaders(\n",
    "            X_train, y_train, batch_size, validation_split\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_epoch = 0\n",
    "        current_patience = 0\n",
    "\n",
    "        with tqdm(total=num_epochs, desc=\"Training\", unit=\"epoch\", ncols=80) as pbar:\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_loss = self.train_step(train_loader)\n",
    "\n",
    "                if early_stopping and validation_fraction > 0:\n",
    "                    val_loss = self.validate_step(val_loader)\n",
    "                    self.scheduler.step(val_loss)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_epoch = epoch\n",
    "                        current_patience = 0\n",
    "\n",
    "                        if self.save_path is not None:\n",
    "                            torch.save(\n",
    "                                self.model.state_dict(), self.save_path + \"_checkpt\"\n",
    "                            )\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "\n",
    "                    print(\n",
    "                        f\"Epoch [{epoch+1}/{num_epochs}],\"\n",
    "                        f\"Train Loss: {epoch_loss:.4f},\"\n",
    "                        f\"Val Loss: {val_loss:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    if current_patience >= patience:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "            if self.save_path is not None:\n",
    "                print(f\"Best model weights saved at epoch {best_epoch+1}\")\n",
    "                self.model.load_state_dict(torch.load(self.save_path + \"_checkpt\"))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    def hyperopt_search(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        param_grid,\n",
    "        metric,\n",
    "        max_evals=100,\n",
    "        problem_type=\"binary_classification\",\n",
    "        extra_info=None,\n",
    "        *kwargs,\n",
    "    ):\n",
    "        self.outer_params = param_grid[\"outer_params\"]\n",
    "        num_epochs = self.outer_params.get(\"num_epochs\", 3)\n",
    "        early_stopping = self.outer_params.get(\"early_stopping\", True)\n",
    "\n",
    "        space = infer_hyperopt_space_s1dcnn(param_grid)\n",
    "        self.num_features = extra_info[\"num_features\"]\n",
    "\n",
    "        # Define the objective function for hyperopt search\n",
    "        def objective(params):\n",
    "            self.logger.debug(f\"Training with hyperparameters: {params}\")\n",
    "            # Split the train data into training and validation sets\n",
    "\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self._set_loss_function(y)\n",
    "\n",
    "            self.model = self.build_model(\n",
    "                self.num_features, self.num_targets, params[\"hidden_size\"]\n",
    "            )\n",
    "\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), lr=params[\"learning_rate\"]\n",
    "            )\n",
    "            self.scheduler = ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"min\",\n",
    "                factor=params[\"scheduler_factor\"],\n",
    "                patience=params[\"scheduler_patience\"],\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "            train_loader, val_loader = self._pandas_to_torch_dataloaders(\n",
    "                X, y, params[\"batch_size\"], params[\"validation_fraction\"]\n",
    "            )\n",
    "\n",
    "            self.model.to(self.device)\n",
    "            self.model.train()\n",
    "\n",
    "            best_val_loss = float(\"inf\")\n",
    "            best_epoch = 0\n",
    "            current_patience = 0\n",
    "\n",
    "            with tqdm(\n",
    "                total=num_epochs, desc=\"Training\", unit=\"epoch\", ncols=80\n",
    "            ) as pbar:\n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_loss = self.train_step(train_loader)\n",
    "\n",
    "                    if early_stopping and params[\"validation_fraction\"] > 0:\n",
    "                        val_loss = self.validate_step(val_loader)\n",
    "                        self.scheduler.step(val_loss)\n",
    "\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_epoch = epoch\n",
    "                            current_patience = 0\n",
    "\n",
    "                        else:\n",
    "                            current_patience += 1\n",
    "\n",
    "                        print(\n",
    "                            f\"Epoch [{epoch+1}/{num_epochs}],\"\n",
    "                            f\"Train Loss: {epoch_loss:.4f},\"\n",
    "                            f\"Val Loss: {val_loss:.4f}\"\n",
    "                        )\n",
    "                        if current_patience >= params[\"early_stopping_patience\"]:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                            break\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Assuming you have a PyTorch DataLoader object for the validation set called `val_loader`\n",
    "            # Convert dataloader to pandas DataFrames\n",
    "            X_val, y_val = pd.DataFrame(), pd.DataFrame()\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_val = pd.concat([X_val, pd.DataFrame(X_batch.numpy())])\n",
    "                y_val = pd.concat([y_val, pd.DataFrame(y_batch.numpy())])\n",
    "\n",
    "            y_pred, y_prob = self.predict(X_val, predict_proba=True)\n",
    "            # Calculate the score using the specified metric\n",
    "\n",
    "            self.evaluator.y_true = y_val\n",
    "            self.evaluator.y_pred = y_pred\n",
    "            self.evaluator.y_prob = y_prob\n",
    "            score = self.evaluator.evaluate_metric(metric_name=metric)\n",
    "\n",
    "            if self.evaluator.maximize[metric][0]:\n",
    "                score = -1 * score\n",
    "\n",
    "            # Return the negative score (to minimize)\n",
    "            return {\n",
    "                \"loss\": score,\n",
    "                \"params\": params,\n",
    "                \"status\": STATUS_OK,\n",
    "                \"trained_model\": self.model,\n",
    "            }\n",
    "\n",
    "        # Define the trials object to keep track of the results\n",
    "        trials = Trials()\n",
    "\n",
    "        self.evaluator = Evaluator(problem_type=problem_type)\n",
    "        threshold = float(-1.0 * self.evaluator.maximize[metric][0])\n",
    "\n",
    "        # Run the hyperopt search\n",
    "        best = fmin(\n",
    "            objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(self.random_state),\n",
    "            early_stop_fn=lambda x: stop_on_perfect_lossCondition(x, threshold),\n",
    "        )\n",
    "\n",
    "        # Get the best hyperparameters and corresponding score\n",
    "        best_params = space_eval(space, best)\n",
    "        best_params[\"outer_params\"] = self.outer_params\n",
    "\n",
    "        best_trial = trials.best_trial\n",
    "        best_score = best_trial[\"result\"][\"loss\"]\n",
    "        self.best_model = best_trial[\"result\"][\"trained_model\"]\n",
    "\n",
    "        self.logger.info(f\"Best hyperparameters: {best_params}\")\n",
    "        self.logger.info(\n",
    "            f\"The best possible score for metric {metric} is {-threshold}, we reached {metric} = {-best_score}\"\n",
    "        )\n",
    "\n",
    "        return best_params, best_score\n",
    "\n",
    "    def predict(self, X_test, predict_proba=False, batch_size=4096):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        test_dataset = CustomDataset(\n",
    "            data=X_test,\n",
    "            transform=None,\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        with torch.no_grad():\n",
    "            for inputs in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                if self.problem_type == \"binary_classification\":\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy().reshape(-1)\n",
    "                    preds = (probs >= 0.5).astype(int)\n",
    "                    probabilities.extend(probs)\n",
    "\n",
    "                elif self.problem_type == \"multiclass_classification\":\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    preds = preds.numpy()\n",
    "                elif self.problem_type == \"regression\":\n",
    "                    preds = outputs.to(\"cpu\").tolist()\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Invalid problem_type. Supported options: binary_classification, multiclass_classification, regression.\"\n",
    "                    )\n",
    "\n",
    "                predictions.extend(preds)\n",
    "\n",
    "        self.logger.debug(\"Model predicting success\")\n",
    "        predictions = np.array(predictions)\n",
    "        probabilities = np.array(probabilities)\n",
    "\n",
    "        self.logger.debug(\"Model predicting success\")\n",
    "        return predictions, probabilities\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data.iloc[index].values.astype(np.float32)  # Convert to float32\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67a830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
