- dataset: housing
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id020
      cv_iterations: 10
      early_stopping: true
      cv_size: 5
      validation_fraction: 0.15
      n_iter_no_change: 6
      max_iter: 1000
    hidden_layer_sizes: &id021
    - - 64
      - 32
      - 16
    - - 256
      - 128
      - 64
      - 32
    - - 128
      - 64
      - 32
      - 16
    activation: &id022
    - relu
    - tanh
    - logistic
    solver: &id023
    - adam
    - lbfgs
    alpha: &id024
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id025
    - 0.0001
    - 0.01
    - 0.1
    beta_1: &id026
    - 0.99
    - 0.8
    beta_2: &id027
    - 0.999
    - 0.9
    batch_size: &id028
    - 512
    - 1024
    - 2048
- dataset: housing
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id008
      hyperopt_evals: 10
      validation_fraction: 0.15
      early_stopping_rounds: 30
      verbose: false
    n_estimators: &id009
    - 100
    - 1000
    max_bin: &id010
    - 256
    - 32
    tree_method: &id011
    - auto
    - hist
    max_depth: &id012
    - 4
    - 10
    learning_rate: &id013
    - 0.1
    - 0.33
    subsample: &id014
    - 0.7
    - 1.0
    colsample_bytree: &id015
    - 0.5
    - 1.0
    min_child_weight: &id016
    - 1
    - 10
    alpha: &id017
    - 0.0
    - 5.0
    gamma: &id018
    - 0.0
    - 5.0
    lambda: &id019
    - 0.0
    - 5.0
- dataset: housing
  model: catboost
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 10
      validation_fraction: 0.15
      early_stopping_rounds: 100
      verbose: false
    iterations: &id002
    - 200
    - 2000
    learning_rate: &id003
    - 0.001
    - 0.1
    depth: &id004
    - 4
    - 10
    l2_leaf_reg: &id005
    - 0.5
    - 5.0
    min_child_samples: &id006
    - 1
    - 100
    bagging_temperature: &id007
    - 0.1
    - 2.0
- dataset: housing
  model: resnet
  best_params: {}
  param_grid:
    outer_params: &id029
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.15
    batch_size: &id030
    - 512
    - 1024
    - 2048
    resnet_depth: &id031
    - resnet18
    - resnet34
    - resnet50
    optimizer_fn: &id032
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id033
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id034
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.15
      early_stopping_patience: 6
    batch_size: &id035
    - 512
    - 1024
    - 2048
    hidden_size: &id036
    - 1024
    - 2048
    - 4096
    optimizer_fn: &id037
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id038
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id039
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    virtual_batch_size_ratio: &id040
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id041
    - 512
    - 1024
    - 2048
    weights: &id042
    - 0
    - 1
    mask_type: &id043
    - sparsemax
    - entmax
    n_d: &id044
    - 6
    - 32
    n_steps: &id045
    - 1
    - 6
    gamma: &id046
    - 1.0
    - 2.0
    n_independent: &id047
    - 1
    - 3
    n_shared: &id048
    - 1
    - 3
    optimizer_fn: &id049
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id050
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id051
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id052
    - 512
    - 1024
    - 2048
    tree_depth: &id053
    - 4
    - 7
    num_trees: &id054
    - 4
    - 10
    chain_trees: &id055
    - false
    - true
    gflu_stages: &id056
    - 3
    - 8
    gflu_dropout: &id057
    - 0.0
    - 0.05
    tree_dropout: &id058
    - 0.0
    - 0.05
    tree_wise_attention_dropout: &id059
    - 0.0
    - 0.05
    embedding_dropout: &id060
    - 0
    - 0.2
    optimizer_fn: &id061
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id062
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id063
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
      attn_feature_importance: false
    learning_rate: &id064
    - 0.001
    - 0.0001
    batch_size: &id065
    - 512
    - 1024
    - 2048
    num_heads: &id066
    - 4
    - 10
    input_embed_dim_multiplier: &id067
    - 2
    - 6
    embedding_initialization: &id068
    - kaiming_uniform
    - kaiming_normal
    embedding_dropout: &id069
    - 0.05
    - 0.2
    shared_embedding_fraction: &id070
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id071
    - 4
    - 8
    attn_dropout: &id072
    - 0.05
    - 0.2
    add_norm_dropout: &id073
    - 0.05
    - 0.2
    ff_dropout: &id074
    - 0.05
    - 0.2
    ff_hidden_multiplier: &id075
    - 4
    - 32
    transformer_activation: &id076
    - GEGLU
    - ReGLU
    - SwiGLU
    optimizer_fn: &id077
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id078
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: &id079
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id080
    - 512
    - 1024
    - 2048
    learning_rate: &id081
    - 0.001
    - 0.0001
    layers: &id082
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    activation: &id083
    - ReLU
    - LeakyReLU
    - Tanh
    initialization: &id084
    - kaiming
    - xavier
    dropout: &id085
    - 0.0
    - 0.3
    embedding_dropout: &id086
    - 0.0
    - 0.3
    optimizer_fn: &id087
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id088
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: node
  best_params: {}
  param_grid:
    outer_params: &id097
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id098
    - 512
    - 1024
    - 2048
    num_layers: &id099
    - 1
    - 2
    - 3
    num_trees: &id100
    - 12
    - 128
    additional_tree_output_dim: &id101
    - 2
    - 3
    - 4
    depth: &id102
    - 5
    - 7
    choice_function: &id103
    - entmax15
    - sparsemax
    bin_function: &id104
    - entmoid15
    - sparsemoid
    input_dropout: &id105
    - 0.0
    - 0.1
    embedding_dropout: &id106
    - 0.0
    - 0.1
    embed_categorical: &id107
    - true
    - false
    optimizer_fn: &id108
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id109
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id131
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id132
    - 512
    - 1024
    - 2048
    learning_rate: &id133
    - 0.01
    - 0.0001
    embedding_bias: &id134
    - true
    - false
    embedding_initialization: &id135
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id136
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id137
    - 4
    - 10
    attn_dropout: &id138
    - 0.05
    - 0.3
    add_norm_dropout: &id139
    - 0.05
    - 0.3
    ff_dropout: &id140
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id141
    - 2
    - 6
    transformer_activation: &id142
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id143
    - 0.05
    - 0.3
    optimizer_fn: &id144
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id145
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id110
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id111
    - 512
    - 1024
    - 2048
    learning_rate: &id112
    - 0.001
    - 0.0001
    attn_embed_dim_multiplier: &id113
    - 2
    - 16
    num_heads: &id114
    - 2
    - 8
    num_attn_blocks: &id115
    - 2
    - 6
    attn_dropouts: &id116
    - 0.0
    - 0.3
    embedding_dim: &id117
    - 8
    - 32
    embedding_initialization: &id118
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id119
    - true
    - false
    share_embedding: &id120
    - true
    - false
    share_embedding_strategy: &id121
    - add
    - fraction
    shared_embedding_fraction: &id122
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id123
    - true
    - false
    layers: &id124
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout: &id125
    - 0.0
    - 0.3
    activation: &id126
    - ReLU
    - LeakyReLU
    initialization: &id127
    - kaiming
    - xavier
    attention_pooling: &id128
    - true
    - false
    optimizer_fn: &id129
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id130
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id089
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    learning_rate: &id090
    - 0.001
    - 0.0001
    batch_size: &id091
    - 512
    - 1024
    - 2048
    gflu_stages: &id092
    - 4
    - 10
    gflu_dropout: &id093
    - 0.0
    - 0.3
    embedding_dropout: &id094
    - 0.0
    - 0.3
    optimizer_fn: &id095
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id096
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: adult
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: adult
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: adult
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: adult
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id029
    batch_size: *id030
    resnet_depth: *id031
    optimizer_fn: *id032
    scheduler_fn: *id033
- dataset: adult
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id034
    batch_size: *id035
    hidden_size: *id036
    optimizer_fn: *id037
    scheduler_fn: *id038
- dataset: adult
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id039
    virtual_batch_size_ratio: *id040
    batch_size: *id041
    weights: *id042
    mask_type: *id043
    n_d: *id044
    n_steps: *id045
    gamma: *id046
    n_independent: *id047
    n_shared: *id048
    optimizer_fn: *id049
    scheduler_fn: *id050
- dataset: adult
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id051
    batch_size: *id052
    tree_depth: *id053
    num_trees: *id054
    chain_trees: *id055
    gflu_stages: *id056
    gflu_dropout: *id057
    tree_dropout: *id058
    tree_wise_attention_dropout: *id059
    embedding_dropout: *id060
    optimizer_fn: *id061
    scheduler_fn: *id062
- dataset: adult
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id063
    learning_rate: *id064
    batch_size: *id065
    num_heads: *id066
    input_embed_dim_multiplier: *id067
    embedding_initialization: *id068
    embedding_dropout: *id069
    shared_embedding_fraction: *id070
    num_attn_blocks: *id071
    attn_dropout: *id072
    add_norm_dropout: *id073
    ff_dropout: *id074
    ff_hidden_multiplier: *id075
    transformer_activation: *id076
    optimizer_fn: *id077
    scheduler_fn: *id078
- dataset: adult
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id079
    batch_size: *id080
    learning_rate: *id081
    layers: *id082
    activation: *id083
    initialization: *id084
    dropout: *id085
    embedding_dropout: *id086
    optimizer_fn: *id087
    scheduler_fn: *id088
- dataset: adult
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id089
    learning_rate: *id090
    batch_size: *id091
    gflu_stages: *id092
    gflu_dropout: *id093
    embedding_dropout: *id094
    optimizer_fn: *id095
    scheduler_fn: *id096
- dataset: adult
  model: node
  best_params: {}
  param_grid:
    outer_params: *id097
    batch_size: *id098
    num_layers: *id099
    num_trees: *id100
    additional_tree_output_dim: *id101
    depth: *id102
    choice_function: *id103
    bin_function: *id104
    input_dropout: *id105
    embedding_dropout: *id106
    embed_categorical: *id107
    optimizer_fn: *id108
    scheduler_fn: *id109
- dataset: adult
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id110
    batch_size: *id111
    learning_rate: *id112
    attn_embed_dim_multiplier: *id113
    num_heads: *id114
    num_attn_blocks: *id115
    attn_dropouts: *id116
    embedding_dim: *id117
    embedding_initialization: *id118
    embedding_bias: *id119
    share_embedding: *id120
    share_embedding_strategy: *id121
    shared_embedding_fraction: *id122
    deep_layers: *id123
    layers: *id124
    dropout: *id125
    activation: *id126
    initialization: *id127
    attention_pooling: *id128
    optimizer_fn: *id129
    scheduler_fn: *id130
- dataset: adult
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id131
    batch_size: *id132
    learning_rate: *id133
    embedding_bias: *id134
    embedding_initialization: *id135
    shared_embedding_fraction: *id136
    num_attn_blocks: *id137
    attn_dropout: *id138
    add_norm_dropout: *id139
    ff_dropout: *id140
    ff_hidden_multiplier: *id141
    transformer_activation: *id142
    embedding_dropout: *id143
    optimizer_fn: *id144
    scheduler_fn: *id145
- dataset: heloc
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: heloc
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: heloc
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: heloc
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id029
    batch_size: *id030
    resnet_depth: *id031
    optimizer_fn: *id032
    scheduler_fn: *id033
- dataset: heloc
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id034
    batch_size: *id035
    hidden_size: *id036
    optimizer_fn: *id037
    scheduler_fn: *id038
- dataset: heloc
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id039
    virtual_batch_size_ratio: *id040
    batch_size: *id041
    weights: *id042
    mask_type: *id043
    n_d: *id044
    n_steps: *id045
    gamma: *id046
    n_independent: *id047
    n_shared: *id048
    optimizer_fn: *id049
    scheduler_fn: *id050
- dataset: heloc
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id051
    batch_size: *id052
    tree_depth: *id053
    num_trees: *id054
    chain_trees: *id055
    gflu_stages: *id056
    gflu_dropout: *id057
    tree_dropout: *id058
    tree_wise_attention_dropout: *id059
    embedding_dropout: *id060
    optimizer_fn: *id061
    scheduler_fn: *id062
- dataset: heloc
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id063
    learning_rate: *id064
    batch_size: *id065
    num_heads: *id066
    input_embed_dim_multiplier: *id067
    embedding_initialization: *id068
    embedding_dropout: *id069
    shared_embedding_fraction: *id070
    num_attn_blocks: *id071
    attn_dropout: *id072
    add_norm_dropout: *id073
    ff_dropout: *id074
    ff_hidden_multiplier: *id075
    transformer_activation: *id076
    optimizer_fn: *id077
    scheduler_fn: *id078
- dataset: heloc
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id079
    batch_size: *id080
    learning_rate: *id081
    layers: *id082
    activation: *id083
    initialization: *id084
    dropout: *id085
    embedding_dropout: *id086
    optimizer_fn: *id087
    scheduler_fn: *id088
- dataset: heloc
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id089
    learning_rate: *id090
    batch_size: *id091
    gflu_stages: *id092
    gflu_dropout: *id093
    embedding_dropout: *id094
    optimizer_fn: *id095
    scheduler_fn: *id096
- dataset: heloc
  model: node
  best_params: {}
  param_grid:
    outer_params: *id097
    batch_size: *id098
    num_layers: *id099
    num_trees: *id100
    additional_tree_output_dim: *id101
    depth: *id102
    choice_function: *id103
    bin_function: *id104
    input_dropout: *id105
    embedding_dropout: *id106
    embed_categorical: *id107
    optimizer_fn: *id108
    scheduler_fn: *id109
- dataset: heloc
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id110
    batch_size: *id111
    learning_rate: *id112
    attn_embed_dim_multiplier: *id113
    num_heads: *id114
    num_attn_blocks: *id115
    attn_dropouts: *id116
    embedding_dim: *id117
    embedding_initialization: *id118
    embedding_bias: *id119
    share_embedding: *id120
    share_embedding_strategy: *id121
    shared_embedding_fraction: *id122
    deep_layers: *id123
    layers: *id124
    dropout: *id125
    activation: *id126
    initialization: *id127
    attention_pooling: *id128
    optimizer_fn: *id129
    scheduler_fn: *id130
- dataset: heloc
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id131
    batch_size: *id132
    learning_rate: *id133
    embedding_bias: *id134
    embedding_initialization: *id135
    shared_embedding_fraction: *id136
    num_attn_blocks: *id137
    attn_dropout: *id138
    add_norm_dropout: *id139
    ff_dropout: *id140
    ff_hidden_multiplier: *id141
    transformer_activation: *id142
    embedding_dropout: *id143
    optimizer_fn: *id144
    scheduler_fn: *id145
- dataset: creditcard
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: creditcard
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: creditcard
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: creditcard
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id029
    batch_size: *id030
    resnet_depth: *id031
    optimizer_fn: *id032
    scheduler_fn: *id033
- dataset: creditcard
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id034
    batch_size: *id035
    hidden_size: *id036
    optimizer_fn: *id037
    scheduler_fn: *id038
- dataset: creditcard
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id039
    virtual_batch_size_ratio: *id040
    batch_size: *id041
    weights: *id042
    mask_type: *id043
    n_d: *id044
    n_steps: *id045
    gamma: *id046
    n_independent: *id047
    n_shared: *id048
    optimizer_fn: *id049
    scheduler_fn: *id050
- dataset: creditcard
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id051
    batch_size: *id052
    tree_depth: *id053
    num_trees: *id054
    chain_trees: *id055
    gflu_stages: *id056
    gflu_dropout: *id057
    tree_dropout: *id058
    tree_wise_attention_dropout: *id059
    embedding_dropout: *id060
    optimizer_fn: *id061
    scheduler_fn: *id062
- dataset: creditcard
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id063
    learning_rate: *id064
    batch_size: *id065
    num_heads: *id066
    input_embed_dim_multiplier: *id067
    embedding_initialization: *id068
    embedding_dropout: *id069
    shared_embedding_fraction: *id070
    num_attn_blocks: *id071
    attn_dropout: *id072
    add_norm_dropout: *id073
    ff_dropout: *id074
    ff_hidden_multiplier: *id075
    transformer_activation: *id076
    optimizer_fn: *id077
    scheduler_fn: *id078
- dataset: creditcard
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id079
    batch_size: *id080
    learning_rate: *id081
    layers: *id082
    activation: *id083
    initialization: *id084
    dropout: *id085
    embedding_dropout: *id086
    optimizer_fn: *id087
    scheduler_fn: *id088
- dataset: creditcard
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id089
    learning_rate: *id090
    batch_size: *id091
    gflu_stages: *id092
    gflu_dropout: *id093
    embedding_dropout: *id094
    optimizer_fn: *id095
    scheduler_fn: *id096
- dataset: creditcard
  model: node
  best_params: {}
  param_grid:
    outer_params: *id097
    batch_size: *id098
    num_layers: *id099
    num_trees: *id100
    additional_tree_output_dim: *id101
    depth: *id102
    choice_function: *id103
    bin_function: *id104
    input_dropout: *id105
    embedding_dropout: *id106
    embed_categorical: *id107
    optimizer_fn: *id108
    scheduler_fn: *id109
- dataset: creditcard
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id110
    batch_size: *id111
    learning_rate: *id112
    attn_embed_dim_multiplier: *id113
    num_heads: *id114
    num_attn_blocks: *id115
    attn_dropouts: *id116
    embedding_dim: *id117
    embedding_initialization: *id118
    embedding_bias: *id119
    share_embedding: *id120
    share_embedding_strategy: *id121
    shared_embedding_fraction: *id122
    deep_layers: *id123
    layers: *id124
    dropout: *id125
    activation: *id126
    initialization: *id127
    attention_pooling: *id128
    optimizer_fn: *id129
    scheduler_fn: *id130
- dataset: creditcard
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id131
    batch_size: *id132
    learning_rate: *id133
    embedding_bias: *id134
    embedding_initialization: *id135
    shared_embedding_fraction: *id136
    num_attn_blocks: *id137
    attn_dropout: *id138
    add_norm_dropout: *id139
    ff_dropout: *id140
    ff_hidden_multiplier: *id141
    transformer_activation: *id142
    embedding_dropout: *id143
    optimizer_fn: *id144
    scheduler_fn: *id145
- dataset: iris
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: iris
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: iris
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: iris
  model: resnet
  best_params: {}
  param_grid:
    outer_params: &id146
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.15
    batch_size: &id147
    - 1024
    - 2048
    - 4096
    resnet_depth: &id148
    - resnet18
    - resnet34
    - resnet50
    optimizer_fn: &id149
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id150
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id151
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.15
      early_stopping_patience: 6
    batch_size: &id152
    - 1024
    - 2048
    - 4096
    hidden_size: &id153
    - 1024
    - 2048
    - 4096
    optimizer_fn: &id154
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id155
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id156
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    virtual_batch_size_ratio: &id157
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id158
    - 1024
    - 2048
    - 4096
    weights: &id159
    - 0
    - 1
    mask_type: &id160
    - sparsemax
    - entmax
    n_d: &id161
    - 6
    - 32
    n_steps: &id162
    - 1
    - 6
    gamma: &id163
    - 1.0
    - 2.0
    n_independent: &id164
    - 1
    - 3
    n_shared: &id165
    - 1
    - 3
    optimizer_fn: &id166
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id167
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id168
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id169
    - 1024
    - 2048
    - 4096
    tree_depth: &id170
    - 4
    - 7
    num_trees: &id171
    - 4
    - 10
    chain_trees: &id172
    - false
    - true
    gflu_stages: &id173
    - 3
    - 8
    gflu_dropout: &id174
    - 0.0
    - 0.05
    tree_dropout: &id175
    - 0.0
    - 0.05
    tree_wise_attention_dropout: &id176
    - 0.0
    - 0.05
    embedding_dropout: &id177
    - 0
    - 0.2
    optimizer_fn: &id178
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id179
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id180
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
      attn_feature_importance: false
    learning_rate: &id181
    - 0.001
    - 0.0001
    batch_size: &id182
    - 1024
    - 2048
    - 4096
    num_heads: &id183
    - 4
    - 10
    input_embed_dim_multiplier: &id184
    - 2
    - 6
    embedding_initialization: &id185
    - kaiming_uniform
    - kaiming_normal
    embedding_dropout: &id186
    - 0.05
    - 0.2
    shared_embedding_fraction: &id187
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id188
    - 4
    - 8
    attn_dropout: &id189
    - 0.05
    - 0.2
    add_norm_dropout: &id190
    - 0.05
    - 0.2
    ff_dropout: &id191
    - 0.05
    - 0.2
    ff_hidden_multiplier: &id192
    - 4
    - 32
    transformer_activation: &id193
    - GEGLU
    - ReGLU
    - SwiGLU
    optimizer_fn: &id194
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id195
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: &id196
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id197
    - 1024
    - 2048
    - 4096
    learning_rate: &id198
    - 0.001
    - 0.0001
    layers: &id199
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    activation: &id200
    - ReLU
    - LeakyReLU
    - Tanh
    initialization: &id201
    - kaiming
    - xavier
    dropout: &id202
    - 0.0
    - 0.3
    embedding_dropout: &id203
    - 0.0
    - 0.3
    optimizer_fn: &id204
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id205
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id206
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    learning_rate: &id207
    - 0.001
    - 0.0001
    batch_size: &id208
    - 1024
    - 2048
    - 4096
    gflu_stages: &id209
    - 4
    - 10
    gflu_dropout: &id210
    - 0.0
    - 0.3
    embedding_dropout: &id211
    - 0.0
    - 0.3
    optimizer_fn: &id212
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id213
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: node
  best_params: {}
  param_grid:
    outer_params: &id214
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id215
    - 1024
    - 2048
    - 4096
    num_layers: &id216
    - 1
    - 2
    - 3
    num_trees: &id217
    - 12
    - 128
    additional_tree_output_dim: &id218
    - 2
    - 3
    - 4
    depth: &id219
    - 5
    - 7
    choice_function: &id220
    - entmax15
    - sparsemax
    bin_function: &id221
    - entmoid15
    - sparsemoid
    input_dropout: &id222
    - 0.0
    - 0.1
    embedding_dropout: &id223
    - 0.0
    - 0.1
    embed_categorical: &id224
    - true
    - false
    optimizer_fn: &id225
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id226
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id227
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id228
    - 1024
    - 2048
    - 4096
    learning_rate: &id229
    - 0.001
    - 0.0001
    attn_embed_dim_multiplier: &id230
    - 2
    - 16
    num_heads: &id231
    - 2
    - 8
    num_attn_blocks: &id232
    - 2
    - 6
    attn_dropouts: &id233
    - 0.0
    - 0.3
    embedding_dim: &id234
    - 8
    - 32
    embedding_initialization: &id235
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id236
    - true
    - false
    share_embedding: &id237
    - true
    - false
    share_embedding_strategy: &id238
    - add
    - fraction
    shared_embedding_fraction: &id239
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id240
    - true
    - false
    layers: &id241
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout: &id242
    - 0.0
    - 0.3
    activation: &id243
    - ReLU
    - LeakyReLU
    initialization: &id244
    - kaiming
    - xavier
    attention_pooling: &id245
    - true
    - false
    optimizer_fn: &id246
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id247
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: iris
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id248
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id249
    - 1024
    - 2048
    - 4096
    learning_rate: &id250
    - 0.01
    - 0.0001
    embedding_bias: &id251
    - true
    - false
    embedding_initialization: &id252
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id253
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id254
    - 4
    - 10
    attn_dropout: &id255
    - 0.05
    - 0.3
    add_norm_dropout: &id256
    - 0.05
    - 0.3
    ff_dropout: &id257
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id258
    - 2
    - 6
    transformer_activation: &id259
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id260
    - 0.05
    - 0.3
    optimizer_fn: &id261
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id262
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: titanic
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: titanic
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: titanic
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id146
    batch_size: *id147
    resnet_depth: *id148
    optimizer_fn: *id149
    scheduler_fn: *id150
- dataset: titanic
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id151
    batch_size: *id152
    hidden_size: *id153
    optimizer_fn: *id154
    scheduler_fn: *id155
- dataset: titanic
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id156
    virtual_batch_size_ratio: *id157
    batch_size: *id158
    weights: *id159
    mask_type: *id160
    n_d: *id161
    n_steps: *id162
    gamma: *id163
    n_independent: *id164
    n_shared: *id165
    optimizer_fn: *id166
    scheduler_fn: *id167
- dataset: titanic
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id168
    batch_size: *id169
    tree_depth: *id170
    num_trees: *id171
    chain_trees: *id172
    gflu_stages: *id173
    gflu_dropout: *id174
    tree_dropout: *id175
    tree_wise_attention_dropout: *id176
    embedding_dropout: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: titanic
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    num_heads: *id183
    input_embed_dim_multiplier: *id184
    embedding_initialization: *id185
    embedding_dropout: *id186
    shared_embedding_fraction: *id187
    num_attn_blocks: *id188
    attn_dropout: *id189
    add_norm_dropout: *id190
    ff_dropout: *id191
    ff_hidden_multiplier: *id192
    transformer_activation: *id193
    optimizer_fn: *id194
    scheduler_fn: *id195
- dataset: titanic
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id196
    batch_size: *id197
    learning_rate: *id198
    layers: *id199
    activation: *id200
    initialization: *id201
    dropout: *id202
    embedding_dropout: *id203
    optimizer_fn: *id204
    scheduler_fn: *id205
- dataset: titanic
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id206
    learning_rate: *id207
    batch_size: *id208
    gflu_stages: *id209
    gflu_dropout: *id210
    embedding_dropout: *id211
    optimizer_fn: *id212
    scheduler_fn: *id213
- dataset: titanic
  model: node
  best_params: {}
  param_grid:
    outer_params: *id214
    batch_size: *id215
    num_layers: *id216
    num_trees: *id217
    additional_tree_output_dim: *id218
    depth: *id219
    choice_function: *id220
    bin_function: *id221
    input_dropout: *id222
    embedding_dropout: *id223
    embed_categorical: *id224
    optimizer_fn: *id225
    scheduler_fn: *id226
- dataset: titanic
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id227
    batch_size: *id228
    learning_rate: *id229
    attn_embed_dim_multiplier: *id230
    num_heads: *id231
    num_attn_blocks: *id232
    attn_dropouts: *id233
    embedding_dim: *id234
    embedding_initialization: *id235
    embedding_bias: *id236
    share_embedding: *id237
    share_embedding_strategy: *id238
    shared_embedding_fraction: *id239
    deep_layers: *id240
    layers: *id241
    dropout: *id242
    activation: *id243
    initialization: *id244
    attention_pooling: *id245
    optimizer_fn: *id246
    scheduler_fn: *id247
- dataset: titanic
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id248
    batch_size: *id249
    learning_rate: *id250
    embedding_bias: *id251
    embedding_initialization: *id252
    shared_embedding_fraction: *id253
    num_attn_blocks: *id254
    attn_dropout: *id255
    add_norm_dropout: *id256
    ff_dropout: *id257
    ff_hidden_multiplier: *id258
    transformer_activation: *id259
    embedding_dropout: *id260
    optimizer_fn: *id261
    scheduler_fn: *id262
- dataset: breastcancer
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id146
    batch_size: *id147
    resnet_depth: *id148
    optimizer_fn: *id149
    scheduler_fn: *id150
- dataset: breastcancer
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id151
    batch_size: *id152
    hidden_size: *id153
    optimizer_fn: *id154
    scheduler_fn: *id155
- dataset: breastcancer
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id156
    virtual_batch_size_ratio: *id157
    batch_size: *id158
    weights: *id159
    mask_type: *id160
    n_d: *id161
    n_steps: *id162
    gamma: *id163
    n_independent: *id164
    n_shared: *id165
    optimizer_fn: *id166
    scheduler_fn: *id167
- dataset: breastcancer
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id168
    batch_size: *id169
    tree_depth: *id170
    num_trees: *id171
    chain_trees: *id172
    gflu_stages: *id173
    gflu_dropout: *id174
    tree_dropout: *id175
    tree_wise_attention_dropout: *id176
    embedding_dropout: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: breastcancer
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    num_heads: *id183
    input_embed_dim_multiplier: *id184
    embedding_initialization: *id185
    embedding_dropout: *id186
    shared_embedding_fraction: *id187
    num_attn_blocks: *id188
    attn_dropout: *id189
    add_norm_dropout: *id190
    ff_dropout: *id191
    ff_hidden_multiplier: *id192
    transformer_activation: *id193
    optimizer_fn: *id194
    scheduler_fn: *id195
- dataset: breastcancer
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id196
    batch_size: *id197
    learning_rate: *id198
    layers: *id199
    activation: *id200
    initialization: *id201
    dropout: *id202
    embedding_dropout: *id203
    optimizer_fn: *id204
    scheduler_fn: *id205
- dataset: breastcancer
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id206
    learning_rate: *id207
    batch_size: *id208
    gflu_stages: *id209
    gflu_dropout: *id210
    embedding_dropout: *id211
    optimizer_fn: *id212
    scheduler_fn: *id213
- dataset: breastcancer
  model: node
  best_params: {}
  param_grid:
    outer_params: *id214
    batch_size: *id215
    num_layers: *id216
    num_trees: *id217
    additional_tree_output_dim: *id218
    depth: *id219
    choice_function: *id220
    bin_function: *id221
    input_dropout: *id222
    embedding_dropout: *id223
    embed_categorical: *id224
    optimizer_fn: *id225
    scheduler_fn: *id226
- dataset: breastcancer
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id227
    batch_size: *id228
    learning_rate: *id229
    attn_embed_dim_multiplier: *id230
    num_heads: *id231
    num_attn_blocks: *id232
    attn_dropouts: *id233
    embedding_dim: *id234
    embedding_initialization: *id235
    embedding_bias: *id236
    share_embedding: *id237
    share_embedding_strategy: *id238
    shared_embedding_fraction: *id239
    deep_layers: *id240
    layers: *id241
    dropout: *id242
    activation: *id243
    initialization: *id244
    attention_pooling: *id245
    optimizer_fn: *id246
    scheduler_fn: *id247
- dataset: breastcancer
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id248
    batch_size: *id249
    learning_rate: *id250
    embedding_bias: *id251
    embedding_initialization: *id252
    shared_embedding_fraction: *id253
    num_attn_blocks: *id254
    attn_dropout: *id255
    add_norm_dropout: *id256
    ff_dropout: *id257
    ff_hidden_multiplier: *id258
    transformer_activation: *id259
    embedding_dropout: *id260
    optimizer_fn: *id261
    scheduler_fn: *id262
- dataset: ageconditions
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id146
    batch_size: *id147
    resnet_depth: *id148
    optimizer_fn: *id149
    scheduler_fn: *id150
- dataset: ageconditions
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id151
    batch_size: *id152
    hidden_size: *id153
    optimizer_fn: *id154
    scheduler_fn: *id155
- dataset: ageconditions
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id156
    virtual_batch_size_ratio: *id157
    batch_size: *id158
    weights: *id159
    mask_type: *id160
    n_d: *id161
    n_steps: *id162
    gamma: *id163
    n_independent: *id164
    n_shared: *id165
    optimizer_fn: *id166
    scheduler_fn: *id167
- dataset: ageconditions
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id168
    batch_size: *id169
    tree_depth: *id170
    num_trees: *id171
    chain_trees: *id172
    gflu_stages: *id173
    gflu_dropout: *id174
    tree_dropout: *id175
    tree_wise_attention_dropout: *id176
    embedding_dropout: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: ageconditions
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    num_heads: *id183
    input_embed_dim_multiplier: *id184
    embedding_initialization: *id185
    embedding_dropout: *id186
    shared_embedding_fraction: *id187
    num_attn_blocks: *id188
    attn_dropout: *id189
    add_norm_dropout: *id190
    ff_dropout: *id191
    ff_hidden_multiplier: *id192
    transformer_activation: *id193
    optimizer_fn: *id194
    scheduler_fn: *id195
- dataset: ageconditions
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id196
    batch_size: *id197
    learning_rate: *id198
    layers: *id199
    activation: *id200
    initialization: *id201
    dropout: *id202
    embedding_dropout: *id203
    optimizer_fn: *id204
    scheduler_fn: *id205
- dataset: ageconditions
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id206
    learning_rate: *id207
    batch_size: *id208
    gflu_stages: *id209
    gflu_dropout: *id210
    embedding_dropout: *id211
    optimizer_fn: *id212
    scheduler_fn: *id213
- dataset: ageconditions
  model: node
  best_params: {}
  param_grid:
    outer_params: *id214
    batch_size: *id215
    num_layers: *id216
    num_trees: *id217
    additional_tree_output_dim: *id218
    depth: *id219
    choice_function: *id220
    bin_function: *id221
    input_dropout: *id222
    embedding_dropout: *id223
    embed_categorical: *id224
    optimizer_fn: *id225
    scheduler_fn: *id226
- dataset: ageconditions
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id227
    batch_size: *id228
    learning_rate: *id229
    attn_embed_dim_multiplier: *id230
    num_heads: *id231
    num_attn_blocks: *id232
    attn_dropouts: *id233
    embedding_dim: *id234
    embedding_initialization: *id235
    embedding_bias: *id236
    share_embedding: *id237
    share_embedding_strategy: *id238
    shared_embedding_fraction: *id239
    deep_layers: *id240
    layers: *id241
    dropout: *id242
    activation: *id243
    initialization: *id244
    attention_pooling: *id245
    optimizer_fn: *id246
    scheduler_fn: *id247
- dataset: ageconditions
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id248
    batch_size: *id249
    learning_rate: *id250
    embedding_bias: *id251
    embedding_initialization: *id252
    shared_embedding_fraction: *id253
    num_attn_blocks: *id254
    attn_dropout: *id255
    add_norm_dropout: *id256
    ff_dropout: *id257
    ff_hidden_multiplier: *id258
    transformer_activation: *id259
    embedding_dropout: *id260
    optimizer_fn: *id261
    scheduler_fn: *id262
- dataset: covertype
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id001
    iterations: *id002
    learning_rate: *id003
    depth: *id004
    l2_leaf_reg: *id005
    min_child_samples: *id006
    bagging_temperature: *id007
- dataset: covertype
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id008
    n_estimators: *id009
    max_bin: *id010
    tree_method: *id011
    max_depth: *id012
    learning_rate: *id013
    subsample: *id014
    colsample_bytree: *id015
    min_child_weight: *id016
    alpha: *id017
    gamma: *id018
    lambda: *id019
- dataset: covertype
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id020
    hidden_layer_sizes: *id021
    activation: *id022
    solver: *id023
    alpha: *id024
    learning_rate_init: *id025
    beta_1: *id026
    beta_2: *id027
    batch_size: *id028
- dataset: covertype
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id146
    batch_size: *id147
    resnet_depth: *id148
    optimizer_fn: *id149
    scheduler_fn: *id150
- dataset: covertype
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id151
    batch_size: *id152
    hidden_size: *id153
    optimizer_fn: *id154
    scheduler_fn: *id155
- dataset: covertype
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id156
    virtual_batch_size_ratio: *id157
    batch_size: *id158
    weights: *id159
    mask_type: *id160
    n_d: *id161
    n_steps: *id162
    gamma: *id163
    n_independent: *id164
    n_shared: *id165
    optimizer_fn: *id166
    scheduler_fn: *id167
- dataset: covertype
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id168
    batch_size: *id169
    tree_depth: *id170
    num_trees: *id171
    chain_trees: *id172
    gflu_stages: *id173
    gflu_dropout: *id174
    tree_dropout: *id175
    tree_wise_attention_dropout: *id176
    embedding_dropout: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: covertype
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    num_heads: *id183
    input_embed_dim_multiplier: *id184
    embedding_initialization: *id185
    embedding_dropout: *id186
    shared_embedding_fraction: *id187
    num_attn_blocks: *id188
    attn_dropout: *id189
    add_norm_dropout: *id190
    ff_dropout: *id191
    ff_hidden_multiplier: *id192
    transformer_activation: *id193
    optimizer_fn: *id194
    scheduler_fn: *id195
- dataset: covertype
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id196
    batch_size: *id197
    learning_rate: *id198
    layers: *id199
    activation: *id200
    initialization: *id201
    dropout: *id202
    embedding_dropout: *id203
    optimizer_fn: *id204
    scheduler_fn: *id205
- dataset: covertype
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id206
    learning_rate: *id207
    batch_size: *id208
    gflu_stages: *id209
    gflu_dropout: *id210
    embedding_dropout: *id211
    optimizer_fn: *id212
    scheduler_fn: *id213
- dataset: covertype
  model: node
  best_params: {}
  param_grid:
    outer_params: *id214
    batch_size: *id215
    num_layers: *id216
    num_trees: *id217
    additional_tree_output_dim: *id218
    depth: *id219
    choice_function: *id220
    bin_function: *id221
    input_dropout: *id222
    embedding_dropout: *id223
    embed_categorical: *id224
    optimizer_fn: *id225
    scheduler_fn: *id226
- dataset: covertype
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id227
    batch_size: *id228
    learning_rate: *id229
    attn_embed_dim_multiplier: *id230
    num_heads: *id231
    num_attn_blocks: *id232
    attn_dropouts: *id233
    embedding_dim: *id234
    embedding_initialization: *id235
    embedding_bias: *id236
    share_embedding: *id237
    share_embedding_strategy: *id238
    shared_embedding_fraction: *id239
    deep_layers: *id240
    layers: *id241
    dropout: *id242
    activation: *id243
    initialization: *id244
    attention_pooling: *id245
    optimizer_fn: *id246
    scheduler_fn: *id247
- dataset: covertype
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id248
    batch_size: *id249
    learning_rate: *id250
    embedding_bias: *id251
    embedding_initialization: *id252
    shared_embedding_fraction: *id253
    num_attn_blocks: *id254
    attn_dropout: *id255
    add_norm_dropout: *id256
    ff_dropout: *id257
    ff_hidden_multiplier: *id258
    transformer_activation: *id259
    embedding_dropout: *id260
    optimizer_fn: *id261
    scheduler_fn: *id262
