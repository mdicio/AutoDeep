- dataset: heloc
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.15
      early_stopping_patience: 6
    batch_size: &id002
    - 512
    - 1024
    - 2048
    hidden_size: &id003
    - 1024
    - 2048
    - 4096
    optimizer_fn: &id004
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id005
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id006
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    virtual_batch_size_ratio: &id007
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id008
    - 512
    - 1024
    - 2048
    weights: &id009
    - 0
    - 1
    mask_type: &id010
    - sparsemax
    - entmax
    n_d: &id011
    - 6
    - 32
    n_steps: &id012
    - 1
    - 6
    gamma: &id013
    - 1.0
    - 2.0
    n_independent: &id014
    - 1
    - 3
    n_shared: &id015
    - 1
    - 3
    optimizer_fn: &id016
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id017
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id018
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id019
    - 512
    - 1024
    - 2048
    tree_depth: &id020
    - 4
    - 7
    num_trees: &id021
    - 4
    - 10
    chain_trees: &id022
    - false
    - true
    gflu_stages: &id023
    - 3
    - 8
    gflu_dropout: &id024
    - 0.0
    - 0.05
    tree_dropout: &id025
    - 0.0
    - 0.05
    tree_wise_attention_dropout: &id026
    - 0.0
    - 0.05
    embedding_dropout: &id027
    - 0
    - 0.2
    optimizer_fn: &id028
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id029
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id030
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
      attn_feature_importance: false
    learning_rate: &id031
    - 0.001
    - 0.0001
    batch_size: &id032
    - 512
    - 1024
    - 2048
    num_heads: &id033
    - 4
    - 10
    input_embed_dim_multiplier: &id034
    - 2
    - 6
    embedding_initialization: &id035
    - kaiming_uniform
    - kaiming_normal
    embedding_dropout: &id036
    - 0.05
    - 0.2
    shared_embedding_fraction: &id037
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id038
    - 4
    - 8
    attn_dropout: &id039
    - 0.05
    - 0.2
    add_norm_dropout: &id040
    - 0.05
    - 0.2
    ff_dropout: &id041
    - 0.05
    - 0.2
    ff_hidden_multiplier: &id042
    - 4
    - 32
    transformer_activation: &id043
    - GEGLU
    - ReGLU
    - SwiGLU
    optimizer_fn: &id044
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id045
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: &id046
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id047
    - 512
    - 1024
    - 2048
    learning_rate: &id048
    - 0.001
    - 0.0001
    layers: &id049
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    activation: &id050
    - ReLU
    - LeakyReLU
    - Tanh
    initialization: &id051
    - kaiming
    - xavier
    dropout: &id052
    - 0.0
    - 0.3
    embedding_dropout: &id053
    - 0.0
    - 0.3
    optimizer_fn: &id054
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id055
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id056
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    learning_rate: &id057
    - 0.001
    - 0.0001
    batch_size: &id058
    - 512
    - 1024
    - 2048
    gflu_stages: &id059
    - 4
    - 10
    gflu_dropout: &id060
    - 0.0
    - 0.3
    embedding_dropout: &id061
    - 0.0
    - 0.3
    optimizer_fn: &id062
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id063
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: node
  best_params: {}
  param_grid:
    outer_params: &id064
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id065
    - 512
    - 1024
    - 2048
    num_layers: &id066
    - 1
    - 2
    - 3
    num_trees: &id067
    - 12
    - 128
    additional_tree_output_dim: &id068
    - 2
    - 3
    - 4
    depth: &id069
    - 5
    - 7
    choice_function: &id070
    - entmax15
    - sparsemax
    bin_function: &id071
    - entmoid15
    - sparsemoid
    input_dropout: &id072
    - 0.0
    - 0.1
    embedding_dropout: &id073
    - 0.0
    - 0.1
    embed_categorical: &id074
    - true
    - false
    optimizer_fn: &id075
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id076
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id077
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 2
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id078
    - 512
    - 1024
    - 2048
    learning_rate: &id079
    - 0.001
    - 0.0001
    attn_embed_dim_multiplier: &id080
    - 2
    - 16
    num_heads: &id081
    - 2
    - 8
    num_attn_blocks: &id082
    - 2
    - 6
    attn_dropouts: &id083
    - 0.0
    - 0.3
    embedding_dim: &id084
    - 8
    - 32
    embedding_initialization: &id085
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id086
    - true
    - false
    share_embedding: &id087
    - true
    - false
    share_embedding_strategy: &id088
    - add
    - fraction
    shared_embedding_fraction: &id089
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id090
    - true
    - false
    layers: &id091
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout: &id092
    - 0.0
    - 0.3
    activation: &id093
    - ReLU
    - LeakyReLU
    - Tanh
    initialization: &id094
    - kaiming
    - xavier
    attention_pooling: &id095
    - true
    - false
    optimizer_fn: &id096
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id097
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id098
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size: &id099
    - 512
    - 1024
    - 2048
    learning_rate: &id100
    - 0.01
    - 0.0001
    embedding_bias: &id101
    - true
    - false
    embedding_initialization: &id102
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id103
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id104
    - 4
    - 10
    attn_dropout: &id105
    - 0.05
    - 0.3
    add_norm_dropout: &id106
    - 0.05
    - 0.3
    ff_dropout: &id107
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id108
    - 2
    - 6
    transformer_activation: &id109
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id110
    - 0.05
    - 0.3
    optimizer_fn: &id111
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id112
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: creditcard
  model: catboost
  best_params: {}
  param_grid:
    outer_params: &id113
      hyperopt_evals: 10
      validation_fraction: 0.15
      early_stopping_rounds: 100
      verbose: false
    iterations: &id114
    - 200
    - 2000
    learning_rate: &id115
    - 0.001
    - 0.1
    depth: &id116
    - 4
    - 10
    l2_leaf_reg: &id117
    - 0.5
    - 5.0
    min_child_samples: &id118
    - 1
    - 100
    bagging_temperature: &id119
    - 0.1
    - 2.0
- dataset: creditcard
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id120
      hyperopt_evals: 10
      validation_fraction: 0.15
      early_stopping_rounds: 30
      verbose: false
    n_estimators: &id121
    - 100
    - 5000
    max_bin: &id122
    - 256
    - 32
    tree_method: &id123
    - auto
    - hist
    max_depth: &id124
    - 4
    - 10
    learning_rate: &id125
    - 0.1
    - 0.33
    subsample: &id126
    - 0.7
    - 1.0
    colsample_bytree: &id127
    - 0.5
    - 1.0
    min_child_weight: &id128
    - 1
    - 10
    alpha: &id129
    - 0.0
    - 5.0
    gamma: &id130
    - 0.0
    - 5.0
    lambda: &id131
    - 0.0
    - 5.0
- dataset: creditcard
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id132
      cv_iterations: 10
      early_stopping: true
      cv_size: 5
      validation_fraction: 0.15
      n_iter_no_change: 6
      max_iter: 1000
    hidden_layer_sizes: &id133
    - - 64
      - 32
      - 16
    - - 256
      - 128
      - 64
      - 32
    - - 128
      - 64
      - 32
      - 16
    activation: &id134
    - relu
    - tanh
    - logistic
    solver: &id135
    - adam
    - lbfgs
    alpha: &id136
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id137
    - 0.0001
    - 0.01
    - 0.1
    beta_1: &id138
    - 0.99
    - 0.8
    beta_2: &id139
    - 0.999
    - 0.9
    batch_size: &id140
    - 512
    - 1024
    - 2048
- dataset: creditcard
  model: resnet
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.15
    batch_size:
    - 512
    - 1024
    - 2048
    resnet_depth:
    - resnet18
    - resnet34
    - resnet50
    optimizer_fn:
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: creditcard
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id001
    batch_size: *id002
    hidden_size: *id003
    optimizer_fn: *id004
    scheduler_fn: *id005
- dataset: creditcard
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id006
    virtual_batch_size_ratio: *id007
    batch_size: *id008
    weights: *id009
    mask_type: *id010
    n_d: *id011
    n_steps: *id012
    gamma: *id013
    n_independent: *id014
    n_shared: *id015
    optimizer_fn: *id016
    scheduler_fn: *id017
- dataset: creditcard
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id018
    batch_size: *id019
    tree_depth: *id020
    num_trees: *id021
    chain_trees: *id022
    gflu_stages: *id023
    gflu_dropout: *id024
    tree_dropout: *id025
    tree_wise_attention_dropout: *id026
    embedding_dropout: *id027
    optimizer_fn: *id028
    scheduler_fn: *id029
- dataset: creditcard
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id030
    learning_rate: *id031
    batch_size: *id032
    num_heads: *id033
    input_embed_dim_multiplier: *id034
    embedding_initialization: *id035
    embedding_dropout: *id036
    shared_embedding_fraction: *id037
    num_attn_blocks: *id038
    attn_dropout: *id039
    add_norm_dropout: *id040
    ff_dropout: *id041
    ff_hidden_multiplier: *id042
    transformer_activation: *id043
    optimizer_fn: *id044
    scheduler_fn: *id045
- dataset: creditcard
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id046
    batch_size: *id047
    learning_rate: *id048
    layers: *id049
    activation: *id050
    initialization: *id051
    dropout: *id052
    embedding_dropout: *id053
    optimizer_fn: *id054
    scheduler_fn: *id055
- dataset: creditcard
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id056
    learning_rate: *id057
    batch_size: *id058
    gflu_stages: *id059
    gflu_dropout: *id060
    embedding_dropout: *id061
    optimizer_fn: *id062
    scheduler_fn: *id063
- dataset: creditcard
  model: node
  best_params: {}
  param_grid:
    outer_params: *id064
    batch_size: *id065
    num_layers: *id066
    num_trees: *id067
    additional_tree_output_dim: *id068
    depth: *id069
    choice_function: *id070
    bin_function: *id071
    input_dropout: *id072
    embedding_dropout: *id073
    embed_categorical: *id074
    optimizer_fn: *id075
    scheduler_fn: *id076
- dataset: creditcard
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id077
    batch_size: *id078
    learning_rate: *id079
    attn_embed_dim_multiplier: *id080
    num_heads: *id081
    num_attn_blocks: *id082
    attn_dropouts: *id083
    embedding_dim: *id084
    embedding_initialization: *id085
    embedding_bias: *id086
    share_embedding: *id087
    share_embedding_strategy: *id088
    shared_embedding_fraction: *id089
    deep_layers: *id090
    layers: *id091
    dropout: *id092
    activation: *id093
    initialization: *id094
    attention_pooling: *id095
    optimizer_fn: *id096
    scheduler_fn: *id097
- dataset: creditcard
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id098
    batch_size: *id099
    learning_rate: *id100
    embedding_bias: *id101
    embedding_initialization: *id102
    shared_embedding_fraction: *id103
    num_attn_blocks: *id104
    attn_dropout: *id105
    add_norm_dropout: *id106
    ff_dropout: *id107
    ff_hidden_multiplier: *id108
    transformer_activation: *id109
    embedding_dropout: *id110
    optimizer_fn: *id111
    scheduler_fn: *id112
- dataset: covertype
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id113
    iterations: *id114
    learning_rate: *id115
    depth: *id116
    l2_leaf_reg: *id117
    min_child_samples: *id118
    bagging_temperature: *id119
- dataset: covertype
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id120
    n_estimators: *id121
    max_bin: *id122
    tree_method: *id123
    max_depth: *id124
    learning_rate: *id125
    subsample: *id126
    colsample_bytree: *id127
    min_child_weight: *id128
    alpha: *id129
    gamma: *id130
    lambda: *id131
- dataset: covertype
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id132
    hidden_layer_sizes: *id133
    activation: *id134
    solver: *id135
    alpha: *id136
    learning_rate_init: *id137
    beta_1: *id138
    beta_2: *id139
    batch_size: *id140
- dataset: covertype
  model: resnet
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.15
    batch_size:
    - 1024
    - 2048
    - 4096
    resnet_depth:
    - resnet18
    - resnet34
    - resnet50
    optimizer_fn:
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      max_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    hidden_size:
    - 1024
    - 2048
    - 4096
    optimizer_fn:
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: tabnet
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    virtual_batch_size_ratio:
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size:
    - 1024
    - 2048
    - 4096
    weights:
    - 0
    - 1
    mask_type:
    - sparsemax
    - entmax
    n_d:
    - 6
    - 32
    n_steps:
    - 1
    - 6
    gamma:
    - 1.0
    - 2.0
    n_independent:
    - 1
    - 3
    n_shared:
    - 1
    - 3
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: gate
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    tree_depth:
    - 4
    - 7
    num_trees:
    - 4
    - 10
    chain_trees:
    - false
    - true
    gflu_stages:
    - 3
    - 8
    gflu_dropout:
    - 0.0
    - 0.05
    tree_dropout:
    - 0.0
    - 0.05
    tree_wise_attention_dropout:
    - 0.0
    - 0.05
    embedding_dropout:
    - 0
    - 0.2
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
      attn_feature_importance: false
    learning_rate:
    - 0.001
    - 0.0001
    batch_size:
    - 1024
    - 2048
    - 4096
    num_heads:
    - 4
    - 10
    input_embed_dim_multiplier:
    - 2
    - 6
    embedding_initialization:
    - kaiming_uniform
    - kaiming_normal
    embedding_dropout:
    - 0.05
    - 0.2
    shared_embedding_fraction:
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks:
    - 4
    - 8
    attn_dropout:
    - 0.05
    - 0.2
    add_norm_dropout:
    - 0.05
    - 0.2
    ff_dropout:
    - 0.05
    - 0.2
    ff_hidden_multiplier:
    - 4
    - 32
    transformer_activation:
    - GEGLU
    - ReGLU
    - SwiGLU
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    learning_rate:
    - 0.001
    - 0.0001
    layers:
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    activation:
    - ReLU
    - LeakyReLU
    - Tanh
    initialization:
    - kaiming
    - xavier
    dropout:
    - 0.0
    - 0.3
    embedding_dropout:
    - 0.0
    - 0.3
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: gandalf
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    learning_rate:
    - 0.001
    - 0.0001
    batch_size:
    - 1024
    - 2048
    - 4096
    gflu_stages:
    - 4
    - 10
    gflu_dropout:
    - 0.0
    - 0.3
    embedding_dropout:
    - 0.0
    - 0.3
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: node
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    num_layers:
    - 1
    - 2
    - 3
    num_trees:
    - 12
    - 128
    additional_tree_output_dim:
    - 2
    - 3
    - 4
    depth:
    - 5
    - 7
    choice_function:
    - entmax15
    - sparsemax
    bin_function:
    - entmoid15
    - sparsemoid
    input_dropout:
    - 0.0
    - 0.1
    embedding_dropout:
    - 0.0
    - 0.1
    embed_categorical:
    - true
    - false
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: autoint
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 2
      val_size: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    learning_rate:
    - 0.001
    - 0.0001
    attn_embed_dim_multiplier:
    - 2
    - 16
    num_heads:
    - 2
    - 8
    num_attn_blocks:
    - 2
    - 6
    attn_dropouts:
    - 0.0
    - 0.3
    embedding_dim:
    - 8
    - 32
    embedding_initialization:
    - kaiming_uniform
    - kaiming_normal
    embedding_bias:
    - true
    - false
    share_embedding:
    - true
    - false
    share_embedding_strategy:
    - add
    - fraction
    shared_embedding_fraction:
    - 0.25
    - 0.1
    - 0.5
    deep_layers:
    - true
    - false
    layers:
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout:
    - 0.0
    - 0.3
    activation:
    - ReLU
    - LeakyReLU
    - Tanh
    initialization:
    - kaiming
    - xavier
    attention_pooling:
    - true
    - false
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params:
      hyperopt_evals: 10
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 6
    batch_size:
    - 1024
    - 2048
    - 4096
    learning_rate:
    - 0.01
    - 0.0001
    embedding_bias:
    - true
    - false
    embedding_initialization:
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction:
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks:
    - 4
    - 10
    attn_dropout:
    - 0.05
    - 0.3
    add_norm_dropout:
    - 0.05
    - 0.3
    ff_dropout:
    - 0.05
    - 0.3
    ff_hidden_multiplier:
    - 2
    - 6
    transformer_activation:
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout:
    - 0.05
    - 0.3
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
