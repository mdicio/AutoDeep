- dataset: titanic
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 128
      validation_fraction: 0.2
      early_stopping_rounds: 30
      verbose: false
    n_estimators: &id002
    - 100
    - 5000
    max_bin: &id003
    - 256
    - 32
    tree_method: &id004
    - auto
    - hist
    max_depth: &id005
    - 4
    - 12
    learning_rate: &id006
    - 0.1
    - 0.33
    subsample: &id007
    - 0.7
    - 1.0
    colsample_bytree: &id008
    - 0.5
    - 1.0
    min_child_weight: &id009
    - 1
    - 10
    alpha: &id010
    - 0.0
    - 5.0
    gamma: &id011
    - 0.0
    - 5.0
    lambda: &id012
    - 0.0
    - 5.0
- dataset: titanic
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id013
      cv_iterations: 2
      early_stopping: true
      cv_size: 4
      validation_fraction: 0.2
      n_iter_no_change: 5
      max_iter: 1000
    hidden_layer_sizes: &id014
    - - 8
    - - 16
      - 8
    - - 32
      - 16
    - - 128
      - 64
      - 32
    activation: &id015
    - relu
    - tanh
    - logistic
    solver: &id016
    - adam
    - lbfgs
    alpha: &id017
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id018
    - 0.001
    - 0.01
    - 0.1
    beta_1: &id019
    - 0.9
    - 0.99
    - 0.8
    beta_2: &id020
    - 0.999
    - 0.9
    batch_size: &id021
    - 8
    - 16
    - 32
    - 64
    - 128
- dataset: titanic
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: &id022
      hyperopt_evals: 2
      num_epochs: 2
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.2
    learning_rate: &id023
    - 0.0001
    - 0.001
    batch_size: &id024
    - 8
    - 16
    - 32
    - 64
    - 128
    scheduler_factor: &id025
    - 0.1
    - 0.9
    scheduler_patience: &id026
    - 10
    - 15
    - 20
- dataset: titanic
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id027
      hyperopt_evals: 100
      num_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.2
      early_stopping_patience: 10
    batch_size: &id028
    - 8
    - 16
    - 32
    - 64
    - 128
    learning_rate: &id029
    - 0.0001
    - 0.01
    hidden_size: &id030
    - 1024
    - 2048
    - 4096
    scheduler_factor: &id031
    - 0.1
    - 0.9
    scheduler_patience: &id032
    - 5
    - 15
- dataset: titanic
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id033
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id034
    - 0.001
    - 0.0001
    virtual_batch_size_ratio: &id035
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id036
    - 16
    - 32
    - 64
    weights: &id037
    - 0
    - 1
    mask_type: &id038
    - sparsemax
    - entmax
    n_d: &id039
    - 4
    - 12
    n_steps: &id040
    - 3
    - 10
    gamma: &id041
    - 1.0
    - 2.0
    n_independent: &id042
    - 2
    - 3
    n_shared: &id043
    - 2
    - 3
    optimizer_fn: &id044
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id045
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id046
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id047
    - 0.001
    - 0.0001
    batch_size: &id048
    - 16
    - 32
    - 64
    - 128
    tree_depth: &id049
    - 4
    - 7
    num_trees: &id050
    - 6
    - 12
    chain_trees: &id051
    - false
    gflu_stages: &id052
    - 2
    - 4
    optimizer_fn: &id053
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id054
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id055
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id056
    - 0.001
    - 0.0001
    batch_size: &id057
    - 16
    - 32
    - 64
    optimizer_fn: &id058
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id059
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: &id060
      hyperopt_evals: 320
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
      precision: 16
    batch_size: &id061
    - 8
    - 16
    - 32
    - 64
    learning_rate: &id062
    - 0.001
    - 0.0001
    layers: &id063
    - 128-64-32
    - 64-32-16
    - 1024-512-32
    - 512-128-32
    activation: &id064
    - ReLU
    - Tanh
    - LeakyReLU
    initialization: &id065
    - kaiming
    - xavier
    dropout: &id066
    - 0.0
    - 0.3
    optimizer_fn: &id067
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id068
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 10
        - 20
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id069
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id070
    - 0.001
    - 0.0001
    batch_size: &id071
    - 16
    - 32
    - 64
    optimizer_fn: &id072
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id073
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: node
  best_params: {}
  param_grid:
    outer_params: &id074
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    num_layers: &id075
    - 1
    - 2
    num_trees: &id076
    - 1024
    - 2048
    additional_tree_output_dim: &id077
    - 2
    - 3
    - 4
    depth: &id078
    - 4
    - 7
    choice_function: &id079
    - entmax15
    - sparsemax
    bin_function: &id080
    - entmoid15
    - sparsemoid
    max_features: &id081
    - null
    - 50
    - 100
    input_dropout: &id082
    - 0.0
    - 0.2
    - 0.4
    initialize_response: &id083
    - normal
    - uniform
    initialize_selection_logits: &id084
    - uniform
    - normal
    threshold_init_beta: &id085
    - 0.1
    - 1.0
    - 10.0
    threshold_init_cutoff: &id086
    - 0.5
    - 0.9
    - 1.5
    cat_embedding_dropout: &id087
    - 0.0
    - 0.1
    - 0.2
    embed_categorical: &id088
    - true
    - false
    head: &id089
    - LinearHead
    embedding_dropout: &id090
    - 0.0
    - 0.1
    - 0.2
    batch_norm_continuous_input: &id091
    - true
    - false
    learning_rate: &id092
    - 0.001
    - 0.0001
    batch_size: &id093
    - 8
    - 16
    - 32
    - 64
    - 128
    optimizer_fn: &id094
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id095
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id096
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    batch_size: &id097
    - 8
    - 16
    - 32
    - 64
    learning_rate: &id098
    - 0.001
    - 0.0001
    optimizer_fn: &id099
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id100
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id101
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    batch_size: &id102
    - 8
    - 16
    - 32
    - 64
    learning_rate: &id103
    - 0.001
    - 0.0001
    optimizer_fn: &id104
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id105
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: titanic
  model: catboost
  best_params: {}
  param_grid:
    outer_params: &id106
      hyperopt_evals: 128
      validation_fraction: 0.2
      early_stopping_rounds: 100
      verbose: false
    iterations: &id107
    - 200
    - 2000
    learning_rate: &id108
    - 0.001
    - 0.1
    depth: &id109
    - 4
    - 10
    colsample_bylevel: &id110
    - 0.05
    - 1.0
    l2_leaf_reg: &id111
    - 0.5
    - 5.0
    min_child_samples: &id112
    - 1
    - 100
    bagging_temperature: &id113
    - 0.1
    - 2.0
- dataset: titanic
  model: lightgbm
  best_params:
    outer_params: &id114
      hyperopt_evals: 32
      validation_fraction: 0.2
      early_stopping_rounds: 100
      verbose: false
    num_leaves: 31
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 10
    min_child_weight: 0.001
    min_child_samples: 4
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
  param_grid:
    outer_params: &id115
      hyperopt_evals: 32
      validation_fraction: 0.2
      early_stopping_rounds: 100
      verbose: false
    num_iterations: &id116
    - 100
    - 500
    num_leaves: &id117
    - 28
    - 31
    max_depth: &id118
    - 6
    - 10
    learning_rate: &id119
    - 0.1
    - 0.2
    n_estimators: &id120
    - 10
    - 50
    min_child_weight: &id121
    - 0.001
    - 0.01
    min_child_samples: &id122
    - 10
    - 20
    subsample: &id123
    - 0.8
    - 1.0
    reg_alpha: &id124
    - 0.0
    - 5.0
    reg_lambda: &id125
    - 0.0
    - 5.0
- dataset: iris
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id001
    n_estimators: *id002
    max_bin: *id003
    tree_method: *id004
    max_depth: *id005
    learning_rate: *id006
    subsample: *id007
    colsample_bytree: *id008
    min_child_weight: *id009
    alpha: *id010
    gamma: *id011
    lambda: *id012
- dataset: iris
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id013
    hidden_layer_sizes: *id014
    activation: *id015
    solver: *id016
    alpha: *id017
    learning_rate_init: *id018
    beta_1: *id019
    beta_2: *id020
    batch_size: *id021
- dataset: iris
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: *id022
    learning_rate: *id023
    batch_size: *id024
    scheduler_factor: *id025
    scheduler_patience: *id026
- dataset: iris
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id027
    batch_size: *id028
    learning_rate: *id029
    hidden_size: *id030
    scheduler_factor: *id031
    scheduler_patience: *id032
- dataset: iris
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id033
    learning_rate: *id034
    virtual_batch_size_ratio: *id035
    batch_size: *id036
    weights: *id037
    mask_type: *id038
    n_d: *id039
    n_steps: *id040
    gamma: *id041
    n_independent: *id042
    n_shared: *id043
    optimizer_fn: *id044
    scheduler_fn: *id045
- dataset: iris
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id046
    learning_rate: *id047
    batch_size: *id048
    tree_depth: *id049
    num_trees: *id050
    chain_trees: *id051
    gflu_stages: *id052
    optimizer_fn: *id053
    scheduler_fn: *id054
- dataset: iris
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id055
    learning_rate: *id056
    batch_size: *id057
    optimizer_fn: *id058
    scheduler_fn: *id059
- dataset: iris
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id060
    batch_size: *id061
    learning_rate: *id062
    layers: *id063
    activation: *id064
    initialization: *id065
    dropout: *id066
    optimizer_fn: *id067
    scheduler_fn: *id068
- dataset: iris
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id069
    learning_rate: *id070
    batch_size: *id071
    optimizer_fn: *id072
    scheduler_fn: *id073
- dataset: iris
  model: node
  best_params: {}
  param_grid:
    outer_params: *id074
    num_layers: *id075
    num_trees: *id076
    additional_tree_output_dim: *id077
    depth: *id078
    choice_function: *id079
    bin_function: *id080
    max_features: *id081
    input_dropout: *id082
    initialize_response: *id083
    initialize_selection_logits: *id084
    threshold_init_beta: *id085
    threshold_init_cutoff: *id086
    cat_embedding_dropout: *id087
    embed_categorical: *id088
    head: *id089
    embedding_dropout: *id090
    batch_norm_continuous_input: *id091
    learning_rate: *id092
    batch_size: *id093
    optimizer_fn: *id094
    scheduler_fn: *id095
- dataset: iris
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id096
    batch_size: *id097
    learning_rate: *id098
    optimizer_fn: *id099
    scheduler_fn: *id100
- dataset: iris
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id101
    batch_size: *id102
    learning_rate: *id103
    optimizer_fn: *id104
    scheduler_fn: *id105
- dataset: iris
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id106
    iterations: *id107
    learning_rate: *id108
    depth: *id109
    colsample_bylevel: *id110
    l2_leaf_reg: *id111
    min_child_samples: *id112
    bagging_temperature: *id113
- dataset: iris
  model: lightgbm
  best_params:
    outer_params: *id114
    num_leaves: 31
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 10
    min_child_weight: 0.001
    min_child_samples: 4
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
  param_grid:
    outer_params: *id115
    num_iterations: *id116
    num_leaves: *id117
    max_depth: *id118
    learning_rate: *id119
    n_estimators: *id120
    min_child_weight: *id121
    min_child_samples: *id122
    subsample: *id123
    reg_alpha: *id124
    reg_lambda: *id125
- dataset: housing
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id126
      hyperopt_evals: 32
      validation_fraction: 0.2
      early_stopping_rounds: 30
      verbose: false
    n_estimators: &id127
    - 100
    - 5000
    max_bin: &id128
    - 256
    - 32
    tree_method: &id129
    - auto
    - hist
    max_depth: &id130
    - 4
    - 10
    learning_rate: &id131
    - 0.1
    - 0.33
    subsample: &id132
    - 0.7
    - 1.0
    colsample_bytree: &id133
    - 0.5
    - 1.0
    min_child_weight: &id134
    - 1
    - 10
    alpha: &id135
    - 0.0
    - 5.0
    gamma: &id136
    - 0.0
    - 5.0
    lambda: &id137
    - 0.0
    - 5.0
- dataset: housing
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id138
      cv_iterations: 10
      early_stopping: true
      cv_size: 5
      validation_fraction: 0.2
      n_iter_no_change: 5
      max_iter: 1000
    hidden_layer_sizes: &id139
    - - 16
      - 32
    - - 16
      - 8
    - - 32
      - 16
    - - 128
      - 64
      - 32
    activation: &id140
    - relu
    - tanh
    - logistic
    solver: &id141
    - adam
    - lbfgs
    alpha: &id142
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id143
    - 0.001
    - 0.01
    - 0.1
    beta_1: &id144
    - 0.9
    - 0.99
    - 0.8
    beta_2: &id145
    - 0.999
    - 0.9
    batch_size: &id146
    - 256
    - 512
    - 1024
    - 2048
- dataset: housing
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: &id147
      hyperopt_evals: 32
      num_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      validation_fraction: 0.2
    learning_rate: &id148
    - 0.0001
    - 0.001
    batch_size: &id149
    - 256
    - 512
    - 1024
    - 2048
    scheduler_factor: &id150
    - 0.1
    - 0.9
    scheduler_patience: &id151
    - 3
    - 5
- dataset: housing
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id152
      hyperopt_evals: 100
      num_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.2
      early_stopping_patience: 6
    batch_size: &id153
    - 256
    - 512
    - 1024
    - 2048
    learning_rate: &id154
    - 0.01
    - 0.0001
    hidden_size: &id155
    - 1024
    - 2048
    - 4096
    scheduler_factor: &id156
    - 0.1
    - 0.9
    scheduler_patience: &id157
    - 4
    - 7
- dataset: housing
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id158
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id159
    - 0.001
    - 0.0001
    virtual_batch_size_ratio: &id160
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id161
    - 256
    - 512
    - 1024
    - 2048
    weights: &id162
    - 0
    - 1
    mask_type: &id163
    - sparsemax
    - entmax
    n_d: &id164
    - 6
    - 12
    n_steps: &id165
    - 3
    - 10
    gamma: &id166
    - 1.0
    - 2.0
    n_independent: &id167
    - 2
    - 3
    n_shared: &id168
    - 2
    - 3
    optimizer_fn: &id169
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id170
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id171
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id172
    - 0.001
    - 0.0001
    batch_size: &id173
    - 256
    - 512
    - 1024
    - 2048
    tree_depth: &id174
    - 4
    - 7
    num_trees: &id175
    - 8
    - 22
    chain_trees: &id176
    - false
    gflu_stages: &id177
    - 2
    - 4
    optimizer_fn: &id178
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id179
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id180
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id181
    - 0.001
    - 0.0001
    batch_size: &id182
    - 256
    - 512
    - 1024
    - 2048
    embedding_initialization: &id183
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id184
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id185
    - 4
    - 10
    attn_dropout: &id186
    - 0.05
    - 0.3
    add_norm_dropout: &id187
    - 0.05
    - 0.3
    ff_dropout: &id188
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id189
    - 2
    - 6
    transformer_activation: &id190
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id191
    - 0.05
    - 0.3
    optimizer_fn: &id192
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id193
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: &id194
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
      precision: 16
    batch_size: &id195
    - 256
    - 512
    - 1024
    - 2048
    learning_rate: &id196
    - 0.001
    - 0.0001
    layers: &id197
    - 128-64-32
    - 64-32-16
    - 512-128-32
    activation: &id198
    - ReLU
    - LeakyReLU
    initialization: &id199
    - kaiming
    - xavier
    dropout: &id200
    - 0.0
    - 0.3
    optimizer_fn: &id201
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id202
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id203
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id204
    - 0.001
    - 0.0001
    batch_size: &id205
    - 256
    - 512
    - 1024
    - 2048
    gflu_stages: &id206
    - 2
    - 8
    gflu_dropout: &id207
    - 0.0
    - 0.3
    embedding_dropout: &id208
    - 0.0
    - 0.3
    optimizer_fn: &id209
      Adam:
        weight_decay:
        - 0.0
        - 0.01
      AdamW:
        weight_decay:
        - 0.0
        - 0.01
    scheduler_fn: &id210
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 10
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: node
  best_params: {}
  param_grid:
    outer_params: &id211
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    learning_rate: &id212
    - 0.01
    - 0.0001
    batch_size: &id213
    - 256
    - 512
    - 1024
    - 2048
    num_layers: &id214
    - 1
    - 2
    num_trees: &id215
    - 1024
    - 2048
    - 4096
    additional_tree_output_dim: &id216
    - 2
    - 3
    - 4
    depth: &id217
    - 4
    - 8
    choice_function: &id218
    - entmax15
    - sparsemax
    bin_function: &id219
    - entmoid15
    - sparsemoid
    input_dropout: &id220
    - 0.0
    - 0.3
    embed_categorical: &id221
    - true
    - false
    optimizer_fn: &id222
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id223
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id224
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    batch_size: &id225
    - 256
    - 512
    - 1024
    - 2048
    learning_rate: &id226
    - 0.001
    - 0.0001
    optimizer_fn: &id227
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id228
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id229
      hyperopt_evals: 32
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 6
    batch_size: &id230
    - 256
    - 512
    - 1024
    - 2048
    learning_rate: &id231
    - 0.01
    - 0.0001
    embedding_bias: &id232
    - true
    - false
    embedding_initialization: &id233
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id234
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id235
    - 4
    - 10
    attn_dropout: &id236
    - 0.05
    - 0.3
    add_norm_dropout: &id237
    - 0.05
    - 0.3
    ff_dropout: &id238
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id239
    - 2
    - 6
    transformer_activation: &id240
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id241
    - 0.05
    - 0.3
    optimizer_fn: &id242
      Adam:
        weight_decay:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 0.0001
        - 0.01
    scheduler_fn: &id243
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
      StepLR:
        step_size:
        - 5
        - 30
        gamma:
        - 0.9
        - 0.99
- dataset: housing
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id106
    iterations: *id107
    learning_rate: *id108
    depth: *id109
    colsample_bylevel: *id110
    l2_leaf_reg: *id111
    min_child_samples: *id112
    bagging_temperature: *id113
- dataset: housing
  model: lightgbm
  best_params:
    outer_params: *id114
    num_leaves: 31
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 10
    min_child_weight: 0.001
    min_child_samples: 4
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
  param_grid:
    outer_params: *id115
    num_iterations: *id116
    num_leaves: *id117
    max_depth: *id118
    learning_rate: *id119
    n_estimators: *id120
    min_child_weight: *id121
    min_child_samples: *id122
    subsample: *id123
    reg_alpha: *id124
    reg_lambda: *id125
- dataset: adult
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id126
    n_estimators: *id127
    max_bin: *id128
    tree_method: *id129
    max_depth: *id130
    learning_rate: *id131
    subsample: *id132
    colsample_bytree: *id133
    min_child_weight: *id134
    alpha: *id135
    gamma: *id136
    lambda: *id137
- dataset: adult
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id138
    hidden_layer_sizes: *id139
    activation: *id140
    solver: *id141
    alpha: *id142
    learning_rate_init: *id143
    beta_1: *id144
    beta_2: *id145
    batch_size: *id146
- dataset: adult
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: *id147
    learning_rate: *id148
    batch_size: *id149
    scheduler_factor: *id150
    scheduler_patience: *id151
- dataset: adult
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id152
    batch_size: *id153
    learning_rate: *id154
    hidden_size: *id155
    scheduler_factor: *id156
    scheduler_patience: *id157
- dataset: adult
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id158
    learning_rate: *id159
    virtual_batch_size_ratio: *id160
    batch_size: *id161
    weights: *id162
    mask_type: *id163
    n_d: *id164
    n_steps: *id165
    gamma: *id166
    n_independent: *id167
    n_shared: *id168
    optimizer_fn: *id169
    scheduler_fn: *id170
- dataset: adult
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id171
    learning_rate: *id172
    batch_size: *id173
    tree_depth: *id174
    num_trees: *id175
    chain_trees: *id176
    gflu_stages: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: adult
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    embedding_initialization: *id183
    shared_embedding_fraction: *id184
    num_attn_blocks: *id185
    attn_dropout: *id186
    add_norm_dropout: *id187
    ff_dropout: *id188
    ff_hidden_multiplier: *id189
    transformer_activation: *id190
    embedding_dropout: *id191
    optimizer_fn: *id192
    scheduler_fn: *id193
- dataset: adult
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id194
    batch_size: *id195
    learning_rate: *id196
    layers: *id197
    activation: *id198
    initialization: *id199
    dropout: *id200
    optimizer_fn: *id201
    scheduler_fn: *id202
- dataset: adult
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id203
    learning_rate: *id204
    batch_size: *id205
    gflu_stages: *id206
    gflu_dropout: *id207
    embedding_dropout: *id208
    optimizer_fn: *id209
    scheduler_fn: *id210
- dataset: adult
  model: node
  best_params: {}
  param_grid:
    outer_params: *id211
    learning_rate: *id212
    batch_size: *id213
    num_layers: *id214
    num_trees: *id215
    additional_tree_output_dim: *id216
    depth: *id217
    choice_function: *id218
    bin_function: *id219
    input_dropout: *id220
    embed_categorical: *id221
    optimizer_fn: *id222
    scheduler_fn: *id223
- dataset: adult
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id224
    batch_size: *id225
    learning_rate: *id226
    optimizer_fn: *id227
    scheduler_fn: *id228
- dataset: adult
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id229
    batch_size: *id230
    learning_rate: *id231
    embedding_bias: *id232
    embedding_initialization: *id233
    shared_embedding_fraction: *id234
    num_attn_blocks: *id235
    attn_dropout: *id236
    add_norm_dropout: *id237
    ff_dropout: *id238
    ff_hidden_multiplier: *id239
    transformer_activation: *id240
    embedding_dropout: *id241
    optimizer_fn: *id242
    scheduler_fn: *id243
- dataset: heloc
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id126
    n_estimators: *id127
    max_bin: *id128
    tree_method: *id129
    max_depth: *id130
    learning_rate: *id131
    subsample: *id132
    colsample_bytree: *id133
    min_child_weight: *id134
    alpha: *id135
    gamma: *id136
    lambda: *id137
- dataset: heloc
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id138
    hidden_layer_sizes: *id139
    activation: *id140
    solver: *id141
    alpha: *id142
    learning_rate_init: *id143
    beta_1: *id144
    beta_2: *id145
    batch_size: *id146
- dataset: heloc
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: *id147
    learning_rate: *id148
    batch_size: *id149
    scheduler_factor: *id150
    scheduler_patience: *id151
- dataset: heloc
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id152
    batch_size: *id153
    learning_rate: *id154
    hidden_size: *id155
    scheduler_factor: *id156
    scheduler_patience: *id157
- dataset: heloc
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id158
    learning_rate: *id159
    virtual_batch_size_ratio: *id160
    batch_size: *id161
    weights: *id162
    mask_type: *id163
    n_d: *id164
    n_steps: *id165
    gamma: *id166
    n_independent: *id167
    n_shared: *id168
    optimizer_fn: *id169
    scheduler_fn: *id170
- dataset: heloc
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id171
    learning_rate: *id172
    batch_size: *id173
    tree_depth: *id174
    num_trees: *id175
    chain_trees: *id176
    gflu_stages: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: heloc
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    embedding_initialization: *id183
    shared_embedding_fraction: *id184
    num_attn_blocks: *id185
    attn_dropout: *id186
    add_norm_dropout: *id187
    ff_dropout: *id188
    ff_hidden_multiplier: *id189
    transformer_activation: *id190
    embedding_dropout: *id191
    optimizer_fn: *id192
    scheduler_fn: *id193
- dataset: heloc
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id194
    batch_size: *id195
    learning_rate: *id196
    layers: *id197
    activation: *id198
    initialization: *id199
    dropout: *id200
    optimizer_fn: *id201
    scheduler_fn: *id202
- dataset: heloc
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id203
    learning_rate: *id204
    batch_size: *id205
    gflu_stages: *id206
    gflu_dropout: *id207
    embedding_dropout: *id208
    optimizer_fn: *id209
    scheduler_fn: *id210
- dataset: heloc
  model: node
  best_params: {}
  param_grid:
    outer_params: *id211
    learning_rate: *id212
    batch_size: *id213
    num_layers: *id214
    num_trees: *id215
    additional_tree_output_dim: *id216
    depth: *id217
    choice_function: *id218
    bin_function: *id219
    input_dropout: *id220
    embed_categorical: *id221
    optimizer_fn: *id222
    scheduler_fn: *id223
- dataset: heloc
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id224
    batch_size: *id225
    learning_rate: *id226
    optimizer_fn: *id227
    scheduler_fn: *id228
- dataset: heloc
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id229
    batch_size: *id230
    learning_rate: *id231
    embedding_bias: *id232
    embedding_initialization: *id233
    shared_embedding_fraction: *id234
    num_attn_blocks: *id235
    attn_dropout: *id236
    add_norm_dropout: *id237
    ff_dropout: *id238
    ff_hidden_multiplier: *id239
    transformer_activation: *id240
    embedding_dropout: *id241
    optimizer_fn: *id242
    scheduler_fn: *id243
- dataset: heloc
  model: catboost
  best_params: {}
  param_grid:
    outer_params: &id244
      hyperopt_evals: 32
      validation_fraction: 0.2
      early_stopping_rounds: 100
      verbose: false
    iterations: &id245
    - 200
    - 2000
    learning_rate: &id246
    - 0.001
    - 0.1
    depth: &id247
    - 4
    - 10
    colsample_bylevel: &id248
    - 0.05
    - 1.0
    l2_leaf_reg: &id249
    - 0.5
    - 5.0
    min_child_samples: &id250
    - 1
    - 100
    bagging_temperature: &id251
    - 0.1
    - 2.0
- dataset: covertype
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id126
    n_estimators: *id127
    max_bin: *id128
    tree_method: *id129
    max_depth: *id130
    learning_rate: *id131
    subsample: *id132
    colsample_bytree: *id133
    min_child_weight: *id134
    alpha: *id135
    gamma: *id136
    lambda: *id137
- dataset: covertype
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id138
    hidden_layer_sizes: *id139
    activation: *id140
    solver: *id141
    alpha: *id142
    learning_rate_init: *id143
    beta_1: *id144
    beta_2: *id145
    batch_size: *id146
- dataset: covertype
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: *id147
    learning_rate: *id148
    batch_size: *id149
    scheduler_factor: *id150
    scheduler_patience: *id151
- dataset: covertype
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id152
    batch_size: *id153
    learning_rate: *id154
    hidden_size: *id155
    scheduler_factor: *id156
    scheduler_patience: *id157
- dataset: covertype
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id158
    learning_rate: *id159
    virtual_batch_size_ratio: *id160
    batch_size: *id161
    weights: *id162
    mask_type: *id163
    n_d: *id164
    n_steps: *id165
    gamma: *id166
    n_independent: *id167
    n_shared: *id168
    optimizer_fn: *id169
    scheduler_fn: *id170
- dataset: covertype
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id171
    learning_rate: *id172
    batch_size: *id173
    tree_depth: *id174
    num_trees: *id175
    chain_trees: *id176
    gflu_stages: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: covertype
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    embedding_initialization: *id183
    shared_embedding_fraction: *id184
    num_attn_blocks: *id185
    attn_dropout: *id186
    add_norm_dropout: *id187
    ff_dropout: *id188
    ff_hidden_multiplier: *id189
    transformer_activation: *id190
    embedding_dropout: *id191
    optimizer_fn: *id192
    scheduler_fn: *id193
- dataset: covertype
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id194
    batch_size: *id195
    learning_rate: *id196
    layers: *id197
    activation: *id198
    initialization: *id199
    dropout: *id200
    optimizer_fn: *id201
    scheduler_fn: *id202
- dataset: covertype
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id203
    learning_rate: *id204
    batch_size: *id205
    gflu_stages: *id206
    gflu_dropout: *id207
    embedding_dropout: *id208
    optimizer_fn: *id209
    scheduler_fn: *id210
- dataset: covertype
  model: node
  best_params: {}
  param_grid:
    outer_params: *id211
    learning_rate: *id212
    batch_size: *id213
    num_layers: *id214
    num_trees: *id215
    additional_tree_output_dim: *id216
    depth: *id217
    choice_function: *id218
    bin_function: *id219
    input_dropout: *id220
    embed_categorical: *id221
    optimizer_fn: *id222
    scheduler_fn: *id223
- dataset: covertype
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id224
    batch_size: *id225
    learning_rate: *id226
    optimizer_fn: *id227
    scheduler_fn: *id228
- dataset: covertype
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id229
    batch_size: *id230
    learning_rate: *id231
    embedding_bias: *id232
    embedding_initialization: *id233
    shared_embedding_fraction: *id234
    num_attn_blocks: *id235
    attn_dropout: *id236
    add_norm_dropout: *id237
    ff_dropout: *id238
    ff_hidden_multiplier: *id239
    transformer_activation: *id240
    embedding_dropout: *id241
    optimizer_fn: *id242
    scheduler_fn: *id243
- dataset: covertype
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id244
    iterations: *id245
    learning_rate: *id246
    depth: *id247
    colsample_bylevel: *id248
    l2_leaf_reg: *id249
    min_child_samples: *id250
    bagging_temperature: *id251
- dataset: creditcard
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id126
    n_estimators: *id127
    max_bin: *id128
    tree_method: *id129
    max_depth: *id130
    learning_rate: *id131
    subsample: *id132
    colsample_bytree: *id133
    min_child_weight: *id134
    alpha: *id135
    gamma: *id136
    lambda: *id137
- dataset: creditcard
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id138
    hidden_layer_sizes: *id139
    activation: *id140
    solver: *id141
    alpha: *id142
    learning_rate_init: *id143
    beta_1: *id144
    beta_2: *id145
    batch_size: *id146
- dataset: creditcard
  model: resnet18
  best_params: {}
  param_grid:
    outer_params: *id147
    learning_rate: *id148
    batch_size: *id149
    scheduler_factor: *id150
    scheduler_patience: *id151
- dataset: creditcard
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id152
    batch_size: *id153
    learning_rate: *id154
    hidden_size: *id155
    scheduler_factor: *id156
    scheduler_patience: *id157
- dataset: creditcard
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id158
    learning_rate: *id159
    virtual_batch_size_ratio: *id160
    batch_size: *id161
    weights: *id162
    mask_type: *id163
    n_d: *id164
    n_steps: *id165
    gamma: *id166
    n_independent: *id167
    n_shared: *id168
    optimizer_fn: *id169
    scheduler_fn: *id170
- dataset: creditcard
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id171
    learning_rate: *id172
    batch_size: *id173
    tree_depth: *id174
    num_trees: *id175
    chain_trees: *id176
    gflu_stages: *id177
    optimizer_fn: *id178
    scheduler_fn: *id179
- dataset: creditcard
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id180
    learning_rate: *id181
    batch_size: *id182
    embedding_initialization: *id183
    shared_embedding_fraction: *id184
    num_attn_blocks: *id185
    attn_dropout: *id186
    add_norm_dropout: *id187
    ff_dropout: *id188
    ff_hidden_multiplier: *id189
    transformer_activation: *id190
    embedding_dropout: *id191
    optimizer_fn: *id192
    scheduler_fn: *id193
- dataset: creditcard
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params: *id194
    batch_size: *id195
    learning_rate: *id196
    layers: *id197
    activation: *id198
    initialization: *id199
    dropout: *id200
    optimizer_fn: *id201
    scheduler_fn: *id202
- dataset: creditcard
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id203
    learning_rate: *id204
    batch_size: *id205
    gflu_stages: *id206
    gflu_dropout: *id207
    embedding_dropout: *id208
    optimizer_fn: *id209
    scheduler_fn: *id210
- dataset: creditcard
  model: node
  best_params: {}
  param_grid:
    outer_params: *id211
    learning_rate: *id212
    batch_size: *id213
    num_layers: *id214
    num_trees: *id215
    additional_tree_output_dim: *id216
    depth: *id217
    choice_function: *id218
    bin_function: *id219
    input_dropout: *id220
    embed_categorical: *id221
    optimizer_fn: *id222
    scheduler_fn: *id223
- dataset: creditcard
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id224
    batch_size: *id225
    learning_rate: *id226
    optimizer_fn: *id227
    scheduler_fn: *id228
- dataset: creditcard
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id229
    batch_size: *id230
    learning_rate: *id231
    embedding_bias: *id232
    embedding_initialization: *id233
    shared_embedding_fraction: *id234
    num_attn_blocks: *id235
    attn_dropout: *id236
    add_norm_dropout: *id237
    ff_dropout: *id238
    ff_hidden_multiplier: *id239
    transformer_activation: *id240
    embedding_dropout: *id241
    optimizer_fn: *id242
    scheduler_fn: *id243
- dataset: creditcard
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id244
    iterations: *id245
    learning_rate: *id246
    depth: *id247
    colsample_bylevel: *id248
    l2_leaf_reg: *id249
    min_child_samples: *id250
    bagging_temperature: *id251
