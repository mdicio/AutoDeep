- dataset: titanic
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 2
    n_estimators: &id002
    - 100
    - 1500
    validation_fraction: &id003
    - 0.1
    - 0.25
    max_bin: &id004
    - 256
    - 32
    tree_method: &id005
    - auto
    - hist
    max_depth: &id006
    - 4
    - 12
    learning_rate: &id007
    - 0.1
    - 0.33
    subsample: &id008
    - 0.7
    - 1.0
    colsample_bytree: &id009
    - 0.5
    - 1.0
    min_child_weight: &id010
    - 1
    - 10
    alpha: &id011
    - 0.0
    - 5.0
    gamma: &id012
    - 0.0
    - 5.0
    lambda: &id013
    - 0.0
    - 5.0
- dataset: titanic
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id014
      cv_iterations: 2
      early_stopping: true
      cv_size: 4
    hidden_layer_sizes: &id015
    - - 8
    - - 16
      - 8
    activation: &id016
    - relu
    - tanh
    - logistic
    solver: &id017
    - adam
    - lbfgs
    alpha: &id018
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id019
    - 0.001
    - 0.01
    - 0.1
    beta_1: &id020
    - 0.9
    - 0.99
    - 0.8
    beta_2: &id021
    - 0.999
    - 0.9
    n_iter_no_change: &id022
    - 10
    - 20
    validation_fraction: &id023
    - 0.1
    - 0.15
    - 0.2
    max_iter: &id024
    - 1000
    - 5000
    batch_size: &id025
    - 16
    - 32
    - 64
    - 128
- dataset: titanic
  model: resnet
  best_params: {}
  param_grid:
    outer_params: &id026
      hyperopt_evals: 10
      num_epochs: 1000
      early_stopping: true
    learning_rate: &id027
    - 0.0001
    - 0.001
    batch_size: &id028
    - 16
    - 32
    - 64
    - 128
    validation_fraction: &id029
    - 0.1
    - 0.2
    - 0.3
    early_stopping_patience: &id030
    - 10
    - 20
    scheduler_factor: &id031
    - 0.1
    - 0.9
    scheduler_patience: &id032
    - 10
    - 15
    - 20
- dataset: titanic
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id033
      hyperopt_evals: 2
      num_epochs: 1000
      early_stopping: true
      shuffle: true
    batch_size: &id034
    - 16
    - 32
    - 64
    - 128
    validation_fraction: &id035
    - 0.1
    - 0.3
    early_stopping_patience: &id036
    - 10
    - 20
    learning_rate: &id037
    - 0.0001
    - 0.01
    hidden_size: &id038
    - 1024
    - 2048
    - 4096
    scheduler_factor: &id039
    - 0.1
    - 0.9
    scheduler_patience: &id040
    - 5
    - 15
- dataset: titanic
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id041
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    learning_rate: &id042
    - 0.001
    - 0.0001
    early_stopping_patience: &id043
    - 3
    - 5
    virtual_batch_size_ratio: &id044
    - 0.25
    - 0.5
    batch_size: &id045
    - 16
    - 32
    - 64
    weights: &id046
    - 0
    - 1
    mask_type: &id047
    - sparsemax
    - entmax
    n_d: &id048
    - 8
    - 22
    n_steps: &id049
    - 3
    - 10
    gamma: &id050
    - 1.0
    - 2.0
    cat_emb_dim: &id051
    - 1
    - 3
    n_independent: &id052
    - 1
    - 5
    n_shared: &id053
    - 1
    - 5
    lambda_sparse: &id054
    - 0.001
    - 0.01
    momentum: &id055
    - 0.001
    - 0.4
    clip_value: &id056
    - 1
    - 2
    optimizer_fn: &id057
      Adam:
        weight_decay:
        - 0.0001
        - 0.1
    scheduler_fn: &id058
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 7
        - 10
- dataset: titanic
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id059
      hyperopt_evals: 10
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
    learning_rate: &id060
    - 0.001
    - 0.0001
    early_stopping_patience: &id061
    - 10
    - 20
    batch_size: &id062
    - 16
    - 32
    - 64
    - 128
    tree_depth: &id063
    - 4
    - 7
    num_trees: &id064
    - 4
    - 22
    chain_trees: &id065
    - false
    gflu_stages: &id066
    - 2
    - 8
    optimizer_fn: &id067
      Adam:
        weight_decay:
        - 0.0001
        - 0.1
    scheduler_fn: &id068
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 7
        - 10
- dataset: titanic
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id069
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    learning_rate: &id070
    - 0.0005
    - 0.001
    - 0.002
    early_stopping_patience: &id071
    - 10
    - 20
    batch_size: &id072
    - 16
    - 32
    - 64
    input_embed_dim: &id073
    - 16
    - 32
    - 64
    embedding_initialization: &id074
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id075
    - true
    - false
    share_embedding: &id076
    - true
    - false
    share_embedding_strategy: &id077
    - add
    - fraction
    - add
    shared_embedding_fraction: &id078
    - 0.1
    - 0.25
    - 0.5
    attn_feature_importance: &id079
    - true
    - false
    num_heads: &id080
    - 6
    - 8
    - 10
    num_attn_blocks: &id081
    - 4
    - 6
    - 8
    transformer_head_dim: &id082
    - 64
    - 128
    attn_dropout: &id083
    - 0.05
    - 0.1
    - 0.15
    add_norm_dropout: &id084
    - 0.05
    - 0.1
    - 0.15
    ff_dropout: &id085
    - 0.05
    - 0.1
    - 0.15
    ff_hidden_multiplier: &id086
    - 3
    - 4
    - 5
    transformer_activation: &id087
    - ReLU
    - GEGLU
    - SwiGLU
    embedding_dropout: &id088
    - 0.05
    - 0.1
    - 0.15
    batch_norm_continuous_input: &id089
    - false
    - true
    optimizer_fn: &id090
      Adam:
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id091
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 7
        - 10
- dataset: titanic
  model: categoryembedding
  best_params: {}
  param_grid:
    outer_params:
      max_epochs: 300
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    early_stopping_patience:
    - 3
    - 5
    batch_size:
    - 8
    - 16
    - 32
    - 64
    learning_rate:
    - 0.001
    - 0.0001
    optimizer_fn:
      Adam:
        lr:
        - 0.001
        - 0.0001
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: titanic
  model: gandalf
  best_params: {}
  param_grid:
    outer_params:
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    learning_rate:
    - 0.001
    - 0.0001
    early_stopping_patience:
    - 3
    - 5
    batch_size:
    - 16
    - 32
    - 64
    optimizer_fn:
      Adam:
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn:
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: titanic
  model: node
  best_params: {}
  param_grid:
    outer_params: &id092
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    num_layers: &id093
    - 1
    - 2
    num_trees: &id094
    - 1024
    - 2048
    additional_tree_output_dim: &id095
    - 2
    - 3
    - 4
    depth: &id096
    - 4
    - 7
    choice_function: &id097
    - entmax15
    - sparsemax
    bin_function: &id098
    - entmoid15
    - sparsemoid
    max_features: &id099
    - null
    - 50
    - 100
    input_dropout: &id100
    - 0.0
    - 0.2
    - 0.4
    initialize_response: &id101
    - normal
    - uniform
    initialize_selection_logits: &id102
    - uniform
    - normal
    threshold_init_beta: &id103
    - 0.1
    - 1.0
    - 10.0
    threshold_init_cutoff: &id104
    - 0.5
    - 0.9
    - 1.5
    cat_embedding_dropout: &id105
    - 0.0
    - 0.1
    - 0.2
    embed_categorical: &id106
    - true
    - false
    head: &id107
    - LinearHead
    embedding_dropout: &id108
    - 0.0
    - 0.1
    - 0.2
    batch_norm_continuous_input: &id109
    - true
    - false
    learning_rate: &id110
    - 0.001
    - 0.0001
    early_stopping_patience: &id111
    - 5
    - 10
    batch_size: &id112
    - 8
    - 16
    - 32
    - 64
    - 128
    optimizer_fn: &id113
      Adam:
        weight_decay:
        - 0.0
        - 0.3
    scheduler_fn: &id114
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: titanic
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id115
      max_epochs: 300
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    early_stopping_patience: &id116
    - 3
    - 5
    batch_size: &id117
    - 8
    - 16
    - 32
    - 64
    optimizer_fn: &id118
      Adam:
        lr:
        - 0.001
        - 0.0001
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id119
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
    attn_embed_dim: &id120
    - 32
    - 16
    - 64
    num_heads: &id121
    - 2
    - 1
    - 8
    num_attn_blocks: &id122
    - 3
    - 1
    - 5
    attn_dropouts: &id123
    - 0.01
    - 0.3
    has_residuals: &id124
    - true
    - true
    - false
    embedding_dim: &id125
    - 16
    - 8
    - 32
    embedding_initialization: &id126
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id127
    - true
    - false
    share_embedding: &id128
    - false
    - true
    share_embedding_strategy: &id129
    - fraction
    - add
    shared_embedding_fraction: &id130
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id131
    - false
    - true
    layers: &id132
    - 128-64-32
    - 64-32
    - 256-128-64-32
    activation: &id133
    - ReLU
    - Tanh
    - LeakyReLU
    use_batch_norm: &id134
    - false
    - true
    initialization: &id135
    - kaiming
    - xavier
    - random
    dropout: &id136
    - 0.01
    - 0.2
    - 0.3
    attention_pooling: &id137
    - false
    - true
    head: &id138
    - LinearHead
    - MixtureDensityHead
    embedding_dropout: &id139
    - 0.1
    - 0.01
    - 0.5
    batch_norm_continuous_input: &id140
    - true
    - false
    learning_rate: &id141
    - 0.001
    - 0.0001
    - 0.01
- dataset: titanic
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id142
      max_epochs: 300
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    early_stopping_patience: &id143
    - 3
    - 5
    batch_size: &id144
    - 8
    - 16
    - 32
    - 64
    learning_rate: &id145
    - 0.001
    - 0.0001
    optimizer_fn: &id146
      Adam:
        lr:
        - 0.001
        - 0.0001
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id147
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: iris
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id001
    n_estimators: *id002
    validation_fraction: *id003
    max_bin: *id004
    tree_method: *id005
    max_depth: *id006
    learning_rate: *id007
    subsample: *id008
    colsample_bytree: *id009
    min_child_weight: *id010
    alpha: *id011
    gamma: *id012
    lambda: *id013
- dataset: iris
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id014
    hidden_layer_sizes: *id015
    activation: *id016
    solver: *id017
    alpha: *id018
    learning_rate_init: *id019
    beta_1: *id020
    beta_2: *id021
    n_iter_no_change: *id022
    validation_fraction: *id023
    max_iter: *id024
    batch_size: *id025
- dataset: iris
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id026
    learning_rate: *id027
    batch_size: *id028
    validation_fraction: *id029
    early_stopping_patience: *id030
    scheduler_factor: *id031
    scheduler_patience: *id032
- dataset: iris
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id033
    batch_size: *id034
    validation_fraction: *id035
    early_stopping_patience: *id036
    learning_rate: *id037
    hidden_size: *id038
    scheduler_factor: *id039
    scheduler_patience: *id040
- dataset: iris
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id041
    learning_rate: *id042
    early_stopping_patience: *id043
    virtual_batch_size_ratio: *id044
    batch_size: *id045
    weights: *id046
    mask_type: *id047
    n_d: *id048
    n_steps: *id049
    gamma: *id050
    cat_emb_dim: *id051
    n_independent: *id052
    n_shared: *id053
    lambda_sparse: *id054
    momentum: *id055
    clip_value: *id056
    optimizer_fn: *id057
    scheduler_fn: *id058
- dataset: iris
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id059
    learning_rate: *id060
    early_stopping_patience: *id061
    batch_size: *id062
    tree_depth: *id063
    num_trees: *id064
    chain_trees: *id065
    gflu_stages: *id066
    optimizer_fn: *id067
    scheduler_fn: *id068
- dataset: iris
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id069
    learning_rate: *id070
    early_stopping_patience: *id071
    batch_size: *id072
    input_embed_dim: *id073
    embedding_initialization: *id074
    embedding_bias: *id075
    share_embedding: *id076
    share_embedding_strategy: *id077
    shared_embedding_fraction: *id078
    attn_feature_importance: *id079
    num_heads: *id080
    num_attn_blocks: *id081
    transformer_head_dim: *id082
    attn_dropout: *id083
    add_norm_dropout: *id084
    ff_dropout: *id085
    ff_hidden_multiplier: *id086
    transformer_activation: *id087
    embedding_dropout: *id088
    batch_norm_continuous_input: *id089
    optimizer_fn: *id090
    scheduler_fn: *id091
- dataset: iris
  model: node
  best_params: {}
  param_grid:
    outer_params: *id092
    num_layers: *id093
    num_trees: *id094
    additional_tree_output_dim: *id095
    depth: *id096
    choice_function: *id097
    bin_function: *id098
    max_features: *id099
    input_dropout: *id100
    initialize_response: *id101
    initialize_selection_logits: *id102
    threshold_init_beta: *id103
    threshold_init_cutoff: *id104
    cat_embedding_dropout: *id105
    embed_categorical: *id106
    head: *id107
    embedding_dropout: *id108
    batch_norm_continuous_input: *id109
    learning_rate: *id110
    early_stopping_patience: *id111
    batch_size: *id112
    optimizer_fn: *id113
    scheduler_fn: *id114
- dataset: iris
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id115
    early_stopping_patience: *id116
    batch_size: *id117
    optimizer_fn: *id118
    scheduler_fn: *id119
    attn_embed_dim: *id120
    num_heads: *id121
    num_attn_blocks: *id122
    attn_dropouts: *id123
    has_residuals: *id124
    embedding_dim: *id125
    embedding_initialization: *id126
    embedding_bias: *id127
    share_embedding: *id128
    share_embedding_strategy: *id129
    shared_embedding_fraction: *id130
    deep_layers: *id131
    layers: *id132
    activation: *id133
    use_batch_norm: *id134
    initialization: *id135
    dropout: *id136
    attention_pooling: *id137
    head: *id138
    embedding_dropout: *id139
    batch_norm_continuous_input: *id140
    learning_rate: *id141
- dataset: iris
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id142
    early_stopping_patience: *id143
    batch_size: *id144
    learning_rate: *id145
    optimizer_fn: *id146
    scheduler_fn: *id147
- dataset: housing
  model: xgb
  best_params: {}
  param_grid:
    outer_params: &id148
      hyperopt_evals: 100
    n_estimators: &id149
    - 100
    - 1100
    validation_fraction: &id150
    - 0.2
    max_bin: &id151
    - 128
    - 512
    tree_method: &id152
    - auto
    max_depth: &id153
    - 4
    - 10
    learning_rate: &id154
    - 0.1
    - 0.33
    subsample: &id155
    - 0.7
    - 1.0
    colsample_bytree: &id156
    - 0.7
    - 1.0
    min_child_weight: &id157
    - 1
    - 50
    alpha: &id158
    - 0.0
    - 5.0
    gamma: &id159
    - 0.0
    - 5.0
    lambda: &id160
    - 0.0
    - 5.0
- dataset: housing
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id161
      cv_iterations: 10
      early_stopping: true
      cv_size: 5
    hidden_layer_sizes: &id162
    - - 16
      - 32
    - - 16
      - 8
    - - 32
      - 16
    - - 45
      - 20
    activation: &id163
    - relu
    - tanh
    - logistic
    solver: &id164
    - adam
    - lbfgs
    alpha: &id165
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id166
    - 0.001
    - 0.01
    - 0.1
    beta_1: &id167
    - 0.9
    - 0.99
    - 0.8
    beta_2: &id168
    - 0.999
    - 0.9
    n_iter_no_change: &id169
    - 3
    - 5
    validation_fraction: &id170
    - 0.1
    - 0.15
    - 0.2
    max_iter: &id171
    - 1000
    - 2000
    batch_size: &id172
    - 256
    - 512
    - 1024
    - 2048
- dataset: housing
  model: resnet
  best_params: {}
  param_grid:
    outer_params: &id173
      hyperopt_evals: 10
      num_epochs: 1000
      early_stopping: true
    learning_rate: &id174
    - 0.0001
    - 0.001
    batch_size: &id175
    - 256
    - 512
    - 1024
    - 2048
    validation_fraction: &id176
    - 0.1
    - 0.3
    early_stopping_patience: &id177
    - 3
    - 5
    scheduler_factor: &id178
    - 0.1
    - 0.9
    scheduler_patience: &id179
    - 3
    - 5
- dataset: housing
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id180
      hyperopt_evals: 10
      num_epochs: 1000
      early_stopping: true
      shuffle: true
    batch_size: &id181
    - 256
    - 512
    - 1024
    - 2048
    validation_fraction: &id182
    - 0.1
    - 0.3
    early_stopping_patience: &id183
    - 3
    - 5
    learning_rate: &id184
    - 0.01
    - 0.0001
    hidden_size: &id185
    - 1024
    - 2048
    - 4096
    scheduler_factor: &id186
    - 0.1
    - 0.9
    scheduler_patience: &id187
    - 4
    - 7
- dataset: housing
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id188
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    learning_rate: &id189
    - 0.001
    - 0.0001
    early_stopping_patience: &id190
    - 3
    - 5
    virtual_batch_size_ratio: &id191
    - 0.125
    - 0.25
    - 0.5
    batch_size: &id192
    - 256
    - 512
    - 1024
    weights: &id193
    - 0
    - 1
    mask_type: &id194
    - sparsemax
    - entmax
    n_d: &id195
    - 8
    - 16
    n_steps: &id196
    - 3
    - 10
    gamma: &id197
    - 1.0
    - 2.0
    cat_emb_dim: &id198
    - 1
    - 3
    n_independent: &id199
    - 1
    - 5
    n_shared: &id200
    - 1
    - 5
    lambda_sparse: &id201
    - 0.001
    - 0.01
    momentum: &id202
    - 0.001
    - 0.4
    clip_value: &id203
    - 1
    - 2
    optimizer_fn: &id204
      Adam:
        weight_decay:
        - 0.0001
        - 0.1
    scheduler_fn: &id205
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
- dataset: housing
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id206
      hyperopt_evals: 10
      auto_lr_find: false
      max_epochs: 1000
      val_size: 0.2
    learning_rate: &id207
    - 0.001
    - 0.0001
    early_stopping_patience: &id208
    - 3
    - 5
    batch_size: &id209
    - 256
    - 512
    - 1024
    tree_depth: &id210
    - 4
    - 7
    num_trees: &id211
    - 8
    - 22
    chain_trees: &id212
    - false
    gflu_stages: &id213
    - 2
    - 8
    optimizer_fn: &id214
      Adam:
        weight_decay:
        - 0.0001
        - 0.1
    scheduler_fn: &id215
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
- dataset: housing
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id216
      max_epochs: 1000
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    learning_rate: &id217
    - 0.0005
    - 0.001
    - 0.002
    early_stopping_patience: &id218
    - 3
    - 5
    batch_size: &id219
    - 256
    - 512
    - 1024
    - 2048
    input_embed_dim: &id220
    - 16
    - 32
    - 64
    embedding_initialization: &id221
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id222
    - true
    - false
    share_embedding: &id223
    - true
    - false
    share_embedding_strategy: &id224
    - add
    - fraction
    - add
    shared_embedding_fraction: &id225
    - 0.1
    - 0.25
    - 0.5
    attn_feature_importance: &id226
    - true
    - false
    num_heads: &id227
    - 6
    - 8
    - 10
    num_attn_blocks: &id228
    - 4
    - 6
    - 8
    transformer_head_dim: &id229
    - 64
    - 128
    attn_dropout: &id230
    - 0.05
    - 0.1
    - 0.15
    add_norm_dropout: &id231
    - 0.05
    - 0.1
    - 0.15
    ff_dropout: &id232
    - 0.05
    - 0.1
    - 0.15
    ff_hidden_multiplier: &id233
    - 3
    - 4
    - 5
    transformer_activation: &id234
    - ReLU
    - GEGLU
    - SwiGLU
    embedding_dropout: &id235
    - 0.05
    - 0.1
    - 0.15
    batch_norm_continuous_input: &id236
    - false
    - true
    optimizer_fn: &id237
      Adam:
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id238
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 7
- dataset: housing
  model: node
  best_params: {}
  param_grid:
    outer_params: &id239
      max_epochs: 100
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    num_layers: &id240
    - 1
    - 2
    num_trees: &id241
    - 1024
    - 2048
    additional_tree_output_dim: &id242
    - 2
    - 3
    - 4
    depth: &id243
    - 4
    - 5
    choice_function: &id244
    - entmax15
    - sparsemax
    bin_function: &id245
    - entmoid15
    - sparsemoid
    max_features: &id246
    - null
    - 50
    - 100
    input_dropout: &id247
    - 0.0
    - 0.2
    - 0.4
    initialize_response: &id248
    - normal
    - uniform
    initialize_selection_logits: &id249
    - uniform
    - normal
    threshold_init_beta: &id250
    - 0.1
    - 1.0
    - 10.0
    threshold_init_cutoff: &id251
    - 0.5
    - 0.9
    - 1.5
    cat_embedding_dropout: &id252
    - 0.0
    - 0.1
    - 0.2
    embed_categorical: &id253
    - true
    - false
    head: &id254
    - LinearHead
    embedding_dropout: &id255
    - 0.0
    - 0.1
    - 0.2
    batch_norm_continuous_input: &id256
    - true
    - false
    learning_rate: &id257
    - 0.001
    - 0.0001
    early_stopping_patience: &id258
    - 3
    - 5
    batch_size: &id259
    - 512
    - 1024
    - 2048
    optimizer_fn: &id260
      Adam:
        weight_decay:
        - 0.0
        - 0.3
    scheduler_fn: &id261
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: housing
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id262
      max_epochs: 300
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    early_stopping_patience: &id263
    - 3
    - 5
    batch_size: &id264
    - 128
    - 256
    - 512
    - 1024
    optimizer_fn: &id265
      Adam:
        lr:
        - 0.001
        - 0.0001
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id266
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
    attn_embed_dim: &id267
    - 32
    - 16
    - 64
    num_heads: &id268
    - 2
    - 1
    - 8
    num_attn_blocks: &id269
    - 3
    - 1
    - 5
    attn_dropouts: &id270
    - 0.01
    - 0.3
    has_residuals: &id271
    - true
    - true
    - false
    embedding_dim: &id272
    - 16
    - 8
    - 32
    embedding_initialization: &id273
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id274
    - true
    - false
    share_embedding: &id275
    - false
    - true
    share_embedding_strategy: &id276
    - fraction
    - add
    shared_embedding_fraction: &id277
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id278
    - false
    - true
    layers: &id279
    - 128-64-32
    - 64-32
    - 256-128-64-32
    activation: &id280
    - ReLU
    - Tanh
    - LeakyReLU
    use_batch_norm: &id281
    - false
    - true
    initialization: &id282
    - kaiming
    - xavier
    - random
    dropout: &id283
    - 0.01
    - 0.2
    - 0.3
    attention_pooling: &id284
    - false
    - true
    head: &id285
    - LinearHead
    - MixtureDensityHead
    embedding_dropout: &id286
    - 0.1
    - 0.01
    - 0.5
    batch_norm_continuous_input: &id287
    - true
    - false
    learning_rate: &id288
    - 0.001
    - 0.0001
    - 0.01
- dataset: housing
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id289
      max_epochs: 300
      early_stopping_tolerance: 1.0e-06
      hyperopt_evals: 10
      val_size: 0.2
      auto_lr_find: false
    early_stopping_patience: &id290
    - 3
    - 5
    batch_size: &id291
    - 128
    - 256
    - 512
    - 1024
    learning_rate: &id292
    - 0.001
    - 0.0001
    optimizer_fn: &id293
      Adam:
        lr:
        - 0.001
        - 0.0001
        weight_decay:
        - 0.0001
        - 0.3
    scheduler_fn: &id294
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 4
        - 9
- dataset: adult
  model: xgb
  best_params: {}
  param_grid:
    outer_params: *id148
    n_estimators: *id149
    validation_fraction: *id150
    max_bin: *id151
    tree_method: *id152
    max_depth: *id153
    learning_rate: *id154
    subsample: *id155
    colsample_bytree: *id156
    min_child_weight: *id157
    alpha: *id158
    gamma: *id159
    lambda: *id160
- dataset: adult
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id161
    hidden_layer_sizes: *id162
    activation: *id163
    solver: *id164
    alpha: *id165
    learning_rate_init: *id166
    beta_1: *id167
    beta_2: *id168
    n_iter_no_change: *id169
    validation_fraction: *id170
    max_iter: *id171
    batch_size: *id172
- dataset: adult
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id173
    learning_rate: *id174
    batch_size: *id175
    validation_fraction: *id176
    early_stopping_patience: *id177
    scheduler_factor: *id178
    scheduler_patience: *id179
- dataset: adult
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id180
    batch_size: *id181
    validation_fraction: *id182
    early_stopping_patience: *id183
    learning_rate: *id184
    hidden_size: *id185
    scheduler_factor: *id186
    scheduler_patience: *id187
- dataset: adult
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id188
    learning_rate: *id189
    early_stopping_patience: *id190
    virtual_batch_size_ratio: *id191
    batch_size: *id192
    weights: *id193
    mask_type: *id194
    n_d: *id195
    n_steps: *id196
    gamma: *id197
    cat_emb_dim: *id198
    n_independent: *id199
    n_shared: *id200
    lambda_sparse: *id201
    momentum: *id202
    clip_value: *id203
    optimizer_fn: *id204
    scheduler_fn: *id205
- dataset: adult
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id206
    learning_rate: *id207
    early_stopping_patience: *id208
    batch_size: *id209
    tree_depth: *id210
    num_trees: *id211
    chain_trees: *id212
    gflu_stages: *id213
    optimizer_fn: *id214
    scheduler_fn: *id215
- dataset: adult
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id216
    learning_rate: *id217
    early_stopping_patience: *id218
    batch_size: *id219
    input_embed_dim: *id220
    embedding_initialization: *id221
    embedding_bias: *id222
    share_embedding: *id223
    share_embedding_strategy: *id224
    shared_embedding_fraction: *id225
    attn_feature_importance: *id226
    num_heads: *id227
    num_attn_blocks: *id228
    transformer_head_dim: *id229
    attn_dropout: *id230
    add_norm_dropout: *id231
    ff_dropout: *id232
    ff_hidden_multiplier: *id233
    transformer_activation: *id234
    embedding_dropout: *id235
    batch_norm_continuous_input: *id236
    optimizer_fn: *id237
    scheduler_fn: *id238
- dataset: adult
  model: node
  best_params: {}
  param_grid:
    outer_params: *id239
    num_layers: *id240
    num_trees: *id241
    additional_tree_output_dim: *id242
    depth: *id243
    choice_function: *id244
    bin_function: *id245
    max_features: *id246
    input_dropout: *id247
    initialize_response: *id248
    initialize_selection_logits: *id249
    threshold_init_beta: *id250
    threshold_init_cutoff: *id251
    cat_embedding_dropout: *id252
    embed_categorical: *id253
    head: *id254
    embedding_dropout: *id255
    batch_norm_continuous_input: *id256
    learning_rate: *id257
    early_stopping_patience: *id258
    batch_size: *id259
    optimizer_fn: *id260
    scheduler_fn: *id261
- dataset: adult
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id262
    early_stopping_patience: *id263
    batch_size: *id264
    optimizer_fn: *id265
    scheduler_fn: *id266
    attn_embed_dim: *id267
    num_heads: *id268
    num_attn_blocks: *id269
    attn_dropouts: *id270
    has_residuals: *id271
    embedding_dim: *id272
    embedding_initialization: *id273
    embedding_bias: *id274
    share_embedding: *id275
    share_embedding_strategy: *id276
    shared_embedding_fraction: *id277
    deep_layers: *id278
    layers: *id279
    activation: *id280
    use_batch_norm: *id281
    initialization: *id282
    dropout: *id283
    attention_pooling: *id284
    head: *id285
    embedding_dropout: *id286
    batch_norm_continuous_input: *id287
    learning_rate: *id288
- dataset: adult
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id289
    early_stopping_patience: *id290
    batch_size: *id291
    learning_rate: *id292
    optimizer_fn: *id293
    scheduler_fn: *id294
