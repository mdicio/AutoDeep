- dataset: iris
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 10
    batch_size: &id002
    - 32
    - 64
    - 128
    - 256
    attn_embed_dim_multiplier: &id003
    - 2
    - 16
    num_heads: &id004
    - 2
    - 8
    num_attn_blocks: &id005
    - 2
    - 6
    attn_dropouts: &id006
    - 0.0
    - 0.3
    embedding_dim: &id007
    - 8
    - 32
    embedding_initialization: &id008
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id009
    - true
    - false
    share_embedding: &id010
    - true
    - false
    share_embedding_strategy: &id011
    - add
    - fraction
    shared_embedding_fraction: &id012
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id013
    - true
    - false
    layers: &id014
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout: &id015
    - 0.0
    - 0.3
    activation: &id016
    - ReLU
    - LeakyReLU
    initialization: &id017
    - kaiming
    - xavier
    attention_pooling: &id018
    - true
    - false
    optimizer_fn: &id019
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id020
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: creditcard
  model: autoint
  best_params: {}
  param_grid:
    outer_params: &id021
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id022
    - 1024
    - 2048
    - 4096
    attn_embed_dim_multiplier: &id023
    - 2
    - 16
    num_heads: &id024
    - 2
    - 8
    num_attn_blocks: &id025
    - 2
    - 6
    attn_dropouts: &id026
    - 0.0
    - 0.3
    embedding_dim: &id027
    - 8
    - 32
    embedding_initialization: &id028
    - kaiming_uniform
    - kaiming_normal
    embedding_bias: &id029
    - true
    - false
    share_embedding: &id030
    - true
    - false
    share_embedding_strategy: &id031
    - add
    - fraction
    shared_embedding_fraction: &id032
    - 0.25
    - 0.1
    - 0.5
    deep_layers: &id033
    - true
    - false
    layers: &id034
    - 128-64-32
    - 128-64-32-16
    - 256-128-64
    dropout: &id035
    - 0.0
    - 0.3
    activation: &id036
    - ReLU
    - LeakyReLU
    initialization: &id037
    - kaiming
    - xavier
    attention_pooling: &id038
    - true
    - false
    optimizer_fn: &id039
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id040
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: ageconditions
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id001
    batch_size: *id002
    attn_embed_dim_multiplier: *id003
    num_heads: *id004
    num_attn_blocks: *id005
    attn_dropouts: *id006
    embedding_dim: *id007
    embedding_initialization: *id008
    embedding_bias: *id009
    share_embedding: *id010
    share_embedding_strategy: *id011
    shared_embedding_fraction: *id012
    deep_layers: *id013
    layers: *id014
    dropout: *id015
    activation: *id016
    initialization: *id017
    attention_pooling: *id018
    optimizer_fn: *id019
    scheduler_fn: *id020
- dataset: adult
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id021
    batch_size: *id022
    attn_embed_dim_multiplier: *id023
    num_heads: *id024
    num_attn_blocks: *id025
    attn_dropouts: *id026
    embedding_dim: *id027
    embedding_initialization: *id028
    embedding_bias: *id029
    share_embedding: *id030
    share_embedding_strategy: *id031
    shared_embedding_fraction: *id032
    deep_layers: *id033
    layers: *id034
    dropout: *id035
    activation: *id036
    initialization: *id037
    attention_pooling: *id038
    optimizer_fn: *id039
    scheduler_fn: *id040
- dataset: housing
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id021
    batch_size: *id022
    attn_embed_dim_multiplier: *id023
    num_heads: *id024
    num_attn_blocks: *id025
    attn_dropouts: *id026
    embedding_dim: *id027
    embedding_initialization: *id028
    embedding_bias: *id029
    share_embedding: *id030
    share_embedding_strategy: *id031
    shared_embedding_fraction: *id032
    deep_layers: *id033
    layers: *id034
    dropout: *id035
    activation: *id036
    initialization: *id037
    attention_pooling: *id038
    optimizer_fn: *id039
    scheduler_fn: *id040
- dataset: heloc
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id021
    batch_size: *id022
    attn_embed_dim_multiplier: *id023
    num_heads: *id024
    num_attn_blocks: *id025
    attn_dropouts: *id026
    embedding_dim: *id027
    embedding_initialization: *id028
    embedding_bias: *id029
    share_embedding: *id030
    share_embedding_strategy: *id031
    shared_embedding_fraction: *id032
    deep_layers: *id033
    layers: *id034
    dropout: *id035
    activation: *id036
    initialization: *id037
    attention_pooling: *id038
    optimizer_fn: *id039
    scheduler_fn: *id040
- dataset: covertype
  model: autoint
  best_params: {}
  param_grid:
    outer_params: *id021
    batch_size: *id022
    attn_embed_dim_multiplier: *id023
    num_heads: *id024
    num_attn_blocks: *id025
    attn_dropouts: *id026
    embedding_dim: *id027
    embedding_initialization: *id028
    embedding_bias: *id029
    share_embedding: *id030
    share_embedding_strategy: *id031
    shared_embedding_fraction: *id032
    deep_layers: *id033
    layers: *id034
    dropout: *id035
    activation: *id036
    initialization: *id037
    attention_pooling: *id038
    optimizer_fn: *id039
    scheduler_fn: *id040
