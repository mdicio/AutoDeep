- dataset: heloc
  model: mlp
  best_params: {}
  param_grid:
    outer_params: &id001
      cv_iterations: 10
      early_stopping: true
      cv_size: 5
      validation_fraction: 0.15
      hyperopt_evals: 50
      n_iter_no_change: 30
      max_iter: 1000
    hidden_layer_sizes: &id002
    - - 64
      - 32
      - 16
    - - 256
      - 128
      - 64
      - 32
    - - 128
      - 64
      - 32
      - 16
    activation: &id003
    - relu
    - tanh
    - logistic
    solver: &id004
    - adam
    - lbfgs
    alpha: &id005
    - 0.0001
    - 0.001
    - 0.01
    learning_rate_init: &id006
    - 0.0001
    - 0.01
    - 0.1
    beta_1: &id007
    - 0.99
    - 0.8
    beta_2: &id008
    - 0.999
    - 0.9
    batch_size: &id009
    - 512
    - 1024
    - 2048
- dataset: heloc
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: &id010
      hyperopt_evals: 50
      max_epochs: 1000
      early_stopping: true
      shuffle: true
      validation_fraction: 0.15
      early_stopping_patience: 6
      tol: 1.0e-05
    batch_size: &id011
    - 1024
    - 4096
    hidden_size: &id012
    - 2048
    - 4096
    optimizer_fn: &id013
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.01
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.01
    scheduler_fn: &id014
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: &id015
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      tol: 1.0e-05
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    virtual_batch_size_ratio: &id016
    - 0.125
    - 0.25
    - 0.5
    - 1.0
    batch_size: &id017
    - 1024
    - 2048
    - 4096
    weights: &id018
    - 0
    - 1
    mask_type: &id019
    - sparsemax
    - entmax
    n_d: &id020
    - 6
    - 32
    n_steps: &id021
    - 1
    - 6
    gamma: &id022
    - 1.0
    - 2.0
    n_independent: &id023
    - 1
    - 3
    n_shared: &id024
    - 1
    - 3
    optimizer_fn: &id025
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id026
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: gate
  best_params: {}
  param_grid:
    outer_params: &id027
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      tol: 1.0e-05
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id028
    - 1024
    - 2048
    - 4096
    tree_depth: &id029
    - 4
    - 6
    num_trees: &id030
    - 4
    - 8
    chain_trees: &id031
    - false
    - true
    gflu_stages: &id032
    - 3
    - 8
    gflu_dropout: &id033
    - 0.0
    - 0.05
    tree_dropout: &id034
    - 0.0
    - 0.05
    tree_wise_attention_dropout: &id035
    - 0.0
    - 0.05
    embedding_dropout: &id036
    - 0
    - 0.2
    optimizer_fn: &id037
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id038
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: &id039
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      tol: 1.0e-05
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
      attn_feature_importance: false
    batch_size: &id040
    - 1024
    - 2048
    - 4096
    num_heads: &id041
    - 4
    - 10
    input_embed_dim_multiplier: &id042
    - 2
    - 6
    embedding_initialization: &id043
    - kaiming_uniform
    - kaiming_normal
    embedding_dropout: &id044
    - 0.05
    - 0.2
    shared_embedding_fraction: &id045
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id046
    - 4
    - 8
    attn_dropout: &id047
    - 0.05
    - 0.2
    add_norm_dropout: &id048
    - 0.05
    - 0.2
    ff_dropout: &id049
    - 0.05
    - 0.2
    ff_hidden_multiplier: &id050
    - 4
    - 32
    transformer_activation: &id051
    - GEGLU
    - ReGLU
    - SwiGLU
    optimizer_fn: &id052
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id053
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id054
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      tol: 1.0e-05
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id055
    - 1024
    - 2048
    - 4096
    gflu_stages: &id056
    - 4
    - 10
    gflu_dropout: &id057
    - 0.0
    - 0.3
    embedding_dropout: &id058
    - 0.0
    - 0.3
    optimizer_fn: &id059
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id060
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id061
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      tol: 1.0e-05
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id062
    - 1024
    - 2048
    - 4096
    embedding_bias: &id063
    - true
    - false
    embedding_initialization: &id064
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id065
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id066
    - 4
    - 10
    attn_dropout: &id067
    - 0.05
    - 0.3
    add_norm_dropout: &id068
    - 0.05
    - 0.3
    ff_dropout: &id069
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id070
    - 2
    - 6
    transformer_activation: &id071
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id072
    - 0.05
    - 0.3
    optimizer_fn: &id073
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id074
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: heloc
  model: catboost
  best_params: {}
  param_grid:
    outer_params: &id075
      hyperopt_evals: 50
      validation_fraction: 0.15
      early_stopping_rounds: 50
      verbose: false
    iterations: &id076
    - 200
    - 3000
    learning_rate: &id077
    - 0.001
    - 0.1
    depth: &id078
    - 4
    - 10
    l2_leaf_reg: &id079
    - 0.5
    - 5.0
    min_child_samples: &id080
    - 1
    - 100
    bagging_temperature: &id081
    - 0.1
    - 2.0
- dataset: diabetes
  model: mlp
  best_params: {}
  param_grid:
    outer_params: *id001
    hidden_layer_sizes: *id002
    activation: *id003
    solver: *id004
    alpha: *id005
    learning_rate_init: *id006
    beta_1: *id007
    beta_2: *id008
    batch_size: *id009
- dataset: diabetes
  model: s1dcnn
  best_params: {}
  param_grid:
    outer_params: *id010
    batch_size: *id011
    hidden_size: *id012
    optimizer_fn: *id013
    scheduler_fn: *id014
- dataset: diabetes
  model: tabnet
  best_params: {}
  param_grid:
    outer_params: *id015
    virtual_batch_size_ratio: *id016
    batch_size: *id017
    weights: *id018
    mask_type: *id019
    n_d: *id020
    n_steps: *id021
    gamma: *id022
    n_independent: *id023
    n_shared: *id024
    optimizer_fn: *id025
    scheduler_fn: *id026
- dataset: diabetes
  model: gate
  best_params: {}
  param_grid:
    outer_params: *id027
    batch_size: *id028
    tree_depth: *id029
    num_trees: *id030
    chain_trees: *id031
    gflu_stages: *id032
    gflu_dropout: *id033
    tree_dropout: *id034
    tree_wise_attention_dropout: *id035
    embedding_dropout: *id036
    optimizer_fn: *id037
    scheduler_fn: *id038
- dataset: diabetes
  model: fttransformer
  best_params: {}
  param_grid:
    outer_params: *id039
    batch_size: *id040
    num_heads: *id041
    input_embed_dim_multiplier: *id042
    embedding_initialization: *id043
    embedding_dropout: *id044
    shared_embedding_fraction: *id045
    num_attn_blocks: *id046
    attn_dropout: *id047
    add_norm_dropout: *id048
    ff_dropout: *id049
    ff_hidden_multiplier: *id050
    transformer_activation: *id051
    optimizer_fn: *id052
    scheduler_fn: *id053
- dataset: diabetes
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id054
    batch_size: *id055
    gflu_stages: *id056
    gflu_dropout: *id057
    embedding_dropout: *id058
    optimizer_fn: *id059
    scheduler_fn: *id060
- dataset: diabetes
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id061
    batch_size: *id062
    embedding_bias: *id063
    embedding_initialization: *id064
    shared_embedding_fraction: *id065
    num_attn_blocks: *id066
    attn_dropout: *id067
    add_norm_dropout: *id068
    ff_dropout: *id069
    ff_hidden_multiplier: *id070
    transformer_activation: *id071
    embedding_dropout: *id072
    optimizer_fn: *id073
    scheduler_fn: *id074
- dataset: diabetes
  model: catboost
  best_params: {}
  param_grid:
    outer_params: *id075
    iterations: *id076
    learning_rate: *id077
    depth: *id078
    l2_leaf_reg: *id079
    min_child_samples: *id080
    bagging_temperature: *id081
- dataset: heloc
  model: resnet
  best_params: {}
  param_grid:
    outer_params: &id082
      hyperopt_evals: 50
      max_epochs: 1000
      early_stopping: true
      early_stopping_patience: 6
      tol: 1.0e-05
      validation_fraction: 0.15
    batch_size: &id083
    - 1024
    - 2048
    - 4096
    resnet_depth: &id084
    - resnet18
    - resnet34
    - resnet50
    optimizer_fn: &id085
      Adam:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
      AdamW:
        weight_decay:
        - 1.0e-05
        - 0.001
        learning_rate:
        - 0.0001
        - 0.001
    scheduler_fn: &id086
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: diabetes
  model: resnet
  best_params: {}
  param_grid:
    outer_params: *id082
    batch_size: *id083
    resnet_depth: *id084
    optimizer_fn: *id085
    scheduler_fn: *id086
