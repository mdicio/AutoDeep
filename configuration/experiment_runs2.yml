- dataset: creditcard
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: &id001
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id002
    - 1024
    - 2048
    - 4096
    gflu_stages: &id003
    - 4
    - 10
    gflu_dropout: &id004
    - 0.0
    - 0.3
    embedding_dropout: &id005
    - 0.0
    - 0.3
    optimizer_fn: &id006
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id007
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: gandalf
  best_params: {}
  param_grid:
    outer_params: *id001
    batch_size: *id002
    gflu_stages: *id003
    gflu_dropout: *id004
    embedding_dropout: *id005
    optimizer_fn: *id006
    scheduler_fn: *id007
- dataset: creditcard
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: &id008
      hyperopt_evals: 50
      auto_lr_find: true
      precision: 16
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 8
    batch_size: &id009
    - 1024
    - 2048
    - 4096
    embedding_bias: &id010
    - true
    - false
    embedding_initialization: &id011
    - kaiming_uniform
    - kaiming_normal
    shared_embedding_fraction: &id012
    - 0.125
    - 0.25
    - 0.5
    num_attn_blocks: &id013
    - 4
    - 10
    attn_dropout: &id014
    - 0.05
    - 0.3
    add_norm_dropout: &id015
    - 0.05
    - 0.3
    ff_dropout: &id016
    - 0.05
    - 0.3
    ff_hidden_multiplier: &id017
    - 2
    - 6
    transformer_activation: &id018
    - GEGLU
    - ReGLU
    - SwiGLU
    embedding_dropout: &id019
    - 0.05
    - 0.3
    optimizer_fn: &id020
      Adam:
        weight_decay:
        - 0.0001
        - 1.0e-05
      AdamW:
        weight_decay:
        - 0.0001
        - 1.0e-05
    scheduler_fn: &id021
      ReduceLROnPlateau:
        factor:
        - 0.1
        - 0.9
        patience:
        - 3
        - 5
      ExponentialLR:
        gamma:
        - 0.9
        - 0.99
- dataset: covertype
  model: tabtransformer
  best_params: {}
  param_grid:
    outer_params: *id008
    batch_size: *id009
    embedding_bias: *id010
    embedding_initialization: *id011
    shared_embedding_fraction: *id012
    num_attn_blocks: *id013
    attn_dropout: *id014
    add_norm_dropout: *id015
    ff_dropout: *id016
    ff_hidden_multiplier: *id017
    transformer_activation: *id018
    embedding_dropout: *id019
    optimizer_fn: *id020
    scheduler_fn: *id021
