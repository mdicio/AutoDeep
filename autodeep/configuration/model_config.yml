random_state: 4200
retrain: True

include_models:
  - XGB  # Extreme Gradient Boosting
  - CatBoost
  - MLP  # Multi-Layer Perceptron
  - TabNet  # TabNet Classifier
  - GATE
  - resnet
  - S1DCNN
  - CategoryEmbedding
  - FTTransformer
  - TabTransformer
  - GANDALF
  - AutoInt
  - Node

model_configs:
  xgb:

    data_params:
      normalize_features: false
      encode_categorical: True
      return_extra_info: False

    default_params:
      retrain: True
      val_size: 0.2
      early_stopping_rounds: 30
      verbose: false
      learning_rate: 0.3
      max_depth: 6
      n_estimators: 100
      min_child_weight: 1
      subsample: 0.8
      colsample_bytree: 0.8
      gamma: 0.0
      early_stopping_rounds: 100

    param_grid:
      learning_rate:
        - 0.3
        - 0.01
      max_depth:
        - 4
        - 6
        - 8
      n_estimators:
        - 50
        - 100
        - 200
      min_child_weight:
        - 1
        - 5
        - 10
      subsample:
        - 0.6
        - 0.8
        - 1.0
      colsample_bytree:
        - 0.6
        - 0.8
        - 1.0
      gamma:
        - 0.0
        - 0.1
        - 0.2

  catboost:
    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      retrain: False
      val_size: 0.15
      early_stopping_rounds: 100
      verbose: false
      iterations: 500

    param_grid:
      iterations:
        - 100 
        - 500

  categoryembedding:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader
    

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      layers:
        - "128-64-32"
        - "256-128"
        - "256-128-64-32"
        - "32-16"
        - "64-32-16"
        - "64-32"
      activation:
        - "ReLU"
        - "LeakyReLU"
      use_batch_norm:
        - True
        - false
      initialization:
        - "kaiming"
        - "xavier"
        - "random"
      dropout:
        - 0.0
        - 0.3
      embedding_dropout:
        - 0.0
        - 0.2

      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  gandalf:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader
    

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      layers:
        - "128-64-32"
        - "256-128"
        - "256-128-64-32"
        - "32-16"
        - "64-32-16"
        - "64-32"
      activation:
        - "ReLU"
        - "LeakyReLU"
      gflu_stages:
        - 2
        - 8
      gflu_dropout:
        - 0.0
        - 0.1
      embedding_dropout:
        - 0.0
        - 0.1
      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  autoint:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      attn_embed_dim_multiplier:
        - 8
        - 32
      num_heads:
        - 1
        - 4
      num_attn_blocks:
        - 1
        - 5
      attn_dropouts:
        - 0.0
        - 0.2
      embedding_dim:
        - 8
        - 32
      shared_embedding_fraction:
        - 0.1
        - 0.4
      layers:
        - "64-32"
        - "128-64-32"
        - "256-128-64-32"
      activation:
        - ReLU
        - LeakyReLU
        - TanH
      dropout:
        - 0.0
        - 0.2
      attention_pooling:
        - True
        - False
      embedding_initialization:
        - kaiming_uniform
        - kaiming_normal
      share_embedding_strategy:
        - add
        - fraction
      embedding_dropout:
        - 0.05
        - 0.2
      batch_norm_continuous_input:
        - True
        - False
      initialization:
        - kaiming
        - xavier
        - random

      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  fttransformer:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      input_embed_dim_multiplier:
        - 1
        - 8
      num_heads:
        - 4
        - 16
      num_attn_blocks:
        - 3
        - 8
      attn_dropout:
        - 0.0
        - 0.2
      add_norm_dropout:
        - 0.0
        - 0.2
      ff_dropout:
        - 0.0
        - 0.2
      ff_hidden_multiplier:
        - 2
        - 6
      shared_embedding_fraction:
        - 0.1
        - 0.4
      transformer_activation:
        - GEGLU
        - ReGLU
        - SwiGLU
        - ReLU
      embedding_initialization:
        - kaiming_uniform
        - kaiming_normal
      share_embedding_strategy:
        - add
        - fraction
      embedding_dropout:
        - 0.05
        - 0.2
      batch_norm_continuous_input:
        - True
        - False
      attention_pooling:
        - True
        - False

      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  gate:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      gflu_stages:
        - 4
        - 7
      gflu_dropout:
        - 0.0
        - 0.2
      tree_depth:
        - 3
        - 6
      num_trees:
        - 5
        - 11
      binning_activation:
        - sparsemoid
        - entmoid
      feature_mask_function:
        - sparsemax
        - entmax
      tree_dropout:
        - 0.0
        - 0.2
      tree_wise_attention:
        - True
        - False
      tree_wise_attention_dropout:
        - 0.0
        - 0.2
      share_head_weights:
        - True
        - False
      embedding_dropout:
        - 0.05
        - 0.2
      batch_norm_continuous_input:
        - True
        - False
      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  node:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      num_layers:
        - 1
        - 2
      num_trees:
        - 512
        - 1024
      additional_tree_output_dim:
        - 2
        - 4
      depth:
        - 4
        - 6
      choice_function:
        - entmax15
        - sparsemax
      bin_function:
        - entmoid15
        - sparsemoid
      input_dropout:
        - 0.0
        - 0.2
      initialize_response:
        - normal
        - uniform
      initialize_selection_logits:
        - normal
        - uniform
      threshold_init_beta:
        - 0.5
        - 1.0
        - 1.5
      threshold_init_cutoff:
        - 0.5
        - 1.0
        - 1.5
      embedding_dropout:
        - 0.0
        - 0.2
      batch_norm_continuous_input:
        - True
        - False
      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  tabnet:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 256
        - 1024
      virtual_batch_size:
        - 16
        - 128
      n_d: 
        - 8
        - 16
        - 32
        - 64
      n_a:
        - 8
        - 16
        - 32
        - 64
      n_steps:
        - 3
        - 4
        - 5
        - 6
      gamma:
        - 1.1
        - 1.3
        - 1.5
      n_independent:
        - 1
        - 2
        - 3
      n_shared:
        - 1
        - 2
        - 3

      mask_type:
        - sparsemax
        - entmax

      embedding_dropout:
        - 0.0
        - 0.1
        - 0.2
      batch_norm_continuous_input:
        - True
        - False

      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  tabtransformer:

    data_params:
      normalize_features: mean_std
      encode_categorical: False  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping_rounds: 10
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      input_embed_dim_multiplier:
        - 1
        - 8
      num_heads:
        - 4
        - 16
      embedding_initialization:
        - kaiming_uniform
        - kaiming_normal
      embedding_bias:
        - True
        - False
      share_embedding:
        - True
        - False
      share_embedding_strategy:
        - add
        - fraction
      shared_embedding_fraction:
        - 0.25
        - 0.5
        - 0.75
      num_attn_blocks:
        - 4
        - 6
        - 8
      transformer_head_dim:
        - 32
        - 64
        - 128
      attn_dropout:
        - 0.1
        - 0.2
        - 0.3
      add_norm_dropout:
        - 0.1
        - 0.2
      ff_dropout:
        - 0.1
        - 0.2
      ff_hidden_multiplier:
        - 2
        - 4
        - 8
      transformer_activation:
        - GEGLU
        - ReLU
        - SwiGLU
      out_ff_layers:
        - "128-64-32"
        - "256-128-64"
      out_ff_activation:
        - ReLU
        - LeakyReLU
      out_ff_dropout:
        - 0.0
        - 0.1
        - 0.2
      out_ff_initialization:
        - kaiming
        - xavier
        - random
      embedding_dropout:
        - 0.0
        - 0.1
        - 0.2
      batch_norm_continuous_input:
        - True
        - False

      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  resnet:
    data_params:
      normalize_features: mean_std
      encode_categorical: True
      return_extra_info: True  # for dataloader

    default_params:
      early_stopping: True
      verbose: false
      iterations: 20
      retrain: False
      auto_lr_find: False
      max_epochs: 1000
      val_size: 0.2
      early_stopping_patience: 5

    param_grid:
      resnet_depth:
        - resnet18
        - resnet34
        - resnet50
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99

  s1dcnn:
    data_params:
      normalize_features: mean_std
      encode_categorical: True  # for dataloader
      return_extra_info: True  # for dataloader

    default_params:
      retrain: False
      max_epochs: 1000
      val_size: 0.15
      early_stopping_patience: 5

    param_grid:
      batch_size:
        - 1024
        - 512
        - 256
        - 128
      hidden_size:
        - 4096
        - 2048
      optimizer_fn:
        Adam:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
        SGD:
          weight_decay:
            - 0.0
            - 0.0000001
          momentum:
            - 0.9
            - 0.99
          learning_rate:
            - 0.1
            - 0.00001
        AdamW:
          weight_decay:
            - 0.0
            - 0.0000001
          learning_rate:
            - 0.1
            - 0.00001
      scheduler_fn:
        ReduceLROnPlateau:
          factor:
            - 0.1
            - 0.5
          patience:
            - 5
            - 15
        StepLR:
          step_size:
            - 10
            - 30
          gamma:
            - 0.1
            - 0.5
        ExponentialLR:
          gamma:
            - 0.9
            - 0.99
      
  mlp:
    data_params:
      normalize_features: mean_std
      encode_categorical: True
      return_extra_info: False

    default_params:
      early_stopping: True
      n_iter_no_change: 10
      max_iter: 1000
      hidden_layer_sizes: [32]
      activation: relu
      solver: adam
      batch_size: 1024
      val_size: 0.15


    param_grid:
      n_iter_no_change:
        - 10
        - 15
      max_iter:
        - 1000
        - 2000
      hidden_layer_sizes:
        - [64,32]
        - [128,64,32]
      activation:
        - relu
        - tanh
      solver:
        - adam
        - sgd
      batch_size:
        - 1024
        - 512
        - 256
        - 128
